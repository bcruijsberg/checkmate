{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e077d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the largest fact checking organisations\n",
    "factors_sub_df=clean_factors_df[clean_factors_df[\"organisation\"].isin([\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"])]\n",
    "factors_sample_df= factors_sub_df.sample(n=1000, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81368</th>\n",
       "      <td>\"Arizona officials caught changing ballots, ha...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90708</th>\n",
       "      <td>The Yeti snow monster from Disneyland's iconic...</td>\n",
       "      <td>https://www.snopes.com/fact-check/disney-yeti/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67021</th>\n",
       "      <td>\"I can tell you that the enhanced interrogatio...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2016/may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10591</th>\n",
       "      <td>Nigerian election tribunal witness goes on the...</td>\n",
       "      <td>https://factcheck.afp.com/doc.afp.com.33NE7Y8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75871</th>\n",
       "      <td>\"We essentially repealed Obamacare because we ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2017/dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71583</th>\n",
       "      <td>President Obama plans to \"impose a tax of at l...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75185</th>\n",
       "      <td>\"Almost half a million people are still eligib...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2016/aug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91346</th>\n",
       "      <td>Two 'racist' Black teenagers shot and killed a...</td>\n",
       "      <td>https://www.snopes.com/fact-check/thugs-shoot-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71546</th>\n",
       "      <td>Says Barack Obama had \"huge majorities\" in Con...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91280</th>\n",
       "      <td>Walter \"Blackie\" Wetzel, a former leader of th...</td>\n",
       "      <td>https://www.snopes.com/fact-check/walter-wetze...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim  \\\n",
       "81368  \"Arizona officials caught changing ballots, ha...   \n",
       "90708  The Yeti snow monster from Disneyland's iconic...   \n",
       "67021  \"I can tell you that the enhanced interrogatio...   \n",
       "10591  Nigerian election tribunal witness goes on the...   \n",
       "75871  \"We essentially repealed Obamacare because we ...   \n",
       "71583  President Obama plans to \"impose a tax of at l...   \n",
       "75185  \"Almost half a million people are still eligib...   \n",
       "91346  Two 'racist' Black teenagers shot and killed a...   \n",
       "71546  Says Barack Obama had \"huge majorities\" in Con...   \n",
       "91280  Walter \"Blackie\" Wetzel, a former leader of th...   \n",
       "\n",
       "                                                     url  \n",
       "81368  https://www.politifact.com/factchecks/2024/nov...  \n",
       "90708     https://www.snopes.com/fact-check/disney-yeti/  \n",
       "67021  https://www.politifact.com/factchecks/2016/may...  \n",
       "10591      https://factcheck.afp.com/doc.afp.com.33NE7Y8  \n",
       "75871  https://www.politifact.com/factchecks/2017/dec...  \n",
       "71583  https://www.politifact.com/factchecks/2011/nov...  \n",
       "75185  https://www.politifact.com/factchecks/2016/aug...  \n",
       "91346  https://www.snopes.com/fact-check/thugs-shoot-...  \n",
       "71546  https://www.politifact.com/factchecks/2011/dec...  \n",
       "91280  https://www.snopes.com/fact-check/walter-wetze...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_sample_df=factors_sample_df[['claim','url']]\n",
    "factors_sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "### First step: create a summary and listing possible problems\n",
    "Retrieve information and create a summary as done in the original workflow of the assistant for these 1000 claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. Your main goal is not to provide answers, \n",
    "but to support the student in developing their own reasoning and critical thinking. You do this by asking open, \n",
    "reflective questions that encourage exploration, justification, and evaluation. You do not take over the student's thinking, \n",
    "and you do not complete tasks for them. Avoid giving conclusions or definitive judgments unless the workflow specifically requires it.\n",
    "\n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "### Important Rules\n",
    "This part focuses on determining whether the subject is clear, the claim is quantitative, how precise it is, how the data was derived, \n",
    "and what additional details are present or missing. \n",
    "You don't need to acquire all missing details right now; just identify what is missing and formulate one clarifying question. \n",
    "If the user says no more details are available, proceed with what you have.\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject. If unclear ‚Üí \"unclear\".\n",
    "2. Determine if the claim is *quantitative*. Set *quantitative* to true/false.\n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". If qualitative, use \"\".\n",
    "4. Identify what the claim is *based on* (e.g., \"survey ‚Ä¶\", \"official statistics\"). If none ‚Üí \"unclear\".\n",
    "5. Briefly *explain your reasoning* (quote/phrase from the claim).\n",
    "6. Ask exactly one *clarifying/confirmation question* that would make the claim checkable.\n",
    "7. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "8. *Summarize concisely* what is currently known about the claim and its checkability.\n",
    "   - Include: subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Mention any active alerts or missing information.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"subject\": string. Use \"unclear\" if the subject is not clear.\n",
    "- \"quantitative\": string. Start with \"true\" or \"false\", followed by a short explanation.\n",
    "- \"precision\": string. One of \"precise\", \"vague\", \"absolute (100%)\", or \"\" (empty string), plus a short explanation.\n",
    "- \"based_on\": string. Either a brief description of the methodology/source or \"unclear\", plus a short explanation.\n",
    "- \"question\": string. One open clarifying or confirmation question; don't ask for specific details.\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"subject\": \"Spanish court sentencing of Catalan leaders (2019)\",\n",
    "  \"quantitative\": \"false, because there is no quantitative data\",\n",
    "  \"precision\": \"precise, because it refers to a specific legal event in a defined time and place\",\n",
    "  \"based_on\": \"news reporting / legal documents, because the information is typically drawn from official court rulings and journalistic coverage\",\n",
    "  \"question\": \"What is the main point you are trying to understand here?\",\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"subject\": \"EU asylum applications\",\n",
    "  \"quantitative\": \"true, because it refers to measurable counts of applications\",\n",
    "  \"precision\": \"vague, because no time frame, comparison, or dataset is identified\",\n",
    "  \"based_on\": \"unclear, because the data source could vary (Eurostat, UNHCR, national agencies, media summaries)\",\n",
    "  \"question\": \"What do you think is important to clarify before evaluating this?\",\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\checkmate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import List\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "class MoreInfoResult(BaseModel):\n",
    "    subject: str = Field(\"\", description=\"The subject of the claim\")\n",
    "    quantitative: str = Field(\"\", description=\"Is the claim quantitative?\")\n",
    "    precision: str = Field(\"\", description=\"How precise is it?\")\n",
    "    based_on: str = Field(\"\", description=\"how was the data collected or derived?\")\n",
    "    question: str = Field(\"\", description=\"Question to user for clarification if needed\")\n",
    "    alerts: List[str] = Field([], description=\"Any alerts or warnings about the claim\")\n",
    "    summary: str = Field(\"\", description=\"A concise summary of the claim\")\n",
    "\n",
    "#low temperature for more factual answers, \n",
    "llmQwen = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "llmGPT5 = ChatOpenAI(model=\"gpt-5\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2657c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def retrieve_info(claim: str) -> dict:\n",
    "    \"\"\"Gather more information about a potentially checkable claim.\"\"\"\n",
    "\n",
    "    # Let LangChain handle structured output using tools/schema\n",
    "    structured_llm = llmGPT5.with_structured_output(MoreInfoResult)  # üëà no method=\"json_mode\"\n",
    "\n",
    "    # You can keep your prompt, but see step 2 below to simplify the JSON part\n",
    "    prompt = get_information_prompt.format(claim=claim)\n",
    "\n",
    "    # Call the model ‚Äì pass the prompt string, not a list of HumanMessage\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # Pydantic model ‚Üí Python dict (perfect for pandas)\n",
    "    return result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dd7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'Speed limit policy‚Äôs impact on CO‚ÇÇ emissions', 'quantitative': 'true, because it asserts a magnitude of CO‚ÇÇ savings (‚Äúdoes not save a lot‚Äù), which is measurable', 'precision': 'vague, because ‚Äúnot a lot‚Äù provides no numeric estimate, threshold, baseline, geography, or timeframe', 'based_on': 'unclear, because no source or method is cited; could rely on transport models, emissions inventories, or policy evaluations', 'question': 'What context are you thinking of (e.g., country/road types/timeframe) and how would you recognize that the CO‚ÇÇ savings are ‚Äúa lot‚Äù versus ‚Äúnot a lot‚Äù?', 'alerts': ['vague quantitative claim', 'geography missing', 'time period missing', 'methodological details absent', 'policy specifics missing'], 'summary': 'Claim concerns the effect of a (unspecified) speed limit on CO‚ÇÇ emissions; it is quantitative but vague. No source or method is provided. Key uncertainties include context (geography, road types), timeframe, policy parameters (which speed limit, where applied), and what counts as ‚Äúa lot.‚Äù These gaps currently limit checkability.'}\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_info(\"The speed limit does not save a lot of CO‚ÇÇ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = factors_sample_df.head(5).copy()\n",
    "test_df[\"analysis\"] = test_df[\"claim\"].apply(retrieve_info)\n",
    "test_analysis_df = test_df[\"analysis\"].apply(pd.Series)\n",
    "test_all_df = pd.concat([test_df, test_analysis_df], axis=1)\n",
    "#factors_sample_df[\"analysis\"] = factors_sample_df[\"claim\"].apply(retrieve_info)\n",
    "#factor_analysis_df = factors_sample_df[\"analysis\"].apply(pd.Series)\n",
    "#factors_all_df = pd.concat([factors_sample_df, factor_analysis_df], axis=1)\n",
    "\n",
    "#factors_all_df.to_csv(\"Data/finetune_data_1.csv\", index=False)\n",
    "#factors_all_df.head(10)\n",
    "test_all_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5da6ce",
   "metadata": {},
   "source": [
    "use GPT5, often regarded as best model for various tasks, including language tasks:\n",
    "- https://artificialanalysis.ai/leaderboards/models\n",
    "- https://www.vellum.ai/llm-leaderboard?utm_source=google&utm_medium=organic\n",
    "- https://www.shakudo.io/blog/top-9-large-language-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1a44e",
   "metadata": {},
   "source": [
    "## Create JSONL messages for finetuning\n",
    "Next, create messages containing a claim, a verdict, and an explanation, then add Socratic questions to encourage critical thinking and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97754686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4995 JSON objects to Data\\socratic_questions.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "#low temperature for more factual answers,\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.2 )\n",
    "\n",
    "SYS = \"\"\"You are given a fact-check CLAIM and its justification as SHORT_EXPLANATION explaining why it is labeled as true, false, mostly true, \n",
    "mostly false, or uncheckable. Your task is to generate five Socratic questions that probe the justification and verdict. The goal is to \n",
    "challenge the reasoning, surface blind spots, and encourage deeper reflection, not to accept the explanation at face value. Since the output\n",
    " will be used to finetune an LLM that critiques the reasoning of a fact-checking model, ensure that your questions reflect the following principles:\n",
    "- Factuality ‚Äì Do the claims rely on verifiable evidence? Could missing or weak evidence be questioned?\n",
    "- Objectivity ‚Äì Is the reasoning neutral, or does it show bias? How could the framing be challenged?\n",
    "- Fairness ‚Äì Are multiple perspectives considered? Is the reasoning applied consistently?\n",
    "- Transparency ‚Äì Is the explanation clear about its sources and reasoning steps? What is hidden or assumed?\n",
    "- Hallucinations ‚Äì Does the explanation risk introducing unsupported or invented information?\n",
    "- Strategies & Alternatives ‚Äì Are there other ways to frame, investigate, or reason about the claim?\n",
    "\n",
    "When writing questions, draw from the following categories of Socratic questioning. Use them as inspiration to diversify your five questions \n",
    "(do not stick to just one category):\n",
    "\n",
    "Purpose ‚Äì probe the aim or agenda.\n",
    "- What is your purpose right now?\n",
    "- Why are you writing this?\n",
    "- What do you want to persuade them of?\n",
    "- What is our central aim or task in this line of thought?\n",
    "\n",
    "Questions ‚Äì probe the underlying questions.\n",
    "- I am not sure exactly what question you are raising. Could you explain it?\n",
    "- Is this question the best one to focus on, or is there a more pressing one?\n",
    "- What questions might we be failing to ask that we should be asking?\n",
    "\n",
    "Information ‚Äì probe the evidence or data.\n",
    "- On what information are you basing that comment?\n",
    "- How do we know this information is accurate? How could we verify it?\n",
    "- Have we failed to consider any information or data we need to consider?\n",
    "\n",
    "Inferences & Conclusions ‚Äì probe how the conclusion was drawn.\n",
    "- How did you reach that conclusion?\n",
    "- Could you explain your reasoning?\n",
    "- Is there an alternative plausible conclusion?\n",
    "\n",
    "Concepts & Ideas ‚Äì probe key ideas being applied.\n",
    "- What is the main idea you are using in your reasoning?\n",
    "- Are we using the appropriate concept, or do we need to reconceptualize the problem?\n",
    "- Do we need more facts, or do we need to rethink how we are labeling the facts?\n",
    "\n",
    "Assumptions ‚Äì probe what is taken for granted.\n",
    "- What exactly are you taking for granted here?\n",
    "- Why are you assuming that? Shouldn‚Äôt we rather assume that‚Ä¶?\n",
    "- What alternative assumptions might we make?\n",
    "\n",
    "Implications & Consequences ‚Äì probe what follows.\n",
    "- What are you implying when you say‚Ä¶?\n",
    "- If we do this, what is likely to happen as a result?\n",
    "- Have you considered the implications of this reasoning?\n",
    "- Viewpoints & Perspectives ‚Äì probe alternative frames.\n",
    "\n",
    "From what point of view are you looking at this?\n",
    "- Is there another point of view we should consider?\n",
    "- Which of these possible viewpoints makes the most sense given the situation?\n",
    "\n",
    "Instructions:\n",
    "- Do not repeat the justification.\n",
    "- Do not state whether the verdict is correct.\n",
    "- Ask probing questions that challenge the reasoning, highlight blind spots, and open space for reconsideration.\n",
    "- Ensure the five questions you generate come from different categories where possible\n",
    "\n",
    "Output format (JSONL):\n",
    "{\n",
    "  \"claim\": \"the original claim\",\n",
    "  \"short_explanation\": \"the original short explanation\",\n",
    "  \"verdict\": \"The verdict as written in the explanation: true, false, mostly true, mostly false or uncheckable\",\n",
    "  \"questions\": [\n",
    "    \"What is our central aim or task in this line of thought?\",\n",
    "    \"What is the underlying question that this explanation is really trying to address?\",\n",
    "    \"How do we know this information is accurate, and how could we verify it?\",\n",
    "    \"Is there an alternative plausible conclusion based on the same reasoning?\",\n",
    "    \"Is there another point of view we should consider when evaluating this claim?\"\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def add_questions(claim: str, short_explanation: str):\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nSHORT_EXPLANATION: {short_explanation}')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        one_line = \" \".join(text.split())\n",
    "\n",
    "        return one_line\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Load data and compute short_explanation as before ---\n",
    "factchecks_df = pd.read_csv(\"Data/factchecks_with_verdicts.csv\")\n",
    "\n",
    "# --- Generate JSONL lines and write them to a single file ---\n",
    "output_path = Path(\"Data/socratic_questions.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "valid_lines = []\n",
    "\n",
    "for _, row in factchecks_df.iterrows():\n",
    "    line = add_questions(row[\"claim\"], row[\"short_explanation\"])\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(line)  # parse the JSON string\n",
    "    except json.JSONDecodeError:\n",
    "        continue  # skip if the model output was not valid JSON\n",
    "\n",
    "    # Expand into one object per question\n",
    "    for q in obj.get(\"questions\", []):\n",
    "        new_obj = {\n",
    "            \"claim\": obj[\"claim\"],\n",
    "            \"short_explanation\": obj[\"short_explanation\"],\n",
    "            \"verdict\": obj[\"verdict\"],\n",
    "            \"question\": q,\n",
    "        }\n",
    "        valid_lines.append(new_obj)\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in valid_lines:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(valid_lines)} JSON objects to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
