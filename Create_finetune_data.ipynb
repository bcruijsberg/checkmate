{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9116532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "false             77641\n",
       "partially true    18796\n",
       "misleading        10165\n",
       "true               9222\n",
       "other              1089\n",
       "unverifiable       1068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fcced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a10f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = [\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"]\n",
    "labels = [\"true\", \"false\", \"partially true\", \"misleading\"]\n",
    "\n",
    "def get_sample(df, label, n=250):\n",
    "    subset = df[(df[\"normalised_rating\"] == label) & (df[\"organisation\"].isin(orgs))]\n",
    "    return subset.sample(n, random_state=23)\n",
    "\n",
    "# Get samples for each label and combine\n",
    "samples = [get_sample(clean_factors_df, label) for label in labels]\n",
    "sampled_clean_factors_df = pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99088244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "true              250\n",
       "false             250\n",
       "partially true    250\n",
       "misleading        250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2308d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def fetch_full_article(url) -> str:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary of an article from a given URL.\n",
    "\n",
    "    Returns: summary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return f\"[Failed to fetch article content from {url}\"\n",
    "\n",
    "# Apply the function to each URL in the DataFrame\n",
    "sampled_clean_factors_df['article'] = sampled_clean_factors_df.apply(\n",
    "    lambda row: pd.Series(fetch_full_article(row['url'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>normalised_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>Claim: A physical book detailing the contents ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/hunter-biden...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>Claim: A video taken at a George Floyd protest...</td>\n",
       "      <td>https://www.snopes.com/fact-check/dpd-car-preg...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>Claim: In September 2020, U.S. President Donal...</td>\n",
       "      <td>https://www.snopes.com/fact-check/trump-execut...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>Claim: A photograph shows Air Force One during...</td>\n",
       "      <td>https://www.snopes.com/fact-check/air-force-on...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>Claim:\\n\\nClaim: Photographs show the results ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/moose-story/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>Claim: PlayStation and Xbox announced that ref...</td>\n",
       "      <td>https://www.snopes.com/fact-check/cyberpunk-20...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>Claim:\\n\\nClaim: On-line coupon can be redeeme...</td>\n",
       "      <td>https://www.snopes.com/fact-check/jamba-juice-...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>Claim: U.S. President Joe Biden wore a hard ha...</td>\n",
       "      <td>https://www.snopes.com/fact-check/biden-wear-h...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>U.S. Rep Bobby Scott cited a staggering statis...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/may...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>Is West Virginia trailing its neighbors in sci...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2019/apr...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  A physical book detailing the contents of Hunt...   \n",
       "1  A video taken at a George Floyd protest in Den...   \n",
       "2  In September 2020, U.S. President Donald Trump...   \n",
       "3  A photograph shows Air Force One during U.S. P...   \n",
       "4           Photographs show the results of a car vs   \n",
       "5  PlayStation and Xbox announced that refunds wo...   \n",
       "6  On-line coupon can be redeemed for a free smoo...   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...   \n",
       "9  \"Our state has fewer science, technology, engi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Claim: A physical book detailing the contents ...   \n",
       "1  Claim: A video taken at a George Floyd protest...   \n",
       "2  Claim: In September 2020, U.S. President Donal...   \n",
       "3  Claim: A photograph shows Air Force One during...   \n",
       "4  Claim:\\n\\nClaim: Photographs show the results ...   \n",
       "5  Claim: PlayStation and Xbox announced that ref...   \n",
       "6  Claim:\\n\\nClaim: On-line coupon can be redeeme...   \n",
       "7  Claim: U.S. President Joe Biden wore a hard ha...   \n",
       "8  U.S. Rep Bobby Scott cited a staggering statis...   \n",
       "9  Is West Virginia trailing its neighbors in sci...   \n",
       "\n",
       "                                                 url normalised_rating  \n",
       "0  https://www.snopes.com/fact-check/hunter-biden...              true  \n",
       "1  https://www.snopes.com/fact-check/dpd-car-preg...              true  \n",
       "2  https://www.snopes.com/fact-check/trump-execut...              true  \n",
       "3  https://www.snopes.com/fact-check/air-force-on...              true  \n",
       "4     https://www.snopes.com/fact-check/moose-story/              true  \n",
       "5  https://www.snopes.com/fact-check/cyberpunk-20...              true  \n",
       "6  https://www.snopes.com/fact-check/jamba-juice-...              true  \n",
       "7  https://www.snopes.com/fact-check/biden-wear-h...              true  \n",
       "8  https://www.politifact.com/factchecks/2011/may...              true  \n",
       "9  https://www.politifact.com/factchecks/2019/apr...              true  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_factchecks_df=sampled_clean_factors_df[['claim','article','url','normalised_rating']]\n",
    "sampled_factchecks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e73c8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many articles failed to fetch\n",
    "no_article_df = sampled_factchecks_df[sampled_factchecks_df['article'].str.contains(\"Failed to fetch article content \", na=False)]\n",
    "no_article_df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "Generate short justifications for the original verdict, based upon the article and the given Normalized rating (verdict).\n",
    "use GPT5, often regarded as best model for various tasks:\n",
    "- https://artificialanalysis.ai/leaderboards/models\n",
    "- https://www.vellum.ai/llm-leaderboard?utm_source=google&utm_medium=organic\n",
    "- https://www.shakudo.io/blog/top-9-large-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal LLM verdict+explanation pipeline ------------------------------------\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import Literal\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "#low temperature for more factual answers\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.2, base_url=\"http://localhost:11434\")\n",
    "\n",
    "SYS = \"\"\"You are a careful fact-checking assistant.\n",
    "Write ONE or TWO concise sentences (≤50 words) that justify the given VERDICT using only evidence in the ARTICLE.\n",
    "No outside facts, speculation, or bullet lists, focus on why the article is TRUE, FALSE, PARTIALLY TRUE or MISLEADING.\n",
    "You don't have to mention the verdict in your explanation, since this is already given.\n",
    "If the article does not provide enough information to justify the verdict, say so.\"\"\"\n",
    "\n",
    "def infer_verdict_and_expl(claim: str, article: str, normalised_rating: str):\n",
    "    if not isinstance(article, str) or \"Failed to fetch article content\" in article:\n",
    "        return None\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nVERDICT: {normalised_rating}\\n\\nARTICLE:\\n\"\"\" {article[:8000]} \"\"\"')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        return \" \".join(text.split())  # collapse whitespace\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Apply to DataFrame (expects columns: 'claim', 'article', 'normalised_rating')\n",
    "factchecks_df = sampled_factchecks_df.copy()\n",
    "factchecks_df.loc[:, \"short_explanation\"] = factchecks_df.apply(\n",
    "    lambda r: infer_verdict_and_expl(r[\"claim\"], r[\"article\"], r[\"normalised_rating\"]),axis=1\n",
    ")\n",
    "\n",
    "factchecks_df[[\"claim\", \"normalised_rating\", \"short_explanation\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "101d21e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>normalised_rating</th>\n",
       "      <th>short_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>true</td>\n",
       "      <td>The article confirms that a physical copy of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>true</td>\n",
       "      <td>The article confirms that police officers fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>true</td>\n",
       "      <td>The claim is partially true because Trump did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>true</td>\n",
       "      <td>This claim is TRUE because the article states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>true</td>\n",
       "      <td>The article provides photographs of the accide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>true</td>\n",
       "      <td>The claim is true because Sony PlayStation and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>true</td>\n",
       "      <td>The article confirms the existence of a legiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>true</td>\n",
       "      <td>The claim that U.S. President Joe Biden wore a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>true</td>\n",
       "      <td>The article cites a 2007 report by Robert Balf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>true</td>\n",
       "      <td>The article states that West Virginia conferre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim normalised_rating  \\\n",
       "0  A physical book detailing the contents of Hunt...              true   \n",
       "1  A video taken at a George Floyd protest in Den...              true   \n",
       "2  In September 2020, U.S. President Donald Trump...              true   \n",
       "3  A photograph shows Air Force One during U.S. P...              true   \n",
       "4           Photographs show the results of a car vs              true   \n",
       "5  PlayStation and Xbox announced that refunds wo...              true   \n",
       "6  On-line coupon can be redeemed for a free smoo...              true   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...              true   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...              true   \n",
       "9  \"Our state has fewer science, technology, engi...              true   \n",
       "\n",
       "                                   short_explanation  \n",
       "0  The article confirms that a physical copy of t...  \n",
       "1  The article confirms that police officers fire...  \n",
       "2  The claim is partially true because Trump did ...  \n",
       "3  This claim is TRUE because the article states ...  \n",
       "4  The article provides photographs of the accide...  \n",
       "5  The claim is true because Sony PlayStation and...  \n",
       "6  The article confirms the existence of a legiti...  \n",
       "7  The claim that U.S. President Joe Biden wore a...  \n",
       "8  The article cites a 2007 report by Robert Balf...  \n",
       "9  The article states that West Virginia conferre...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factchecks_df[[\"claim\", \"normalised_rating\", \"short_explanation\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results to a csv file\n",
    "factchecks_df.to_csv(\"Data/factchecks_with_verdicts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1a44e",
   "metadata": {},
   "source": [
    "## Create JSONL messages for finetuning\n",
    "Next, create messages containing a claim, a verdict, and an explanation, then add Socratic questions to encourage critical thinking and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e257b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal LLM verdict+explanation pipeline ------------------------------------\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import Literal\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "#low temperature for more factual answers\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.2, base_url=\"http://localhost:11434\")\n",
    "\n",
    "SYS = \"\"\"You are a careful fact-checking assistant.\n",
    "Write ONE or TWO concise sentences (≤50 words) that justify the given VERDICT using only evidence in the ARTICLE.\n",
    "No outside facts, speculation, or bullet lists, focus on why the article is TRUE, FALSE, PARTIALLY TRUE or MISLEADING.\n",
    "You don't have to mention the verdict in your explanation, since this is already given.\n",
    "If the article does not provide enough information to justify the verdict, say so.\"\"\"\n",
    "\n",
    "def infer_verdict_and_expl(claim: str, article: str, normalised_rating: str):\n",
    "    if not isinstance(article, str) or \"Failed to fetch article content\" in article:\n",
    "        return None\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nVERDICT: {normalised_rating}\\n\\nARTICLE:\\n\"\"\" {article[:8000]} \"\"\"')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        return \" \".join(text.split())  # collapse whitespace\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Apply to DataFrame (expects columns: 'claim', 'article', 'normalised_rating')\n",
    "factchecks_df = sampled_factchecks_df.copy()\n",
    "factchecks_df.loc[:, \"short_explanation\"] = factchecks_df.apply(\n",
    "    lambda r: infer_verdict_and_expl(r[\"claim\"], r[\"article\"], r[\"normalised_rating\"]),axis=1\n",
    ")\n",
    "\n",
    "factchecks_df[[\"claim\", \"normalised_rating\", \"short_explanation\"]].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
