{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9116532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "false             77641\n",
       "partially true    18796\n",
       "misleading        10165\n",
       "true               9222\n",
       "other              1089\n",
       "unverifiable       1068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = [\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"]\n",
    "labels = [\"true\", \"false\", \"partially true\", \"misleading\"]\n",
    "\n",
    "def get_sample(df, label, n=250):\n",
    "    subset = df[(df[\"normalised_rating\"] == label) & (df[\"organisation\"].isin(orgs))]\n",
    "    return subset.sample(n, random_state=23)\n",
    "\n",
    "# Get samples for each label and combine\n",
    "samples = [get_sample(clean_factors_df, label) for label in labels]\n",
    "sampled_clean_factors_df = pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99088244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "true              250\n",
       "false             250\n",
       "partially true    250\n",
       "misleading        250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2308d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def fetch_full_article(url) -> str:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary of an article from a given URL.\n",
    "\n",
    "    Returns: summary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return f\"[Failed to fetch article content from {url}\"\n",
    "\n",
    "# Apply the function to each URL in the DataFrame\n",
    "sampled_clean_factors_df['article'] = sampled_clean_factors_df.apply(\n",
    "    lambda row: pd.Series(fetch_full_article(row['url'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>normalised_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>Claim: A physical book detailing the contents ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/hunter-biden...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>Claim: A video taken at a George Floyd protest...</td>\n",
       "      <td>https://www.snopes.com/fact-check/dpd-car-preg...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>Claim: In September 2020, U.S. President Donal...</td>\n",
       "      <td>https://www.snopes.com/fact-check/trump-execut...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>Claim: A photograph shows Air Force One during...</td>\n",
       "      <td>https://www.snopes.com/fact-check/air-force-on...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>Claim:\\n\\nClaim: Photographs show the results ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/moose-story/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>Claim: PlayStation and Xbox announced that ref...</td>\n",
       "      <td>https://www.snopes.com/fact-check/cyberpunk-20...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>Claim:\\n\\nClaim: On-line coupon can be redeeme...</td>\n",
       "      <td>https://www.snopes.com/fact-check/jamba-juice-...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>Claim: U.S. President Joe Biden wore a hard ha...</td>\n",
       "      <td>https://www.snopes.com/fact-check/biden-wear-h...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>U.S. Rep Bobby Scott cited a staggering statis...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/may...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>Is West Virginia trailing its neighbors in sci...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2019/apr...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  A physical book detailing the contents of Hunt...   \n",
       "1  A video taken at a George Floyd protest in Den...   \n",
       "2  In September 2020, U.S. President Donald Trump...   \n",
       "3  A photograph shows Air Force One during U.S. P...   \n",
       "4           Photographs show the results of a car vs   \n",
       "5  PlayStation and Xbox announced that refunds wo...   \n",
       "6  On-line coupon can be redeemed for a free smoo...   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...   \n",
       "9  \"Our state has fewer science, technology, engi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Claim: A physical book detailing the contents ...   \n",
       "1  Claim: A video taken at a George Floyd protest...   \n",
       "2  Claim: In September 2020, U.S. President Donal...   \n",
       "3  Claim: A photograph shows Air Force One during...   \n",
       "4  Claim:\\n\\nClaim: Photographs show the results ...   \n",
       "5  Claim: PlayStation and Xbox announced that ref...   \n",
       "6  Claim:\\n\\nClaim: On-line coupon can be redeeme...   \n",
       "7  Claim: U.S. President Joe Biden wore a hard ha...   \n",
       "8  U.S. Rep Bobby Scott cited a staggering statis...   \n",
       "9  Is West Virginia trailing its neighbors in sci...   \n",
       "\n",
       "                                                 url normalised_rating  \n",
       "0  https://www.snopes.com/fact-check/hunter-biden...              true  \n",
       "1  https://www.snopes.com/fact-check/dpd-car-preg...              true  \n",
       "2  https://www.snopes.com/fact-check/trump-execut...              true  \n",
       "3  https://www.snopes.com/fact-check/air-force-on...              true  \n",
       "4     https://www.snopes.com/fact-check/moose-story/              true  \n",
       "5  https://www.snopes.com/fact-check/cyberpunk-20...              true  \n",
       "6  https://www.snopes.com/fact-check/jamba-juice-...              true  \n",
       "7  https://www.snopes.com/fact-check/biden-wear-h...              true  \n",
       "8  https://www.politifact.com/factchecks/2011/may...              true  \n",
       "9  https://www.politifact.com/factchecks/2019/apr...              true  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_factchecks_df=sampled_clean_factors_df[['claim','article','url','normalised_rating']]\n",
    "sampled_factchecks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e73c8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many articles failed to fetch\n",
    "no_article_df = sampled_factchecks_df[sampled_factchecks_df['article'].str.contains(\"Failed to fetch article content \", na=False)]\n",
    "no_article_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd9ae343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_factchecks_df.to_csv(\"Data/sample_factchecks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "### Generating short justifications and connecting the verdict as in the Eurpean Fact Checking Project\n",
    "Generate short justifications for the original verdict, based upon the article and the given Normalized rating (verdict).\n",
    "use GPT5, often regarded as best model for various tasks:\n",
    "- https://artificialanalysis.ai/leaderboards/models\n",
    "- https://www.vellum.ai/llm-leaderboard?utm_source=google&utm_medium=organic\n",
    "- https://www.shakudo.io/blog/top-9-large-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>normalised_rating</th>\n",
       "      <th>short_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article states that Marco Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article states that Denver po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article quotes Trump as sayin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article confirms that the pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article provides a detailed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article states that Sony Play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article directly quotes a Jam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article provides evidence tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article cites a 2010 report t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article cites raw numbers of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim normalised_rating  \\\n",
       "0  A physical book detailing the contents of Hunt...              true   \n",
       "1  A video taken at a George Floyd protest in Den...              true   \n",
       "2  In September 2020, U.S. President Donald Trump...              true   \n",
       "3  A photograph shows Air Force One during U.S. P...              true   \n",
       "4           Photographs show the results of a car vs              true   \n",
       "5  PlayStation and Xbox announced that refunds wo...              true   \n",
       "6  On-line coupon can be redeemed for a free smoo...              true   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...              true   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...              true   \n",
       "9  \"Our state has fewer science, technology, engi...              true   \n",
       "\n",
       "                                   short_explanation  \n",
       "0  TRUE because the article states that Marco Pol...  \n",
       "1  TRUE because the article states that Denver po...  \n",
       "2  TRUE because the article quotes Trump as sayin...  \n",
       "3  TRUE because the article confirms that the pho...  \n",
       "4  TRUE because the article provides a detailed a...  \n",
       "5  TRUE because the article states that Sony Play...  \n",
       "6  TRUE because the article directly quotes a Jam...  \n",
       "7  TRUE because the article provides evidence tha...  \n",
       "8  TRUE because the article cites a 2010 report t...  \n",
       "9  TRUE because the article cites raw numbers of ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimal LLM verdict+explanation pipeline ------------------------------------\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import Literal\n",
    "#from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "#low temperature for more factual answers\n",
    "#llm = ChatOllama(model=\"llama3.2\", temperature=0.2, base_url=\"http://localhost:11434\")\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.2 )\n",
    "\n",
    "SYS = \"\"\"You are a careful fact-checking assistant.\n",
    "Write ONE or TWO concise sentences (≤50 words) that justify the given VERDICT using only evidence in the ARTICLE.\n",
    "No outside facts, speculation, or bullet lists, focus on why the article is TRUE, FALSE, MOSTLY TRUE, MOSTLY FALSE, or UNCHECKABLE.\n",
    "Start with the verdict in capitals, start with \"TRUE because\", \"FALSE because\", \"MOSTLY TRUE because\", \"MOSTLY FALSE because\", or \"UNCHECKABLE because\".\n",
    "If the Normalised rating is TRUE or FALSE, the verdict must be TRUE or FALSE.\n",
    "If the Normalised rating is PARTIALLY TRUE or MISLEADING, the verdict must be MOSTLY TRUE or MOSTLY FALSE.\n",
    "If the article does not provide enough information to justify the verdict, say so.\n",
    "\n",
    " Examples:\n",
    "    “The Imperial College London study estimated Omicron's reinfection risk is 5.4 times higher than Delta's, indicating faster spread. \n",
    "    South Africa's data showed higher reinfection risk with Omicron. However, the evidence is preliminary and not peer-reviewed, \n",
    "    so it's mostly true.” → MOSTLY TRUE\n",
    "    “The World Bank does not have a single 'climate fund', and Oxfam's Climate Finance Unchecked report (2017-2023) found no missing funds \n",
    "    but rather a lack of tracking for project components labeled 'climate finance' during approval. \n",
    "    The claim originated from a social media post by Brian Tamaki, which was debunked by AAP FactCheck using Oxfam's report data.\n",
    "    [0] Charity report did not find World Bank climate change fund was missing $41 billion” → FALSE\"\"\"\n",
    "\n",
    "def infer_verdict_and_expl(claim: str, article: str, normalised_rating: str):\n",
    "    if not isinstance(article, str) or \"Failed to fetch article content\" in article:\n",
    "        return None\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nVERDICT: {normalised_rating}\\n\\nARTICLE:\\n\"\"\" {article} \"\"\"')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        return \" \".join(text.split())  # collapse whitespace\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Apply to DataFrame (expects columns: 'claim', 'article', 'normalised_rating')\n",
    "factchecks_df = pd.read_csv(\"Data/sample_factchecks.csv\")\n",
    "factchecks_df.loc[:, \"short_explanation\"] = factchecks_df.apply(\n",
    "    lambda r: infer_verdict_and_expl(r[\"claim\"], r[\"article\"], r[\"normalised_rating\"]),axis=1\n",
    ")\n",
    "\n",
    "factchecks_df[[\"claim\", \"normalised_rating\", \"short_explanation\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "154ca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results to a csv file\n",
    "factchecks_df.to_csv(\"Data/factchecks_with_verdicts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1a44e",
   "metadata": {},
   "source": [
    "## Create JSONL messages for finetuning\n",
    "Next, create messages containing a claim, a verdict, and an explanation, then add Socratic questions to encourage critical thinking and reflection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
