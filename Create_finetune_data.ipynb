{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9116532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "false             77641\n",
       "partially true    18796\n",
       "misleading        10165\n",
       "true               9222\n",
       "other              1089\n",
       "unverifiable       1068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = [\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"]\n",
    "labels = [\"true\", \"false\", \"partially true\", \"misleading\"]\n",
    "\n",
    "def get_sample(df, label, n=250):\n",
    "    subset = df[(df[\"normalised_rating\"] == label) & (df[\"organisation\"].isin(orgs))]\n",
    "    return subset.sample(n, random_state=23)\n",
    "\n",
    "# Get samples for each label and combine\n",
    "samples = [get_sample(clean_factors_df, label) for label in labels]\n",
    "sampled_clean_factors_df = pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99088244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalised_rating\n",
       "true              250\n",
       "false             250\n",
       "partially true    250\n",
       "misleading        250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_clean_factors_df['normalised_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2308d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def fetch_full_article(url) -> str:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary of an article from a given URL.\n",
    "\n",
    "    Returns: summary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return f\"[Failed to fetch article content from {url}\"\n",
    "\n",
    "# Apply the function to each URL in the DataFrame\n",
    "sampled_clean_factors_df['article'] = sampled_clean_factors_df.apply(\n",
    "    lambda row: pd.Series(fetch_full_article(row['url'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>normalised_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>Claim: A physical book detailing the contents ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/hunter-biden...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>Claim: A video taken at a George Floyd protest...</td>\n",
       "      <td>https://www.snopes.com/fact-check/dpd-car-preg...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>Claim: In September 2020, U.S. President Donal...</td>\n",
       "      <td>https://www.snopes.com/fact-check/trump-execut...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>Claim: A photograph shows Air Force One during...</td>\n",
       "      <td>https://www.snopes.com/fact-check/air-force-on...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>Claim:\\n\\nClaim: Photographs show the results ...</td>\n",
       "      <td>https://www.snopes.com/fact-check/moose-story/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>Claim: PlayStation and Xbox announced that ref...</td>\n",
       "      <td>https://www.snopes.com/fact-check/cyberpunk-20...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>Claim:\\n\\nClaim: On-line coupon can be redeeme...</td>\n",
       "      <td>https://www.snopes.com/fact-check/jamba-juice-...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>Claim: U.S. President Joe Biden wore a hard ha...</td>\n",
       "      <td>https://www.snopes.com/fact-check/biden-wear-h...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>U.S. Rep Bobby Scott cited a staggering statis...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/may...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>Is West Virginia trailing its neighbors in sci...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2019/apr...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  A physical book detailing the contents of Hunt...   \n",
       "1  A video taken at a George Floyd protest in Den...   \n",
       "2  In September 2020, U.S. President Donald Trump...   \n",
       "3  A photograph shows Air Force One during U.S. P...   \n",
       "4           Photographs show the results of a car vs   \n",
       "5  PlayStation and Xbox announced that refunds wo...   \n",
       "6  On-line coupon can be redeemed for a free smoo...   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...   \n",
       "9  \"Our state has fewer science, technology, engi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Claim: A physical book detailing the contents ...   \n",
       "1  Claim: A video taken at a George Floyd protest...   \n",
       "2  Claim: In September 2020, U.S. President Donal...   \n",
       "3  Claim: A photograph shows Air Force One during...   \n",
       "4  Claim:\\n\\nClaim: Photographs show the results ...   \n",
       "5  Claim: PlayStation and Xbox announced that ref...   \n",
       "6  Claim:\\n\\nClaim: On-line coupon can be redeeme...   \n",
       "7  Claim: U.S. President Joe Biden wore a hard ha...   \n",
       "8  U.S. Rep Bobby Scott cited a staggering statis...   \n",
       "9  Is West Virginia trailing its neighbors in sci...   \n",
       "\n",
       "                                                 url normalised_rating  \n",
       "0  https://www.snopes.com/fact-check/hunter-biden...              true  \n",
       "1  https://www.snopes.com/fact-check/dpd-car-preg...              true  \n",
       "2  https://www.snopes.com/fact-check/trump-execut...              true  \n",
       "3  https://www.snopes.com/fact-check/air-force-on...              true  \n",
       "4     https://www.snopes.com/fact-check/moose-story/              true  \n",
       "5  https://www.snopes.com/fact-check/cyberpunk-20...              true  \n",
       "6  https://www.snopes.com/fact-check/jamba-juice-...              true  \n",
       "7  https://www.snopes.com/fact-check/biden-wear-h...              true  \n",
       "8  https://www.politifact.com/factchecks/2011/may...              true  \n",
       "9  https://www.politifact.com/factchecks/2019/apr...              true  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_factchecks_df=sampled_clean_factors_df[['claim','article','url','normalised_rating']]\n",
    "sampled_factchecks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e73c8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many articles failed to fetch\n",
    "no_article_df = sampled_factchecks_df[sampled_factchecks_df['article'].str.contains(\"Failed to fetch article content \", na=False)]\n",
    "no_article_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd9ae343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_factchecks_df.to_csv(\"Data/sample_factchecks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "### Generating short justifications and connecting the verdict as in the Eurpean Fact Checking Project\n",
    "Generate short justifications for the original verdict, based upon the article and the given Normalized rating (verdict).\n",
    "use GPT5, often regarded as best model for various tasks:\n",
    "- https://artificialanalysis.ai/leaderboards/models\n",
    "- https://www.vellum.ai/llm-leaderboard?utm_source=google&utm_medium=organic\n",
    "- https://www.shakudo.io/blog/top-9-large-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>normalised_rating</th>\n",
       "      <th>short_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A physical book detailing the contents of Hunt...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the nonprofit group Marco Polo cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A video taken at a George Floyd protest in Den...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article confirms a video show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September 2020, U.S. President Donald Trump...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because Trump said at a rally, \"Maybe I'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A photograph shows Air Force One during U.S. P...</td>\n",
       "      <td>true</td>\n",
       "      <td>FALSE because the photograph was taken in 2004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographs show the results of a car vs</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the article provides evidence, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PlayStation and Xbox announced that refunds wo...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because Sony PlayStation and Microsoft Xb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>On-line coupon can be redeemed for a free smoo...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because a Jamba Juice Company Customer Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S. President Joe Biden wore a hard hat backw...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the hard hat's suspension was con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Nearly 2,000 high schools - roughly 12 percen...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because the claim is supported by a 2007 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Our state has fewer science, technology, engi...</td>\n",
       "      <td>true</td>\n",
       "      <td>TRUE because West Virginia conferred the small...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim normalised_rating  \\\n",
       "0  A physical book detailing the contents of Hunt...              true   \n",
       "1  A video taken at a George Floyd protest in Den...              true   \n",
       "2  In September 2020, U.S. President Donald Trump...              true   \n",
       "3  A photograph shows Air Force One during U.S. P...              true   \n",
       "4           Photographs show the results of a car vs              true   \n",
       "5  PlayStation and Xbox announced that refunds wo...              true   \n",
       "6  On-line coupon can be redeemed for a free smoo...              true   \n",
       "7  U.S. President Joe Biden wore a hard hat backw...              true   \n",
       "8  \"Nearly 2,000 high schools - roughly 12 percen...              true   \n",
       "9  \"Our state has fewer science, technology, engi...              true   \n",
       "\n",
       "                                   short_explanation  \n",
       "0  TRUE because the nonprofit group Marco Polo cr...  \n",
       "1  TRUE because the article confirms a video show...  \n",
       "2  TRUE because Trump said at a rally, \"Maybe I'l...  \n",
       "3  FALSE because the photograph was taken in 2004...  \n",
       "4  TRUE because the article provides evidence, in...  \n",
       "5  TRUE because Sony PlayStation and Microsoft Xb...  \n",
       "6  TRUE because a Jamba Juice Company Customer Se...  \n",
       "7  TRUE because the hard hat's suspension was con...  \n",
       "8  TRUE because the claim is supported by a 2007 ...  \n",
       "9  TRUE because West Virginia conferred the small...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "#low temperature for more factual answers, \n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.2 )\n",
    "\n",
    "SYS = \"\"\"You are a careful fact-checking assistant.\n",
    "Write ONE or TWO concise sentences (≤50 words) that justify the given VERDICT using only evidence in the ARTICLE.\n",
    "No outside facts, speculation, or bullet lists, focus on why the article is TRUE, FALSE, MOSTLY TRUE, MOSTLY FALSE, or UNCHECKABLE.\n",
    "Start with the verdict in capitals, start with \"TRUE because\", \"FALSE because\", \"MOSTLY TRUE because\", \"MOSTLY FALSE because\", or \"UNCHECKABLE because\".\n",
    "If the Normalised rating is TRUE or FALSE, the verdict must be TRUE or FALSE.\n",
    "If the Normalised rating is PARTIALLY TRUE or MISLEADING, the verdict must be MOSTLY TRUE or MOSTLY FALSE.\n",
    "If the article does not provide enough information to justify the verdict, say so.\n",
    "\n",
    " Examples:\n",
    "    “The Imperial College London study estimated Omicron's reinfection risk is 5.4 times higher than Delta's, indicating faster spread. \n",
    "    South Africa's data showed higher reinfection risk with Omicron. However, the evidence is preliminary and not peer-reviewed, \n",
    "    so it's mostly true.” → MOSTLY TRUE\n",
    "    “The World Bank does not have a single 'climate fund', and Oxfam's Climate Finance Unchecked report (2017-2023) found no missing funds \n",
    "    but rather a lack of tracking for project components labeled 'climate finance' during approval. \n",
    "    The claim originated from a social media post by Brian Tamaki, which was debunked by AAP FactCheck using Oxfam's report data.\n",
    "    [0] Charity report did not find World Bank climate change fund was missing $41 billion” → FALSE\"\"\"\n",
    "\n",
    "def infer_verdict_and_expl(claim: str, article: str, normalised_rating: str):\n",
    "    if not isinstance(article, str) or \"Failed to fetch article content\" in article:\n",
    "        return None\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nVERDICT: {normalised_rating}\\n\\nARTICLE:\\n\"\"\" {article} \"\"\"')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        return \" \".join(text.split())  # collapse whitespace\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Apply to DataFrame (expects columns: 'claim', 'article', 'normalised_rating')\n",
    "factchecks_df = pd.read_csv(\"Data/sample_factchecks.csv\")\n",
    "factchecks_df.loc[:, \"short_explanation\"] = factchecks_df.apply(\n",
    "    lambda r: infer_verdict_and_expl(r[\"claim\"], r[\"article\"], r[\"normalised_rating\"]),axis=1\n",
    ")\n",
    "\n",
    "factchecks_df[[\"claim\", \"normalised_rating\", \"short_explanation\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154ca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results to a csv file\n",
    "\n",
    "factchecks_df.to_csv(\"Data/factchecks_with_verdicts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(factchecks_df[\"article\"][2])\n",
    "print(factchecks_df[\"short_explanation\"][2 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1a44e",
   "metadata": {},
   "source": [
    "## Create JSONL messages for finetuning\n",
    "Next, create messages containing a claim, a verdict, and an explanation, then add Socratic questions to encourage critical thinking and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97754686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4995 JSON objects to Data\\socratic_questions.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "#low temperature for more factual answers,\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.2 )\n",
    "\n",
    "SYS = \"\"\"You are given a fact-check CLAIM and its justification as SHORT_EXPLANATION explaining why it is labeled as true, false, mostly true, \n",
    "mostly false, or uncheckable. Your task is to generate five Socratic questions that probe the justification and verdict. The goal is to \n",
    "challenge the reasoning, surface blind spots, and encourage deeper reflection, not to accept the explanation at face value. Since the output\n",
    " will be used to finetune an LLM that critiques the reasoning of a fact-checking model, ensure that your questions reflect the following principles:\n",
    "- Factuality – Do the claims rely on verifiable evidence? Could missing or weak evidence be questioned?\n",
    "- Objectivity – Is the reasoning neutral, or does it show bias? How could the framing be challenged?\n",
    "- Fairness – Are multiple perspectives considered? Is the reasoning applied consistently?\n",
    "- Transparency – Is the explanation clear about its sources and reasoning steps? What is hidden or assumed?\n",
    "- Hallucinations – Does the explanation risk introducing unsupported or invented information?\n",
    "- Strategies & Alternatives – Are there other ways to frame, investigate, or reason about the claim?\n",
    "\n",
    "When writing questions, draw from the following categories of Socratic questioning. Use them as inspiration to diversify your five questions \n",
    "(do not stick to just one category):\n",
    "\n",
    "Purpose – probe the aim or agenda.\n",
    "- What is your purpose right now?\n",
    "- Why are you writing this?\n",
    "- What do you want to persuade them of?\n",
    "- What is our central aim or task in this line of thought?\n",
    "\n",
    "Questions – probe the underlying questions.\n",
    "- I am not sure exactly what question you are raising. Could you explain it?\n",
    "- Is this question the best one to focus on, or is there a more pressing one?\n",
    "- What questions might we be failing to ask that we should be asking?\n",
    "\n",
    "Information – probe the evidence or data.\n",
    "- On what information are you basing that comment?\n",
    "- How do we know this information is accurate? How could we verify it?\n",
    "- Have we failed to consider any information or data we need to consider?\n",
    "\n",
    "Inferences & Conclusions – probe how the conclusion was drawn.\n",
    "- How did you reach that conclusion?\n",
    "- Could you explain your reasoning?\n",
    "- Is there an alternative plausible conclusion?\n",
    "\n",
    "Concepts & Ideas – probe key ideas being applied.\n",
    "- What is the main idea you are using in your reasoning?\n",
    "- Are we using the appropriate concept, or do we need to reconceptualize the problem?\n",
    "- Do we need more facts, or do we need to rethink how we are labeling the facts?\n",
    "\n",
    "Assumptions – probe what is taken for granted.\n",
    "- What exactly are you taking for granted here?\n",
    "- Why are you assuming that? Shouldn’t we rather assume that…?\n",
    "- What alternative assumptions might we make?\n",
    "\n",
    "Implications & Consequences – probe what follows.\n",
    "- What are you implying when you say…?\n",
    "- If we do this, what is likely to happen as a result?\n",
    "- Have you considered the implications of this reasoning?\n",
    "- Viewpoints & Perspectives – probe alternative frames.\n",
    "\n",
    "From what point of view are you looking at this?\n",
    "- Is there another point of view we should consider?\n",
    "- Which of these possible viewpoints makes the most sense given the situation?\n",
    "\n",
    "Instructions:\n",
    "- Do not repeat the justification.\n",
    "- Do not state whether the verdict is correct.\n",
    "- Ask probing questions that challenge the reasoning, highlight blind spots, and open space for reconsideration.\n",
    "- Ensure the five questions you generate come from different categories where possible\n",
    "\n",
    "Output format (JSONL):\n",
    "{\n",
    "  \"claim\": \"the original claim\",\n",
    "  \"short_explanation\": \"the original short explanation\",\n",
    "  \"verdict\": \"The verdict as written in the explanation: true, false, mostly true, mostly false or uncheckable\",\n",
    "  \"questions\": [\n",
    "    \"What is our central aim or task in this line of thought?\",\n",
    "    \"What is the underlying question that this explanation is really trying to address?\",\n",
    "    \"How do we know this information is accurate, and how could we verify it?\",\n",
    "    \"Is there an alternative plausible conclusion based on the same reasoning?\",\n",
    "    \"Is there another point of view we should consider when evaluating this claim?\"\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def add_questions(claim: str, short_explanation: str):\n",
    "    msgs = [\n",
    "        SystemMessage(content=SYS),\n",
    "        HumanMessage(content=f'CLAIM: {claim}\\nSHORT_EXPLANATION: {short_explanation}')\n",
    "    ]\n",
    "    try:\n",
    "        resp = llm.invoke(msgs)\n",
    "        text = getattr(resp, \"content\", str(resp)).strip()\n",
    "        one_line = \" \".join(text.split())\n",
    "\n",
    "        return one_line\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Load data and compute short_explanation as before ---\n",
    "factchecks_df = pd.read_csv(\"Data/factchecks_with_verdicts.csv\")\n",
    "\n",
    "# --- Generate JSONL lines and write them to a single file ---\n",
    "output_path = Path(\"Data/socratic_questions.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "valid_lines = []\n",
    "\n",
    "for _, row in factchecks_df.iterrows():\n",
    "    line = add_questions(row[\"claim\"], row[\"short_explanation\"])\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(line)  # parse the JSON string\n",
    "    except json.JSONDecodeError:\n",
    "        continue  # skip if the model output was not valid JSON\n",
    "\n",
    "    # Expand into one object per question\n",
    "    for q in obj.get(\"questions\", []):\n",
    "        new_obj = {\n",
    "            \"claim\": obj[\"claim\"],\n",
    "            \"short_explanation\": obj[\"short_explanation\"],\n",
    "            \"verdict\": obj[\"verdict\"],\n",
    "            \"question\": q,\n",
    "        }\n",
    "        valid_lines.append(new_obj)\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in valid_lines:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(valid_lines)} JSON objects to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
