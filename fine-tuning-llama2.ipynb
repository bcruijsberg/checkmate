{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dd3f8b",
   "metadata": {},
   "source": [
    "Check with this tutorials:\n",
    "https://www.youtube.com/watch?v=W_xh6qNSfAQ\n",
    "https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n",
    "\n",
    "some models:\n",
    "4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "- \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "- \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "- \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "- \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "- \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "- \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "- \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "- \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "- \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "- \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "- \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    " More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd77850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 12.0. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_id = 'unsloth/Meta-Llama-3.1-8B-bnb-4bit'\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d133a80",
   "metadata": {},
   "source": [
    "Load the data from the JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfbd60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': \"A physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50.\",\n",
       " 'short_explanation': \"TRUE because the nonprofit group Marco Polo created a physical book detailing the contents of Hunter Biden's laptop, which can be obtained for a $50 donation.\",\n",
       " 'verdict': 'TRUE',\n",
       " 'question': \"What is the purpose of creating a physical book from the contents of Hunter Biden's laptop, and how does it serve the public interest?\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the training data\n",
    "training_data = []\n",
    "with open(\"Data/socratic_questions.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        training_data.append(json.loads(line))\n",
    "\n",
    "ds = Dataset.from_list(training_data)\n",
    "ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ba3a2",
   "metadata": {},
   "source": [
    "One issue is this dataset has multiple columns. For Ollama and llama.cpp to function like a custom Assistant, we must only have 2 columns prompt and an output column.\n",
    "\n",
    "The template needs an PROMPT and OUTPUT field. Here we use Claim and the justification for the fact check combined as the prompt, and \"question\" for the OUTPUT field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b529f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a9a75cbef646a6a1204e6692470705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging columns:   0%|          | 0/4995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965cc23f17884e5faecaffbaea44ee65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to ShareGPT:   0%|          | 0/4995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': \"The claim is A physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50..\\nAnd the justification of the fact-check is TRUE because the nonprofit group Marco Polo created a physical book detailing the contents of Hunter Biden's laptop, which can be obtained for a $50 donation.\\n\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"What is the purpose of creating a physical book from the contents of Hunter Biden's laptop, and how does it serve the public interest?\"}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset_simple = to_sharegpt(\n",
    "    ds,\n",
    "    merged_prompt = \"[[The claim is {claim}.\\n]][[And the justification of the fact-check is {short_explanation}\\n]]\",\n",
    "    output_column_name = \"question\",\n",
    ")\n",
    "dataset_simple[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6868a3",
   "metadata": {},
   "source": [
    "Finally use `standardize_sharegpt`! to convert all `user`, `assistant` and `system` tags to OpenAI Hugging Face style: using `user` and `assistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60d9585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c0d2960cf4716b4ca8c5d72fe49ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=12):   0%|          | 0/4995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"The claim is A physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50..\\nAnd the justification of the fact-check is TRUE because the nonprofit group Marco Polo created a physical book detailing the contents of Hunter Biden's laptop, which can be obtained for a $50 donation.\\n\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"What is the purpose of creating a physical book from the contents of Hunter Biden's laptop, and how does it serve the public interest?\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset_standard = standardize_sharegpt(dataset_simple)\n",
    "dataset_standard[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5b7fe",
   "metadata": {},
   "source": [
    "Next use a chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3203c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b4eab5bc19429b9814febea91a89c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_template = \"\"\"Below describes some details about a facty-checked claim.\n",
    "Ask a critical socratic question that would help to critically analyse the justification.\n",
    ">>> Claim and justification:\n",
    "{INPUT}\n",
    ">>> Critical socratic question:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    dataset_standard,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73747460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Add LoRA adapters (include embed_tokens + lm_head if using base model)\n",
    "target_modules = [\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "    \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "] \n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    target_modules=target_modules,\n",
    "    lora_alpha=64*2,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eba51e",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71be9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37147b7fd2574eefbdc8869289af6c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,995 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 167,772,160 of 8,198,033,408 (2.05% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.774600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.909100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.2751634240150451, metrics={'train_runtime': 200.2012, 'train_samples_per_second': 2.398, 'train_steps_per_second': 0.3, 'total_flos': 5503717509955584.0, 'train_loss': 1.2751634240150451, 'epoch': 0.09607686148919135})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "#pretokenize the dataset (faster training)\n",
    "def tokenize_fn(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"]\n",
    "    return out\n",
    "tok_ds = dataset.map(tokenize_fn, batched=True, num_proc=1)\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(  # supervised fine-tuning trainer\n",
    "    model = model,\n",
    "    train_dataset = tok_ds,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text_field = None,\n",
    "    max_seq_length = 2048,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-4,  #when the learning rate is set to 2e-4 it overshoots\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abe220",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8276505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From what point of view are we evaluating the claim that a physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50, and are there other perspectives, such as that of the seller or the buyers, that we should consider?<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The claim is A physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50. \\n\"\\\n",
    "                                \"And the justification of the fact-check is This claim is true. On June 1, 2023, the nonprofit group Marco Polo, which was founded by Donald Trump White House aide Garrett Ziegler, uploaded a large selection of content from a laptop owned and abandoned by U.S. President Joe Biden's son Hunter. He had left it at a Delaware computer repair shop in October 2019.\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a7caa",
   "metadata": {},
   "source": [
    "Save the lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff1af8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model\\\\tokenizer_config.json',\n",
       " 'lora_model\\\\special_tokens_map.json',\n",
       " 'lora_model\\\\chat_template.jinja',\n",
       " 'lora_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1dc1a",
   "metadata": {},
   "source": [
    "Load model first before Inference (Set to False if it is already loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f16ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 20:29:27.996000 19296 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\OneDrive\\Documenten\\OU\\Graduation\\checkmate\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 12.0. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How did the conclusion that the claim is true come to be reached, and are there any alternative plausible conclusions that could be drawn from the same evidence?<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "if True: # Load model first before Inference (Set to False if it is already loaded)\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "pass\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The claim is A physical book detailing the contents of Hunter Biden's abandoned laptop is being sold for $50. \\n\"\\\n",
    "                                \"And the justification of the fact-check is This claim is true. On June 1, 2023, the nonprofit group Marco Polo, which was founded by Donald Trump White House aide Garrett Ziegler, uploaded a large selection of content from a laptop owned and abandoned by U.S. President Joe Biden's son Hunter. He had left it at a Delaware computer repair shop in October 2019.\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19191f76",
   "metadata": {},
   "source": [
    "### Exporting to Ollama\n",
    "\n",
    "Before you do this, you need to install llama.cpp. \n",
    "- On a windows computer run: `winget install llama.cpp`\n",
    "- Next check where windows installed llama.cpp: `where llama-quantize.exe`\n",
    "- Copy `llama-quantize.exe` to `llama.cpp `in your project folder\n",
    "\n",
    "if the `lama.cpp` folder already exists first run in Powershell:\n",
    "`if (Test-Path \".\\llama.cpp\") {Remove-Item \".\\llama.cpp\" -Recurse -Force}`\n",
    "Next copy the `lama-quantize` file to `llama.cpp`\n",
    "\n",
    "`q4_k_m` doesn't work with the code below, run the `f16` option and manualy in powershel convert it into `q4_k_m`:\n",
    "- `# Pick your 16-bit GGUF file name:`\n",
    "- `$IN  = \"model\\unsloth.BF16.gguf\"   # adjust to your actual file`\n",
    "- `$OUT = \"model\\unsloth.q4_k_m.gguf\"`\n",
    "\n",
    "- `.\\llama-quantize.exe \"$IN\" \"$OUT\" q4_k_m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3) # Wait for a few seconds for Ollama to load!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e77de5",
   "metadata": {},
   "source": [
    "Finally load the model in ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e19ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 0% ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 0% ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 3% ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 5% ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 7% ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 9% ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 12% ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 14% ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 16% ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 19% ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 20% ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 23% ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 25% ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 26% ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 28% ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 31% ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 32% ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 35% ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 38% ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 39% ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 42% ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 45% ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 47% ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 49% ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 51% ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 53% ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 55% ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 57% ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 58% ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 61% ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 64% ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 66% ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 68% ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 71% ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 72% ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 75% ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 78% ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 79% ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 81% ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 84% ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 85% ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 88% ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 91% ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 93% ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 96% ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 99% ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:0f9b0553e6e665b9abc04248ba162d5291a5085c7d7e26e8acbe4fd522c73df3 \u001b[K\n",
      "creating new layer sha256:e83a2ef0a3a5e08868887e01bc2700b5b06613f8b2a76666e7d30596bc229adc \u001b[K\n",
      "creating new layer sha256:cd8900fcbf07e5d96dce6759ce9cf50746f1e17c457cadad46a1b0bfacefbd61 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create unsloth_llama_q4_k_m -f ./model/Modelfile_q4_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928e6933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           ID              SIZE      MODIFIED     \n",
      "unsloth_llama_q4_k_m:latest    354540974481    4.9 GB    41 hours ago    \n",
      "unsloth_model:latest           4cf99a75cee6    8.5 GB    41 hours ago    \n",
      "llama3:8b                      365c0bd3c000    4.7 GB    10 days ago     \n",
      "qwen3:4b                       e55aed6fe643    2.5 GB    2 weeks ago     \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
