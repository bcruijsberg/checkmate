{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Vectorstore\n",
    "\n",
    "Huggingface embeddings is used, since you don't need an API key\n",
    "\n",
    "FAISS is used for the vectorstore, which is stored locally.\n",
    "\n",
    "First, create a vector store with embeddings from the excell with all checked facts (FACTor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import numpy as np\n",
    "\n",
    "# --- load ---\n",
    "df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# avoid NaN -> empty string\n",
    "df = df.replace({np.nan: \"\"})\n",
    "\n",
    "def to_doc(row):\n",
    "    # IMPORTANT: do NOT put URL in page_content\n",
    "    page = (\n",
    "        f\"Title: {row['title']}\\n\"\n",
    "        f\"Claim: {row['claim']}\\n\"\n",
    "        f\"Date published: {row['date_published']}\\n\"\n",
    "        f\"Author: {row['author']}\\n\"\n",
    "        f\"Organisation: {row['organisation']}\\n\"\n",
    "        f\"Original Verdict: {row['original_verdict']}\\n\"\n",
    "        f\"Normalized Rating: {row['normalised_rating']}\"\n",
    "    )\n",
    "    return Document(\n",
    "        page_content=page,\n",
    "        metadata={\n",
    "            # keep url authoritative in metadata only\n",
    "            \"url\": str(row.get(\"url\", \"\")).strip(),\n",
    "            # (optional) keep other fields here too for rendering\n",
    "            \"title\": str(row.get(\"title\", \"\")).strip(),\n",
    "            \"date_published\": str(row.get(\"date_published\", \"\")).strip(),\n",
    "            \"organisation\": str(row.get(\"organisation\", \"\")).strip(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "documents = [to_doc(row) for _, row in df.iterrows()]\n",
    "\n",
    "# split (metadata is preserved)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# embed + index\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cuda'},          # or 'cpu'\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more excelsheets with information on the organisations and authors publishing the claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load existing FAISS index \n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Load the csv files\n",
    "author_df = pd.read_csv(\"Data/author_stats.csv\")\n",
    "org_df = pd.read_csv(\"Data/org_stats.csv\")\n",
    "\n",
    "# Convert files into documents\n",
    "def process_sheet1(df):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        combined_text = (\n",
    "            f\"Title: {row['title']}\\n\"\n",
    "            f\"Claim: {row['claim']}\\n\"\n",
    "            f\"Date published: {row['date_published']}\\n\"\n",
    "            f\"Author: {row['author']}\\n\"\n",
    "            f\"Organisation: {row['organisation']}\\n\"\n",
    "            f\"Original Verdict: {row['original_verdict']}\\n\"\n",
    "            f\"Normalized Rating: {row['normalised_rating']}\"\n",
    "        )\n",
    "        documents.append(Document(page_content=combined_text))\n",
    "    return documents\n",
    "\n",
    "# --- Convert Sheet 2 (different structure, e.g., metadata and comments) ---\n",
    "def process_sheet2(df):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        combined_text = (\n",
    "            f\"Statement: {row['statement']}\\n\"\n",
    "            f\"Source: {row['source']}\\n\"\n",
    "            f\"Reviewer Comment: {row['comment']}\\n\"\n",
    "            f\"Published On: {row['published_date']}\"\n",
    "        )\n",
    "        documents.append(Document(page_content=combined_text))\n",
    "    return documents\n",
    "\n",
    "# --- Combine all new documents ---\n",
    "all_new_documents = process_sheet1(sheet1_df) + process_sheet2(sheet2_df)\n",
    "\n",
    "# --- Split into chunks ---\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(all_new_documents)\n",
    "\n",
    "# --- Add to vectorstore and save ---\n",
    "vectorstore.add_documents(split_docs)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
