{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis of a claim\n",
    "To Check on Checkability, the first workflow of the EUFactcheck programme will be followed (https://eufactcheck.eu/about-us/eufactcheck-flowchart/). The student will also be involved to provide more information and to check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\checkmate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "#from langchain_ollama import ChatOllama\n",
    "import tqdm as notebook_tqdm\n",
    "from tavily import TavilyClient\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Initialize Tavily client \n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "#os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "#os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "#os.environ[\"LANGSMITH_PROJECT\"]=\"pr-left-technician-100\"\n",
    "\n",
    "#low temperature for more factual answers,\n",
    "#llm = ChatOllama(model=\"qwen3:4b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "llm = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of all the nodes in Claim analysis\n",
    "\n",
    "These functions are all the nodes in the claim graph and also show the edges in case of conditional nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All the nodes \"\"\"\n",
    "\n",
    "from prompts import (\n",
    "    checkable_check_prompt,\n",
    "    confirmation_checkable_prompt,\n",
    "    get_information_prompt,\n",
    "    confirmation_clarification_prompt,\n",
    "    get_summary_prompt,\n",
    "    confirmation_check_prompt,\n",
    "    rag_queries_prompt,\n",
    "    match_check_prompt,\n",
    "    structure_claim_prompt,\n",
    "    identify_source_prompt,\n",
    "    source_location_prompt,\n",
    "    source_queries_prompt,\n",
    "    confirm_queries_prompt,\n",
    "    select_primary_source_prompt,\n",
    "    search_queries_prompt,\n",
    "    iterate_search_prompt,\n",
    "    get_socratic_question,\n",
    ")\n",
    "from state_scope import (\n",
    "    AgentStateClaim, \n",
    "    SubjectResult, \n",
    "    MoreInfoResult, \n",
    "    SummaryResult, \n",
    "    ConfirmationResult,\n",
    "    ConfirmationFinalResult,\n",
    "    ConfirmationMatch,\n",
    "    ClaimMatchingOutput,\n",
    "    GetSource, \n",
    "    GetSourceLocation,\n",
    "    GetSearchQueries,\n",
    "    PrimarySourceSelection, \n",
    ")\n",
    "from langgraph.types import Overwrite\n",
    "from langchain_core.messages import BaseMessage,HumanMessage,AIMessage, ToolMessage, get_buffer_string\n",
    "from typing import List, Dict, Any, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command, Send\n",
    "from utils import get_new_user_reply,_domain\n",
    "from tooling import llm, llm_tools, llm_tuned, tools_dict\n",
    "import json\n",
    "\n",
    "\n",
    "# Maximum number of messages to send to the prompt\n",
    "MAX_HISTORY_MESSAGES = 6\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ROUTER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# def router(state: AgentStateClaim) -> Command[\n",
    "#     Literal[\n",
    "#         \"checkable_fact\",\n",
    "#         \"checkable_confirmation\",\n",
    "#         \"retrieve_information\",\n",
    "#         \"clarify_information\",\n",
    "#         \"produce_summary\",\n",
    "#         \"critical_question\",\n",
    "#         \"get_confirmation\",\n",
    "#         \"get_rag_queries\",\n",
    "#         \"confirm_rag_queries\",\n",
    "#         \"rag_retrieve_worker\",\n",
    "#         \"reduce_rag_results\",\n",
    "#         \"structure_claim_matching\",\n",
    "#         \"match_or_continue\",\n",
    "#         \"get_source\",\n",
    "#         \"get_location_source\",\n",
    "#         \"get_source_queries\",\n",
    "#         \"confirm_search_queries\",\n",
    "#         \"select_primary_source\",\n",
    "#         \"iterate_search\",\n",
    "#         ]\n",
    "# ]:\n",
    "#     \"\"\" Route to correct node, after user reply \"\"\"\n",
    "\n",
    "#     return Command(\n",
    "#         goto=state.get(\"next_node\") or \"checkable_fact\"\n",
    "#         )\n",
    "\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "# # CRITICAL QUESTION NODE\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def critical_question(state: AgentStateClaim) -> Command[Literal[\"checkable_confirmation\",\"get_confirmation\"]]:\n",
    "\n",
    "    \"\"\" Ask a socratic question to make the user think about the consequences of a fact checking a claim \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # retrieve conversation history fact-check messages and critical messages\n",
    "    conversation_history_critical = list(state.get(\"messages_critical\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    messages_critical_str = get_buffer_string(conversation_history_critical[-MAX_HISTORY_MESSAGES:] )\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_socratic_question.format(\n",
    "        alerts=alerts_str,\n",
    "        claim=state.get(\"claim\"),\n",
    "        summary=state.get(\"summary\"),\n",
    "        messages_critical=messages_critical_str \n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = llm_tuned.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=result.content)\n",
    "  \n",
    "    return Command(\n",
    "        goto=state.get(\"next_node\"),\n",
    "            update={\n",
    "                \"critical_question\": result.content,\n",
    "                \"messages_critical\": [ai_chat_msg],\n",
    "                \"next_node\": None\n",
    "            }        \n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CHECKABLE_FACT NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def checkable_fact(state: AgentStateClaim) -> Command[Literal[\"checkable_confirmation\"]]:\n",
    "\n",
    "    \"\"\" Check if a claim is potentially checkable. \"\"\"\n",
    "\n",
    "    #Retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:] \n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(SubjectResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = checkable_check_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        additional_context=state.get(\"additional_context\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # checkable is a boolean in State\n",
    "    is_checkable = result.checkable == \"POTENTIALLY CHECKABLE\"\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    explanation_text = (\n",
    "        f\"**Checkability analysis**\\n\"\n",
    "        f\"- Checkable: `{result.checkable}`\\n\"\n",
    "        f\"- Reason: {result.explanation}\\n\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=explanation_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=\"checkable_confirmation\",\n",
    "        update={\n",
    "            \"question\": result.question,\n",
    "            \"checkable\": is_checkable,\n",
    "            \"explanation\": result.explanation,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": \"checkable_confirmation\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CHECKABLE_CONFIRMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def checkable_confirmation(state: AgentStateClaim) -> Command[Literal[\"retrieve_information\",\"__end__\",\"checkable_fact\"]]:\n",
    "    \n",
    "    \"\"\" Get confirmation from user on the gathered information. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", \"\"))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"checkable_confirmation\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "        # Get user reply, if the last message was a user message\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = confirmation_checkable_prompt.format(\n",
    "            claim=state.get(\"claim\", \"\"),\n",
    "            checkable=state.get(\"checkable\", \"\"),\n",
    "            explanation=state.get(\"explanation\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We'll continue with this claim.\"\n",
    "        else:\n",
    "            confirm_text = \"Okay let's revise the claim or stop here.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            if state.get(\"checkable\"):\n",
    "                return Command(\n",
    "                        goto=\"retrieve_information\", \n",
    "                        update={\n",
    "                            \"confirmed\": result.confirmed,\n",
    "                            \"messages\": [ai_chat_msg],\n",
    "                            \"awaiting_user\": False,\n",
    "                            \"additional_context\": None,\n",
    "                            \"next_node\": None\n",
    "                        }\n",
    "                )   \n",
    "            else: \n",
    "                # user confirmed but claim is not checkable → end\n",
    "                end_msg = AIMessage(content=\"This claim appears to be uncheckable, so we'll stop the process here.\")\n",
    "                return Command(\n",
    "                        goto=END, \n",
    "                        update={\n",
    "                            \"confirmed\": result.confirmed,\n",
    "                            \"messages\": [ai_chat_msg] + [end_msg],\n",
    "                            \"awaiting_user\": False,\n",
    "                            \"next_node\": None\n",
    "                        }\n",
    "                )   \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"checkable_fact\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"awaiting_user\": False,\n",
    "                        \"additional_context\": user_answer,\n",
    "                        \"next_node\": None\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE_INFORMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def retrieve_information(state: AgentStateClaim) -> Command[Literal[\"clarify_information\"]]:\n",
    "\n",
    "    \"\"\" Gather more information about a potentially checkable claim. \"\"\"\n",
    "\n",
    "    #Retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(MoreInfoResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_information_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        additional_context=state.get(\"additional_context\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    details_text = (\n",
    "        \"**Here’s what I extracted from your claim:**\\n\"\n",
    "        f\"- Subject: {result.subject or 'not clearly specified'}\\n\"\n",
    "        f\"- Quantitative: {result.quantitative}\\n\"\n",
    "        f\"- Precision: {result.precision}\\n\"\n",
    "        f\"- Based on: {result.based_on}\\n\"\n",
    "    )\n",
    "\n",
    "    if result.alerts:\n",
    "        details_text += \"\\n**Missing / to verify:**\\n\" + \"\\n\".join(f\"- {a}\" for a in result.alerts)\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=details_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=\"clarify_information\", \n",
    "        update={\n",
    "            \"subject\": result.subject,\n",
    "            \"quantitative\": result.quantitative,\n",
    "            \"precision\": result.precision,\n",
    "            \"based_on\": result.based_on,\n",
    "            \"question\": result.question,\n",
    "            \"alerts\": result.alerts or [],\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": None,\n",
    "        }\n",
    "    )   \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CLARIFY_INFORMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def clarify_information(state: AgentStateClaim) -> Command[Literal[\"produce_summary\", \"retrieve_information\"]]:\n",
    "\n",
    "    \"\"\" Get confirmation from user on the gathered information. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", \"\"))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"clarify_information\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        #retrieve alerts and format to string for the prompt\n",
    "        alerts=state.get(\"alerts\", [])\n",
    "        alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirmation_clarification_prompt.format(\n",
    "            subject=state.get(\"subject\", \"\"),\n",
    "            quantitative=state.get(\"quantitative\", \"\"),\n",
    "            precision=state.get(\"precision\", \"\"),\n",
    "            based_on=state.get(\"based_on\", \"\"),\n",
    "            claim=state.get(\"claim\", \"\"),\n",
    "            question=state.get(\"question\", \"\"),\n",
    "            alerts=alerts_str,\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"Thanks, I’ll use this information to draft the summary.\"\n",
    "        else:\n",
    "            confirm_text = \"Let’s collect a bit more information.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            return Command(\n",
    "                    goto=\"produce_summary\", \n",
    "                    update={\n",
    "                        \"confirmed\": result.confirmed,\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"additional_context\": None,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"retrieve_information\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"additional_context\": user_answer,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# PRODUCE SUMMARY NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def produce_summary(state: AgentStateClaim) -> Command[Literal[\"get_confirmation\"]]:\n",
    "\n",
    "    \"\"\" Get a summary on the gathered information. \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(SummaryResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_summary_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        quantitative=state.get(\"quantitative\", \"\"),\n",
    "        precision=state.get(\"precision\", \"\"),\n",
    "        based_on=state.get(\"based_on\", \"\"),\n",
    "        alerts=alerts_str,\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    summary_text = (\n",
    "        f\"**Summary of our findings so far:**\\n\\n{result.summary}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=summary_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command( \n",
    "            goto=\"get_confirmation\",\n",
    "            update={\n",
    "                \"summary\": result.summary,\n",
    "                \"question\": result.question,\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"subject\": result.subject,\n",
    "                \"quantitative\": result.quantitative,\n",
    "                \"precision\": result.precision,\n",
    "                \"based_on\": result.based_on,\n",
    "                \"alerts\": result.alerts or [],\n",
    "                \"claim_source\": result.claim_source,\n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"get_confirmation\",\n",
    "            }\n",
    "    )       \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GET_CONFIRMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "   \n",
    "def get_confirmation(state: AgentStateClaim) -> Command[Literal[\"produce_summary\", \"get_rag_queries\"]]:\n",
    "\n",
    "    \"\"\" Get confirmation from user on the gathered information.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", []))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"get_confirmation\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationFinalResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirmation_check_prompt.format(\n",
    "            summary=state.get(\"summary\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"Let's check if this claim has already been researched\"\n",
    "        else:\n",
    "            confirm_text = \"Let's revisit the summary and adjust it if needed.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            return Command(\n",
    "                    goto=\"get_rag_queries\", \n",
    "                    update={\n",
    "                        \"confirmed\": result.confirmed,\n",
    "                        \"subject\": result.subject,\n",
    "                        \"quantitative\": result.quantitative,\n",
    "                        \"precision\": result.precision,\n",
    "                        \"based_on\": result.based_on,\n",
    "                        \"question\": result.question,\n",
    "                        \"alerts\": result.alerts or [],\n",
    "                        \"claim_source\": result.claim_source,\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"produce_summary\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"subject\": result.subject,\n",
    "                        \"quantitative\": result.quantitative,\n",
    "                        \"precision\": result.precision,\n",
    "                        \"based_on\": result.based_on,\n",
    "                        \"question\": result.question,\n",
    "                        \"alerts\": result.alerts or [],\n",
    "                        \"claim_source\": result.claim_source,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )\n",
    "  \n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES FOR CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_rag_queries(state: AgentStateClaim) -> Command[Literal[\"confirm_rag_queries\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = rag_queries_prompt.format(\n",
    "        summary=state.get(\"summary\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"confirm_rag_queries\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "        }\n",
    "    )    \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CONFIRM CLAIM MATCHING QUERIES NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def confirm_rag_queries(state: AgentStateClaim) -> dict:\n",
    "    \n",
    "    \"\"\"Update state based on user confirmation. Routing is handled by a router.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        # pause for user\n",
    "        return {\n",
    "            \"next_node\": \"confirm_rag_queries\",\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # retrieve search_queries and format to string for the prompt\n",
    "        search_queries = state.get(\"search_queries\", [])\n",
    "        search_queries_str= \"\\n\".join(f\"- {a}\" for a in search_queries)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirm_queries_prompt.format(\n",
    "            search_queries=search_queries_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "         # human-readable assistant message for the chat\n",
    "        queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We search the Claims database with these queries.\\n\" + queries_text\n",
    "        else:\n",
    "            confirm_text = (\n",
    "                \"Is there anything else you would like to change about the search queries?\\n\"\n",
    "                f\"{queries_text}\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return {\n",
    "            \"confirmed\": result.confirmed,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": (not result.confirmed),\n",
    "        }\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ROUTER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def route_rag_confirm(state: AgentStateClaim):\n",
    "\n",
    "    \"\"\" Route based on user confirmation of RAG queries. \"\"\"\n",
    "\n",
    "    # retrieve search queries\n",
    "    q = state.get(\"search_queries\", [])\n",
    "    if state.get(\"next_node\") == \"confirm_rag_queries\":\n",
    "        return \"__end__\"\n",
    "\n",
    "    # If not confirmed, go back to get_rag_queries\n",
    "    if not state.get(\"confirmed\", False):\n",
    "        return \"confirm_rag_queries\"\n",
    "\n",
    "    # If confirmed, proceed to retrieval\n",
    "    return [\n",
    "        Send(\"rag_retrieve_worker\", {\"current_query\": q})\n",
    "        for q in state.get(\"search_queries\", [])\n",
    "        if q\n",
    "    ]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# WORKER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "async def rag_retrieve_worker(state: AgentStateClaim) -> Dict[str, Any]:\n",
    "    \"\"\" Perform retrieval for a given query. \"\"\"\n",
    "\n",
    "    # Get the current query from state\n",
    "    q = state[\"current_query\"]\n",
    "\n",
    "    # Call the retriever tool\n",
    "    retriever_tool = tools_dict[\"retriever_tool\"] \n",
    "    out = await retriever_tool.ainvoke({\"query\": q, \"k\": 10})  \n",
    "\n",
    "    # Return the RAG trace entry\n",
    "    return {\n",
    "        \"rag_trace\": [{\n",
    "            \"tool_name\": \"retriever_tool\",\n",
    "            \"args\": {\"query\": q, \"k\": 10},\n",
    "            \"output\": out,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# REDUCER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reduce_rag_results(state: AgentStateClaim) -> Command[Literal[\"structure_claim_matching\"]]:\n",
    "\n",
    "    \"\"\" Reduce the RAG retrieval results into a single trace. \"\"\"\n",
    "\n",
    "    # Retrieve RAG traces from state\n",
    "    rag_trace = state.get(\"rag_trace\", [])\n",
    "\n",
    "    # Combine all RAG traces into one\n",
    "    return Command(\n",
    "        goto=\"structure_claim_matching\",\n",
    "        update={\n",
    "            \"tool_trace\": rag_trace,   # so your structure_claim_matching stays the same\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# STRUCTURE CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def structure_claim_matching(state: AgentStateClaim) -> Command[Literal[\"match_or_continue\",\"get_source\"]]:\n",
    "   \n",
    "    \"\"\"Take the raw retrieval trace and turn it into structured output.\"\"\"\n",
    "\n",
    "      # Use structured output\n",
    "    structured_llm = llm.with_structured_output(ClaimMatchingOutput, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = structure_claim_prompt.format(\n",
    "        summary=state.get(\"summary\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        tool_trace=state.get(\"tool_trace\", \"\"),\n",
    "    )\n",
    "\n",
    "    # This returns an instance of ClaimMatchingOutput already parsed\n",
    "    result = structured_llm.invoke(prompt)\n",
    " \n",
    "    # Build a human-readable message for the chat UI\n",
    "    explanation_lines = []\n",
    "    explanation_lines.append(\"**Claim matching results**\")\n",
    "    explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    # # Show the queries + reasoning\n",
    "    # if result.queries:\n",
    "    #     explanation_lines.append(\"Here are the search questions that were used (or would be appropriate) to look for similar claims:\")\n",
    "    #     for i, q in enumerate(result.queries, start=1):\n",
    "    #         explanation_lines.append(f\"- **Q{i}:** {q.query}\")\n",
    "    #         explanation_lines.append(f\"  - Why this query: {q.reasoning}\")\n",
    "    #     explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    # Show the top claims\n",
    "    if result.top_claims:\n",
    "        explanation_lines.append(\"From the retrieved information, these existing claims might be relevant to what you're investigating:\")\n",
    "        for i, c in enumerate(result.top_claims, start=1):\n",
    "            # URL line only if present\n",
    "            url_part = f\"\\n  - {c.allowed_url}\" if c.allowed_url else \"\"\n",
    "            explanation_lines.append(f\"- **Claim {i}:** {c.short_summary}{url_part}\")\n",
    "            explanation_lines.append(f\"  - How it aligns or differs from your claim: {c.alignment_rationale}\")\n",
    "        explanation_lines.append(\"\")  # blank line\n",
    "    else:\n",
    "        explanation_lines.append(\"No strong matching existing claims were identified based on the retrieved information.\")\n",
    "        explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    explanation_text = \"\\n\".join(explanation_lines)\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=explanation_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    if result.top_claims:\n",
    "        return Command(\n",
    "            goto=\"match_or_continue\",\n",
    "            update={\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"claim_matching_result\": result, \n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"match_or_continue\",\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"get_source\", \n",
    "            update={\n",
    "                \"messages\": [ai_chat_msg],  \n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"get_source\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# MATCHED OR CONTUE RESEARCH NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def match_or_continue(state: AgentStateClaim) -> Command[Literal[\"get_source\", \"__end__\"]]:\n",
    "\n",
    "    \"\"\" Decide whether to continue researching or end the process if a matching claim was found.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Do any of these match your claim? Or do you want to continue researching as suggested?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"match_or_continue\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationMatch, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt =  match_check_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # # human-readable assistant message for the chat\n",
    "        if result.match:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    \"This claim appears to match an already researched claim. \"\n",
    "                    \"We can stop the process here.\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    \"No exact match found. Let's continue researching.\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Goto next node and update State\n",
    "        if result.match:\n",
    "            return Command(\n",
    "                    goto=\"__end__\", \n",
    "                    update={\n",
    "                        \"match\": result.match,\n",
    "                        \"messages\": [ai_chat_msg], \n",
    "                        \"awaiting_user\": False,\n",
    "                        \"next_node\": None\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"get_source\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],  \n",
    "                        \"awaiting_user\": True,\n",
    "                        \"next_node\": \"get_source\",\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE SOURCE, AND CHECK IF PRIMARY SOURCE KNOWN\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_source(state: AgentStateClaim) -> Command[Literal[\"get_location_source\"]]:\n",
    "\n",
    "    \"\"\" Ask the user for the  source of the claim if no match was found.\"\"\"\n",
    "    \n",
    "    # Retrieve the source of the claim\n",
    "    claim_source=state.get(\"claim_source\")\n",
    "    \n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        if claim_source:\n",
    "            ask_msg = AIMessage(content=\"Was {claim_source} the primary source (the first time this claim was made)?\")\n",
    "        elif claim_source is None or claim_source == \"\":\n",
    "            ask_msg = AIMessage(content=\"Do you know the source and is this the primary source (when it was first made)?\")\n",
    "\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"get_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSource, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = identify_source_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "            claim_source=claim_source,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        if result.primary_source:\n",
    "            question_text = (\n",
    "                f\"Great, we have the primary source as **{result.primary_source}**.\\n\\n\"\n",
    "                f\"Can you locate or provide a URL to the specific social media post, \"\n",
    "                f\"speech, or article where {result.primary_source} first made this claim? \"\n",
    "                f\"Or can you describe the source if you don't have a URL?\"\n",
    "            )\n",
    "        else:\n",
    "            question_text = (\n",
    "                \"I couldn't identify a primary source from that. \"\n",
    "                \"Can you locate or provide a URL to the specific social media post, \"\n",
    "                \"speech, or article where this claim was found? \"\n",
    "                \"Or can you describe the source if you don't have a URL?\"\n",
    "            )\n",
    "        ai_chat_msg = AIMessage(content=question_text)\n",
    "\n",
    "         # Goto next node and update State\n",
    "        return Command(\n",
    "                goto=\"get_location_source\", \n",
    "                update={\n",
    "                    \"claim_source\": result.claim_source,\n",
    "                    \"primary_source\": result.primary_source,\n",
    "                    \"messages\": [ai_chat_msg],\n",
    "                    \"awaiting_user\": True,\n",
    "                    \"next_node\": None,\n",
    "                }\n",
    "        ) \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE LOCATION OF SOURCE NODE, OR A DESCRIPTION\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_location_source(state: AgentStateClaim) -> Command[Literal[\"get_search_queries\",\"get_source_queries\"]]:\n",
    "\n",
    "    \"\"\" Ask the user where the claim was found. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"next_node\": \"get_location_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSourceLocation, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  = source_location_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        #check if the primary source is known\n",
    "        primary_source = state.get(\"primary_source\")\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if primary_source:\n",
    "            chat_text = (\n",
    "                f\"We will now continue researching the claim.\"\n",
    "            )\n",
    "        else:\n",
    "            chat_text = (\n",
    "                f\"We will now first try to locate the primary source (who made the claim first).\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if primary_source:\n",
    "            return Command(\n",
    "                goto=\"get_search_queries\", \n",
    "                update={\n",
    "                    \"claim_url\": result.claim_url,\n",
    "                    \"source_description\": result.source_description,\n",
    "                    \"messages\":  [ai_chat_msg],\n",
    "                    \"next_node\": None,\n",
    "                    \"awaiting_user\": False,\n",
    "                }\n",
    "            )    \n",
    "        else:\n",
    "            return Command(\n",
    "                goto=\"get_source_queries\",\n",
    "                update={\n",
    "                    \"claim_url\": result.claim_url,\n",
    "                    \"source_description\": result.source_description,\n",
    "                    \"messages\":  [ai_chat_msg],\n",
    "                    \"next_node\": None,\n",
    "                    \"awaiting_user\": False,\n",
    "                }\n",
    "            )  \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES TO LOCATE PRIMARY SOURCE NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_source_queries(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = source_queries_prompt.format(\n",
    "        messages=messages_str,\n",
    "        summary = state.get(\"summary\", \"\"),\n",
    "        claim = state.get(\"claim\", \"\"),\n",
    "        claim_source=state.get(\"claim_source\", \"\"),\n",
    "        claim_url = state.get(\"claim_url\", \"\"),\n",
    "        claim_description = state.get(\"claim_description\", \"\")\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"critical_question\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "            \"research_focus\": \"select_primary_source\",\n",
    "        }\n",
    "    )    \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CONFIRM SEARCH QUERIES NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def confirm_search_queries(state: AgentStateClaim) -> dict:\n",
    "    \n",
    "    \"\"\"Update state based on user confirmation. Routing is handled by a router.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        # pause for user\n",
    "        return {\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # retrieve search_queries and format to string for the prompt\n",
    "        search_queries = state.get(\"search_queries\", [])\n",
    "        search_queries_str= \"\\n\".join(f\"- {a}\" for a in search_queries)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirm_queries_prompt.format(\n",
    "            search_queries=search_queries_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "         # human-readable assistant message for the chat\n",
    "        queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We will continue searching.\"\n",
    "        else:\n",
    "            confirm_text = (\n",
    "                \"Is there anything else you would like to change about the search queries?\\n\"\n",
    "                f\"{queries_text}\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return {\n",
    "            \"confirmed\": result.confirmed,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": (not result.confirmed),\n",
    "        }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RESET THE SEARCH STATE NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reset_search_state(state: AgentStateClaim):\n",
    "    return {\n",
    "        \"tavily_context\": Overwrite([]),\n",
    "        }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES ORCHESTRATOR NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def route_after_confirm(state: AgentStateClaim):\n",
    "\n",
    "    \"\"\" Route based on user confirmation of search queries. \"\"\"\n",
    "\n",
    "    # If we need to stop and wait for user input\n",
    "    if state.get(\"next_node\") == \"confirm_search_queries\":\n",
    "        return \"__end__\"\n",
    "\n",
    "    # If user hasn't confirmed, loop\n",
    "    if not state.get(\"confirmed\", False):\n",
    "        return \"confirm_search_queries\"\n",
    "\n",
    "    # Confirmed => fan out to workers\n",
    "    return [\n",
    "        Send(\"find_sources_worker\", {\"current_query\": q})\n",
    "        for q in state.get(\"search_queries\", [])\n",
    "        if q\n",
    "    ]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES WORKER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "async def find_sources_worker(state: AgentStateClaim) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Worker: run Tavily for one query, return one compact result block.\"\"\"\n",
    "\n",
    "    # Run tavily search for the current query, get top 18 results\n",
    "    q = state[\"current_query\"]\n",
    "    tavily_tool = tools_dict.get(\"tavily_search\")\n",
    "\n",
    "    tool_output = await tavily_tool.ainvoke({\"query\": q, \"max_results\": 18})\n",
    "    out_dict = tool_output.model_dump() if hasattr(tool_output, \"model_dump\") else dict(tool_output)\n",
    "\n",
    "    compact = {\"query\": out_dict.get(\"query\", q), \"results\": []}\n",
    "\n",
    "    # Add the top 9 results only to the compact Dictionary\n",
    "    for r in (out_dict.get(\"results\") or []):\n",
    "        r = r.model_dump() if hasattr(r, \"model_dump\") else dict(r)\n",
    "        url = (r.get(\"url\") or \"\").strip()\n",
    "        if not url:\n",
    "            continue\n",
    "        compact[\"results\"].append({\"title\": r.get(\"title\"), \"url\": url})\n",
    "        if len(compact[\"results\"]) >= 9:\n",
    "            break\n",
    "\n",
    "    # Return and wrap in a list so it merges via operator.add\n",
    "    return {\"tavily_context\": [compact]}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES REDUCER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reduce_sources(state: AgentStateClaim) -> Command[Literal[\"select_primary_source\", \"iterate_search\"]]:\n",
    "\n",
    "    \"\"\"Merge worker outputs, enforce diversity across queries, build message.\"\"\"\n",
    "\n",
    "    # Retrieve all tavily results from the worker outputs\n",
    "    tavily_results = state.get(\"tavily_context\", [])\n",
    "\n",
    "    # remember used urls and domains to enforce diversity\n",
    "    used_urls = set()\n",
    "    used_domains = set()\n",
    "    final_blocks: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Deduplicate across queries, and set limit to 3 results per query\n",
    "    for block in tavily_results:\n",
    "        query = block.get(\"query\")\n",
    "        results = block.get(\"results\", [])\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        compact = {\"query\": query, \"results\": []}\n",
    "        for r in results:\n",
    "            url = (r.get(\"url\") or \"\").strip()\n",
    "            if not url:\n",
    "                continue\n",
    "            dom = _domain(url)\n",
    "\n",
    "            if url in used_urls or dom in used_domains:\n",
    "                continue\n",
    "\n",
    "            compact[\"results\"].append({\"title\": r.get(\"title\"), \"url\": url})\n",
    "            used_urls.add(url)\n",
    "            used_domains.add(dom)\n",
    "\n",
    "            if len(compact[\"results\"]) >= 3:\n",
    "                break\n",
    "\n",
    "        final_blocks.append(compact)\n",
    "\n",
    "    # Human-readable assistant message for the chat\n",
    "    lines = [\"Here are the top results I found for each search query:\", \"\"]\n",
    "    startnr = 1\n",
    "    \n",
    "    for block in final_blocks:\n",
    "        query = block.get(\"query\")\n",
    "        results = block.get(\"results\", [])\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        # Add query header and blank lines\n",
    "        lines.append(\"\") \n",
    "        lines.append(f\"**Query:** {query}\")\n",
    "        lines.append(\"\") \n",
    "\n",
    "        # List results\n",
    "        for i, r in enumerate(results, start=startnr):\n",
    "            title = r.get(\"title\")\n",
    "            url = r.get(\"url\")\n",
    "            lines.append(f\"{i}. **[{title}]({url})**\")\n",
    "        startnr += len(results)\n",
    "  \n",
    "    new_msgs = [AIMessage(content=\"\\n\".join(lines))]\n",
    "\n",
    "    # Decide next node based on research focus\n",
    "    research_focus = state.get(\"research_focus\")\n",
    "    state_next_node = \"select_primary_source\" if research_focus == \"select_primary_source\" else \"iterate_search\"\n",
    "    \n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=state_next_node,\n",
    "        update={\n",
    "            # overwrite with the finalized/deduped set for downstream nodes\n",
    "            \"tavily_context\": final_blocks,\n",
    "            \"messages\": new_msgs,\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": None,\n",
    "            \"search_queries\":[],\n",
    "        },\n",
    "    )\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# SELECT PRIMARY SOURCE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def select_primary_source(state: AgentStateClaim) -> Command[Literal[\"get_search_queries\"]]:\n",
    "\n",
    "    \"\"\" pick the best / most likely primary source. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Does any of these sources correspond to the primary source of the claim?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"select_primary_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Get the context and conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output \n",
    "        structured_llm = llm.with_structured_output(PrimarySourceSelection, method=\"json_mode\")\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = select_primary_source_prompt.format(\n",
    "            claim_source=state.get(\"claim_source\", \"\"),\n",
    "            claim_url=state.get(\"claim_url\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "            messages=messages_str,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # retrieve existing alerts\n",
    "        alerts = list(state.get(\"alerts\", []))\n",
    "\n",
    "        # human-readable assistant message\n",
    "        if result.primary_source:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    f\"Great, you have identified **{result.claim_source}** as the primary source of the claim. \"\n",
    "                    \"I'll proceed with the research.\"\n",
    "                )\n",
    "            )\n",
    "        else:   \n",
    "            # Add an alert if the primary source is not found\n",
    "            if not result.primary_source:\n",
    "                alerts.append(\"primary source not found\")\n",
    "                ai_chat_msg = AIMessage(\n",
    "                    content=(\n",
    "                        \"I couldn’t identify a clear primary source from these results. \"\n",
    "                        \"I'll continue with the research anyway, but note that the original source is still missing.\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return Command(\n",
    "            goto=\"get_search_queries\",\n",
    "            update={\n",
    "                \"primary_source\": result.primary_source,\n",
    "                \"claim_source\": result.claim_source,\n",
    "                \"claim_url\": result.claim_url,\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"alerts\": alerts,\n",
    "                \"next_node\": None,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES TO FALSIFY OR VERIFY CLAIM NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_search_queries(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = search_queries_prompt.format(\n",
    "        messages=messages_str,\n",
    "        alerts=alerts_str,\n",
    "        summary = state.get(\"summary\", \"\"),\n",
    "        claim = state.get(\"claim\", \"\"),\n",
    "        claim_source=state.get(\"claim_source\", \"\"),\n",
    "        claim_url = state.get(\"claim_url\", \"\"),\n",
    "        claim_description = state.get(\"claim_description\", \"\")\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"critical_question\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "            \"research_focus\": \"iterate_search\",\n",
    "        }\n",
    "    ) \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ASK TO ITERATE SEARCH NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def iterate_search(state: AgentStateClaim) -> Command[Literal[\"__end__\"]]:\n",
    "\n",
    "    \"\"\" pick the best / most likely primary source. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Do you want to search once more?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"iterate_search\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Get the context and conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output \n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = iterate_search_prompt.format(\n",
    "                    user_answer=user_answer,\n",
    "                    messages=messages_str,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # retrieve existing alerts\n",
    "        alerts = list(state.get(\"alerts\", []))\n",
    "\n",
    "        # human-readable assistant message\n",
    "        if result.confirmed:\n",
    "            ai_chat_msg = AIMessage(content=\"Let's search once more.\")\n",
    "        else:   \n",
    "            ai_chat_msg = AIMessage(content=\"Good luck with your research, we have completed the search process.\")\n",
    "\n",
    "       \n",
    "        return Command(\n",
    "            goto=\"__end__\",\n",
    "            update={\n",
    "                \"confirmed\": result.confirmed,\n",
    "                \"messages\": [ai_chat_msg],\n",
    "            }\n",
    "        )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim analysis graph\n",
    "\n",
    "Build the claim analysis graph, that takes the user step by step through analysing and scoping the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CLAIM GRAPH\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "from claim_nodes import checkable_fact,checkable_confirmation,retrieve_information,clarify_information,produce_summary,get_confirmation\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from state_scope import AgentStateClaim\n",
    "\n",
    "claim = StateGraph(AgentStateClaim)\n",
    "\n",
    "#getting all info about the claim nodes\n",
    "claim.add_node(\"checkable_fact\", checkable_fact)\n",
    "claim.add_node(\"checkable_confirmation\", checkable_confirmation)\n",
    "claim.add_node(\"retrieve_information\", retrieve_information)\n",
    "claim.add_node(\"clarify_information\", clarify_information)\n",
    "claim.add_node(\"produce_summary\", produce_summary)\n",
    "claim.add_node(\"get_confirmation\", get_confirmation)\n",
    "claim.add_node(\"critical_question\", critical_question)\n",
    "\n",
    "#Claim matching nodes\n",
    "claim.add_node(\"get_rag_queries\", get_rag_queries)\n",
    "claim.add_node(\"confirm_rag_queries\", confirm_rag_queries)\n",
    "claim.add_node(\"rag_retrieve_worker\", rag_retrieve_worker)\n",
    "claim.add_node(\"reduce_rag_results\", reduce_rag_results)\n",
    "claim.add_node(\"structure_claim_matching\", structure_claim_matching)\n",
    "claim.add_node(\"match_or_continue\", match_or_continue)\n",
    "\n",
    "# Source finding nodes and search query nodes\n",
    "claim.add_node(\"get_source\", get_source)\n",
    "claim.add_node(\"get_location_source\", get_location_source)\n",
    "claim.add_node(\"get_source_queries\", get_source_queries)\n",
    "claim.add_node(\"confirm_search_queries\", confirm_search_queries)\n",
    "claim.add_node(\"reset_search_state\", reset_search_state)\n",
    "claim.add_node(\"find_sources_worker\", find_sources_worker)\n",
    "claim.add_node(\"reduce_sources\", reduce_sources)\n",
    "claim.add_node(\"select_primary_source\", select_primary_source)\n",
    "claim.add_node(\"get_search_queries\", get_search_queries)\n",
    "claim.add_node(\"iterate_search\",iterate_search)\n",
    "#claim.add_node(\"router\", router)\n",
    "\n",
    "# Entry point\n",
    "claim.add_edge(START, \"checkable_fact\")\n",
    "claim.add_edge(\"checkable_fact\", \"critical_question\")\n",
    "claim.add_edge(\"retrieve_information\", \"clarify_information\")\n",
    "claim.add_edge(\"produce_summary\", \"critical_question\")\n",
    "\n",
    "# Connecting claim matching nodes\n",
    "claim.add_edge(\"get_rag_queries\", \"confirm_rag_queries\")\n",
    "claim.add_conditional_edges(\"confirm_rag_queries\", route_rag_confirm)\n",
    "claim.add_edge(\"rag_retrieve_worker\", \"reduce_rag_results\")\n",
    "claim.add_edge(\"reduce_rag_results\", \"structure_claim_matching\")\n",
    "\n",
    "# Connecting source finding and search query nodes\n",
    "claim.add_edge(\"get_source_queries\", \"critical_question\")\n",
    "claim.add_edge(\"get_search_queries\", \"critical_question\")\n",
    "claim.add_edge(\"confirm_search_queries\", \"reset_search_state\")\n",
    "claim.add_conditional_edges(\"reset_search_state\", route_after_confirm)\n",
    "claim.add_edge(\"find_sources_worker\", \"reduce_sources\")\n",
    "claim.add_edge(\"iterate_search\", END)\n",
    "\n",
    "claim_flow = claim.compile()\n",
    "\n",
    "#visualization\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(claim_flow.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
