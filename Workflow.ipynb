{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis of a claim\n",
    "To Check on Checkability, the first workflow of the EUFactcheck programme will be followed (https://eufactcheck.eu/about-us/eufactcheck-flowchart/). The student will also be involved to provide more information and to check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\checkmate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "#from langchain_ollama import ChatOllama\n",
    "import tqdm as notebook_tqdm\n",
    "from tavily import TavilyClient\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Initialize Tavily client \n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "#os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "#os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "#os.environ[\"LANGSMITH_PROJECT\"]=\"pr-left-technician-100\"\n",
    "\n",
    "#low temperature for more factual answers,\n",
    "#llm = ChatOllama(model=\"qwen3:4b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "llm = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of all the nodes in Claim analysis\n",
    "\n",
    "These functions are all the nodes in the claim graph and also show the edges in case of conditional nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All the nodes \"\"\"\n",
    "\n",
    "from prompts import (\n",
    "    checkable_check_prompt,\n",
    "    confirmation_checkable_prompt,\n",
    "    get_information_prompt,\n",
    "    confirmation_clarification_prompt,\n",
    "    get_summary_prompt,\n",
    "    confirmation_check_prompt,\n",
    "    rag_queries_prompt,\n",
    "    match_check_prompt,\n",
    "    structure_claim_prompt,\n",
    "    identify_source_prompt,\n",
    "    source_location_prompt,\n",
    "    source_queries_prompt,\n",
    "    confirm_queries_prompt,\n",
    "    select_primary_source_prompt,\n",
    "    search_queries_prompt,\n",
    "    iterate_search_prompt,\n",
    "    get_socratic_question,\n",
    ")\n",
    "from state_scope import (\n",
    "    AgentStateClaim, \n",
    "    SubjectResult, \n",
    "    MoreInfoResult, \n",
    "    SummaryResult, \n",
    "    ConfirmationResult,\n",
    "    ConfirmationFinalResult,\n",
    "    ConfirmationMatch,\n",
    "    ClaimMatchingOutput,\n",
    "    GetSource, \n",
    "    GetSourceLocation,\n",
    "    GetSearchQueries,\n",
    "    PrimarySourceSelection, \n",
    ")\n",
    "from langgraph.types import Overwrite\n",
    "from langchain_core.messages import BaseMessage,HumanMessage,AIMessage, ToolMessage, get_buffer_string\n",
    "from typing import List, Dict, Any, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command, Send\n",
    "from utils import get_new_user_reply,_domain\n",
    "from tooling import llm, llm_tools, llm_tuned, tools_dict\n",
    "import json\n",
    "\n",
    "\n",
    "# Maximum number of messages to send to the prompt\n",
    "MAX_HISTORY_MESSAGES = 6\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ROUTER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def router(state: AgentStateClaim) -> Command[\n",
    "    Literal[\n",
    "        \"checkable_fact\",\n",
    "        \"checkable_confirmation\",\n",
    "        \"retrieve_information\",\n",
    "        \"clarify_information\",\n",
    "        \"produce_summary\",\n",
    "        \"critical_question\",\n",
    "        \"get_confirmation\",\n",
    "        \"get_rag_queries\",\n",
    "        \"confirm_rag_queries\",\n",
    "        \"rag_retrieve_worker\",\n",
    "        \"reduce_rag_results\",\n",
    "        \"structure_claim_matching\",\n",
    "        \"match_or_continue\",\n",
    "        \"get_source\",\n",
    "        \"get_location_source\",\n",
    "        \"get_source_queries\",\n",
    "        \"confirm_search_queries\",\n",
    "        \"select_primary_source\",\n",
    "        \"iterate_search\",\n",
    "        ]\n",
    "]:\n",
    "    \"\"\" Route to correct node, after user reply \"\"\"\n",
    "\n",
    "    return Command(\n",
    "        goto=state.get(\"next_node\") or \"checkable_fact\"\n",
    "        )\n",
    "\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "# # CRITICAL QUESTION NODE\n",
    "# # ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def critical_question(state: AgentStateClaim) -> Command[Literal[\"checkable_confirmation\",\"get_confirmation\"]]:\n",
    "\n",
    "    \"\"\" Ask a socratic question to make the user think about the consequences of a fact checking a claim \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # retrieve conversation history fact-check messages and critical messages\n",
    "    conversation_history_critical = list(state.get(\"messages_critical\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    messages_critical_str = get_buffer_string(conversation_history_critical[-MAX_HISTORY_MESSAGES:] )\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_socratic_question.format(\n",
    "        alerts=alerts_str,\n",
    "        claim=state.get(\"claim\"),\n",
    "        summary=state.get(\"summary\"),\n",
    "        messages_critical=messages_critical_str \n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = llm_tuned.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=result.content)\n",
    "  \n",
    "    return Command(\n",
    "        goto=state.get(\"next_node\"),\n",
    "            update={\n",
    "                \"critical_question\": result.content,\n",
    "                \"messages_critical\": [ai_chat_msg],\n",
    "                \"next_node\": None\n",
    "            }        \n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CHECKABLE_FACT NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def checkable_fact(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Check if a claim is potentially checkable. \"\"\"\n",
    "\n",
    "    #Retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:] \n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(SubjectResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = checkable_check_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        additional_context=state.get(\"additional_context\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # checkable is a boolean in State\n",
    "    is_checkable = result.checkable == \"POTENTIALLY CHECKABLE\"\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    explanation_text = (\n",
    "        f\"**Checkability analysis**\\n\"\n",
    "        f\"- Checkable: `{result.checkable}`\\n\"\n",
    "        f\"- Reason: {result.explanation}\\n\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=explanation_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=\"critical_question\",\n",
    "        update={\n",
    "            \"question\": result.question,\n",
    "            \"checkable\": is_checkable,\n",
    "            \"explanation\": result.explanation,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": \"checkable_confirmation\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CHECKABLE_CONFIRMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def checkable_confirmation(state: AgentStateClaim) -> Command[Literal[\"retrieve_information\",\"__end__\",\"checkable_fact\"]]:\n",
    "    \n",
    "    \"\"\" Get confirmation from user on the gathered information. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", \"\"))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"checkable_confirmation\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "        # Get user reply, if the last message was a user message\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = confirmation_checkable_prompt.format(\n",
    "            claim=state.get(\"claim\", \"\"),\n",
    "            checkable=state.get(\"checkable\", \"\"),\n",
    "            explanation=state.get(\"explanation\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We'll continue with this claim.\"\n",
    "        else:\n",
    "            confirm_text = \"Okay let's revise the claim or stop here.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            if state.get(\"checkable\"):\n",
    "                return Command(\n",
    "                        goto=\"retrieve_information\", \n",
    "                        update={\n",
    "                            \"confirmed\": result.confirmed,\n",
    "                            \"messages\": [ai_chat_msg],\n",
    "                            \"awaiting_user\": False,\n",
    "                            \"additional_context\": None,\n",
    "                            \"next_node\": None\n",
    "                        }\n",
    "                )   \n",
    "            else: \n",
    "                # user confirmed but claim is not checkable → end\n",
    "                end_msg = AIMessage(content=\"This claim appears to be uncheckable, so we'll stop the process here.\")\n",
    "                return Command(\n",
    "                        goto=END, \n",
    "                        update={\n",
    "                            \"confirmed\": result.confirmed,\n",
    "                            \"messages\": [ai_chat_msg] + [end_msg],\n",
    "                            \"awaiting_user\": False,\n",
    "                            \"next_node\": None\n",
    "                        }\n",
    "                )   \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"checkable_fact\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"awaiting_user\": False,\n",
    "                        \"additional_context\": user_answer,\n",
    "                        \"next_node\": None\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE_INFORMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def retrieve_information(state: AgentStateClaim) -> Command[Literal[\"clarify_information\"]]:\n",
    "\n",
    "    \"\"\" Gather more information about a potentially checkable claim. \"\"\"\n",
    "\n",
    "    #Retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(MoreInfoResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_information_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        additional_context=state.get(\"additional_context\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    details_text = (\n",
    "        \"**Here’s what I extracted from your claim:**\\n\"\n",
    "        f\"- Subject: {result.subject or 'not clearly specified'}\\n\"\n",
    "        f\"- Quantitative: {result.quantitative}\\n\"\n",
    "        f\"- Precision: {result.precision}\\n\"\n",
    "        f\"- Based on: {result.based_on}\\n\"\n",
    "    )\n",
    "\n",
    "    if result.alerts:\n",
    "        details_text += \"\\n**Missing / to verify:**\\n\" + \"\\n\".join(f\"- {a}\" for a in result.alerts)\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=details_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=\"clarify_information\", \n",
    "        update={\n",
    "            \"subject\": result.subject,\n",
    "            \"quantitative\": result.quantitative,\n",
    "            \"precision\": result.precision,\n",
    "            \"based_on\": result.based_on,\n",
    "            \"question\": result.question,\n",
    "            \"alerts\": result.alerts or [],\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": None,\n",
    "        }\n",
    "    )   \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CLARIFY_INFORMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def clarify_information(state: AgentStateClaim) -> Command[Literal[\"produce_summary\", \"retrieve_information\"]]:\n",
    "\n",
    "    \"\"\" Get confirmation from user on the gathered information. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", \"\"))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"clarify_information\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        #retrieve alerts and format to string for the prompt\n",
    "        alerts=state.get(\"alerts\", [])\n",
    "        alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirmation_clarification_prompt.format(\n",
    "            subject=state.get(\"subject\", \"\"),\n",
    "            quantitative=state.get(\"quantitative\", \"\"),\n",
    "            precision=state.get(\"precision\", \"\"),\n",
    "            based_on=state.get(\"based_on\", \"\"),\n",
    "            claim=state.get(\"claim\", \"\"),\n",
    "            question=state.get(\"question\", \"\"),\n",
    "            alerts=alerts_str,\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"Thanks, I’ll use this information to draft the summary.\"\n",
    "        else:\n",
    "            confirm_text = \"Let’s collect a bit more information.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            return Command(\n",
    "                    goto=\"produce_summary\", \n",
    "                    update={\n",
    "                        \"confirmed\": result.confirmed,\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"additional_context\": None,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"retrieve_information\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"additional_context\": user_answer,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# PRODUCE SUMMARY NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def produce_summary(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Get a summary on the gathered information. \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last messages into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(SummaryResult, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_summary_prompt.format(\n",
    "        claim=state.get(\"claim\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        quantitative=state.get(\"quantitative\", \"\"),\n",
    "        precision=state.get(\"precision\", \"\"),\n",
    "        based_on=state.get(\"based_on\", \"\"),\n",
    "        alerts=alerts_str,\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # human-readable assistant message for the chat\n",
    "    summary_text = (\n",
    "        f\"**Summary of our findings so far:**\\n\\n{result.summary}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=summary_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    return Command( \n",
    "            goto=\"critical_question\",\n",
    "            update={\n",
    "                \"summary\": result.summary,\n",
    "                \"question\": result.question,\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"subject\": result.subject,\n",
    "                \"quantitative\": result.quantitative,\n",
    "                \"precision\": result.precision,\n",
    "                \"based_on\": result.based_on,\n",
    "                \"alerts\": result.alerts or [],\n",
    "                \"claim_source\": result.claim_source,\n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"get_confirmation\",\n",
    "            }\n",
    "    )       \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GET_CONFIRMATION NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "   \n",
    "def get_confirmation(state: AgentStateClaim) -> Command[Literal[\"produce_summary\", \"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Get confirmation from user on the gathered information.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        ask_msg = AIMessage(content=state.get(\"question\", []))\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"get_confirmation\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationFinalResult, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirmation_check_prompt.format(\n",
    "            summary=state.get(\"summary\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"Let's check if this claim has already been researched\"\n",
    "        else:\n",
    "            confirm_text = \"Let's revisit the summary and adjust it if needed.\"\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            return Command(\n",
    "                    goto=\"get_rag_queries\", \n",
    "                    update={\n",
    "                        \"confirmed\": result.confirmed,\n",
    "                        \"subject\": result.subject,\n",
    "                        \"quantitative\": result.quantitative,\n",
    "                        \"precision\": result.precision,\n",
    "                        \"based_on\": result.based_on,\n",
    "                        \"question\": result.question,\n",
    "                        \"alerts\": result.alerts or [],\n",
    "                        \"claim_source\": result.claim_source,\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"produce_summary\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],\n",
    "                        \"subject\": result.subject,\n",
    "                        \"quantitative\": result.quantitative,\n",
    "                        \"precision\": result.precision,\n",
    "                        \"based_on\": result.based_on,\n",
    "                        \"question\": result.question,\n",
    "                        \"alerts\": result.alerts or [],\n",
    "                        \"claim_source\": result.claim_source,\n",
    "                        \"next_node\": None,\n",
    "                    }\n",
    "            )\n",
    "  \n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES FOR CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_rag_queries(state: AgentStateClaim) -> Command[Literal[\"confirm_rag_queries\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = rag_queries_prompt.format(\n",
    "        summary=state.get(\"summary\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        messages=messages_str,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"confirm_rag_queries\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "        }\n",
    "    )    \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CONFIRM CLAIM MATCHING QUERIES NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def confirm_rag_queries(state: AgentStateClaim) -> dict:\n",
    "    \n",
    "    \"\"\"Update state based on user confirmation. Routing is handled by a router.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        # pause for user\n",
    "        return {\n",
    "            \"next_node\": \"confirm_rag_queries\",\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # retrieve search_queries and format to string for the prompt\n",
    "        search_queries = state.get(\"search_queries\", [])\n",
    "        search_queries_str= \"\\n\".join(f\"- {a}\" for a in search_queries)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirm_queries_prompt.format(\n",
    "            search_queries=search_queries_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "         # human-readable assistant message for the chat\n",
    "        queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We search the Claims database with these queries.\\n\" + queries_text\n",
    "        else:\n",
    "            confirm_text = (\n",
    "                \"Is there anything else you would like to change about the search queries?\\n\"\n",
    "                f\"{queries_text}\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return {\n",
    "            \"confirmed\": result.confirmed,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": (not result.confirmed),\n",
    "        }\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ROUTER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def route_rag_confirm(state: AgentStateClaim):\n",
    "\n",
    "    \"\"\" Route based on user confirmation of RAG queries. \"\"\"\n",
    "\n",
    "    # retrieve search queries\n",
    "    q = state.get(\"search_queries\", [])\n",
    "    if state.get(\"next_node\") == \"confirm_rag_queries\":\n",
    "        return \"__end__\"\n",
    "\n",
    "    # If not confirmed, go back to get_rag_queries\n",
    "    if not state.get(\"confirmed\", False):\n",
    "        return \"confirm_rag_queries\"\n",
    "\n",
    "    # If confirmed, proceed to retrieval\n",
    "    return [\n",
    "        Send(\"rag_retrieve_worker\", {\"current_query\": q})\n",
    "        for q in state.get(\"search_queries\", [])\n",
    "        if q\n",
    "    ]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# WORKER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "async def rag_retrieve_worker(state: AgentStateClaim) -> Dict[str, Any]:\n",
    "    \"\"\" Perform retrieval for a given query. \"\"\"\n",
    "\n",
    "    # Get the current query from state\n",
    "    q = state[\"current_query\"]\n",
    "\n",
    "    # Call the retriever tool\n",
    "    retriever_tool = tools_dict[\"retriever_tool\"] \n",
    "    out = await retriever_tool.ainvoke({\"query\": q, \"k\": 10})  \n",
    "\n",
    "    # Return the RAG trace entry\n",
    "    return {\n",
    "        \"rag_trace\": [{\n",
    "            \"tool_name\": \"retriever_tool\",\n",
    "            \"args\": {\"query\": q, \"k\": 10},\n",
    "            \"output\": out,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# REDUCER CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reduce_rag_results(state: AgentStateClaim) -> Command[Literal[\"structure_claim_matching\"]]:\n",
    "\n",
    "    \"\"\" Reduce the RAG retrieval results into a single trace. \"\"\"\n",
    "\n",
    "    # Retrieve RAG traces from state\n",
    "    rag_trace = state.get(\"rag_trace\", [])\n",
    "\n",
    "    # Combine all RAG traces into one\n",
    "    return Command(\n",
    "        goto=\"structure_claim_matching\",\n",
    "        update={\n",
    "            \"tool_trace\": rag_trace,   # so your structure_claim_matching stays the same\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# STRUCTURE CLAIM MATCHING NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def structure_claim_matching(state: AgentStateClaim) -> Command[Literal[\"match_or_continue\",\"get_source\"]]:\n",
    "   \n",
    "    \"\"\"Take the raw retrieval trace and turn it into structured output.\"\"\"\n",
    "\n",
    "      # Use structured output\n",
    "    structured_llm = llm.with_structured_output(ClaimMatchingOutput, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = structure_claim_prompt.format(\n",
    "        summary=state.get(\"summary\", \"\"),\n",
    "        subject=state.get(\"subject\", \"\"),\n",
    "        tool_trace=state.get(\"tool_trace\", \"\"),\n",
    "    )\n",
    "\n",
    "    # This returns an instance of ClaimMatchingOutput already parsed\n",
    "    result = structured_llm.invoke(prompt)\n",
    " \n",
    "    # Build a human-readable message for the chat UI\n",
    "    explanation_lines = []\n",
    "    explanation_lines.append(\"**Claim matching results**\")\n",
    "    explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    # # Show the queries + reasoning\n",
    "    # if result.queries:\n",
    "    #     explanation_lines.append(\"Here are the search questions that were used (or would be appropriate) to look for similar claims:\")\n",
    "    #     for i, q in enumerate(result.queries, start=1):\n",
    "    #         explanation_lines.append(f\"- **Q{i}:** {q.query}\")\n",
    "    #         explanation_lines.append(f\"  - Why this query: {q.reasoning}\")\n",
    "    #     explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    # Show the top claims\n",
    "    if result.top_claims:\n",
    "        explanation_lines.append(\"From the retrieved information, these existing claims might be relevant to what you're investigating:\")\n",
    "        for i, c in enumerate(result.top_claims, start=1):\n",
    "            # URL line only if present\n",
    "            url_part = f\"\\n  - {c.allowed_url}\" if c.allowed_url else \"\"\n",
    "            explanation_lines.append(f\"- **Claim {i}:** {c.short_summary}{url_part}\")\n",
    "            explanation_lines.append(f\"  - How it aligns or differs from your claim: {c.alignment_rationale}\")\n",
    "        explanation_lines.append(\"\")  # blank line\n",
    "    else:\n",
    "        explanation_lines.append(\"No strong matching existing claims were identified based on the retrieved information.\")\n",
    "        explanation_lines.append(\"\")  # blank line\n",
    "\n",
    "    explanation_text = \"\\n\".join(explanation_lines)\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=explanation_text)\n",
    "\n",
    "    # Goto next node and update State\n",
    "    if result.top_claims:\n",
    "        return Command(\n",
    "            goto=\"match_or_continue\",\n",
    "            update={\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"claim_matching_result\": result, \n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"match_or_continue\",\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"get_source\", \n",
    "            update={\n",
    "                \"messages\": [ai_chat_msg],  \n",
    "                \"awaiting_user\": True,\n",
    "                \"next_node\": \"get_source\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# MATCHED OR CONTUE RESEARCH NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def match_or_continue(state: AgentStateClaim) -> Command[Literal[\"get_source\", \"__end__\"]]:\n",
    "\n",
    "    \"\"\" Decide whether to continue researching or end the process if a matching claim was found.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Do any of these match your claim? Or do you want to continue researching as suggested?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"match_or_continue\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(ConfirmationMatch, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt =  match_check_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # # human-readable assistant message for the chat\n",
    "        if result.match:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    \"This claim appears to match an already researched claim. \"\n",
    "                    \"We can stop the process here.\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    \"No exact match found. Let's continue researching.\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Goto next node and update State\n",
    "        if result.match:\n",
    "            return Command(\n",
    "                    goto=\"__end__\", \n",
    "                    update={\n",
    "                        \"match\": result.match,\n",
    "                        \"messages\": [ai_chat_msg], \n",
    "                        \"awaiting_user\": False,\n",
    "                        \"next_node\": None\n",
    "                    }\n",
    "            )       \n",
    "        else:\n",
    "            return Command(\n",
    "                    goto=\"get_source\", \n",
    "                    update={\n",
    "                        \"messages\": [ai_chat_msg],  \n",
    "                        \"awaiting_user\": True,\n",
    "                        \"next_node\": \"get_source\",\n",
    "                    }\n",
    "            )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE SOURCE, AND CHECK IF PRIMARY SOURCE KNOWN\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_source(state: AgentStateClaim) -> Command[Literal[\"get_location_source\"]]:\n",
    "\n",
    "    \"\"\" Ask the user for the  source of the claim if no match was found.\"\"\"\n",
    "    \n",
    "    # Retrieve the source of the claim\n",
    "    claim_source=state.get(\"claim_source\")\n",
    "    \n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        if claim_source:\n",
    "            ask_msg = AIMessage(content=\"Was {claim_source} the primary source (the first time this claim was made)?\")\n",
    "        elif claim_source is None or claim_source == \"\":\n",
    "            ask_msg = AIMessage(content=\"Do you know the source and is this the primary source (when it was first made)?\")\n",
    "\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"get_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSource, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = identify_source_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "            claim_source=claim_source,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        if result.primary_source:\n",
    "            question_text = (\n",
    "                f\"Great, we have the primary source as **{result.primary_source}**.\\n\\n\"\n",
    "                f\"Can you locate or provide a URL to the specific social media post, \"\n",
    "                f\"speech, or article where {result.primary_source} first made this claim? \"\n",
    "                f\"Or can you describe the source if you don't have a URL?\"\n",
    "            )\n",
    "        else:\n",
    "            question_text = (\n",
    "                \"I couldn't identify a primary source from that. \"\n",
    "                \"Can you locate or provide a URL to the specific social media post, \"\n",
    "                \"speech, or article where this claim was found? \"\n",
    "                \"Or can you describe the source if you don't have a URL?\"\n",
    "            )\n",
    "        ai_chat_msg = AIMessage(content=question_text)\n",
    "\n",
    "         # Goto next node and update State\n",
    "        return Command(\n",
    "                goto=\"get_location_source\", \n",
    "                update={\n",
    "                    \"claim_source\": result.claim_source,\n",
    "                    \"primary_source\": result.primary_source,\n",
    "                    \"messages\": [ai_chat_msg],\n",
    "                    \"awaiting_user\": True,\n",
    "                    \"next_node\": None,\n",
    "                }\n",
    "        ) \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RETRIEVE LOCATION OF SOURCE NODE, OR A DESCRIPTION\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_location_source(state: AgentStateClaim) -> Command[Literal[\"get_search_queries\",\"get_source_queries\"]]:\n",
    "\n",
    "    \"\"\" Ask the user where the claim was found. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"next_node\": \"get_location_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSourceLocation, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  = source_location_prompt.format(\n",
    "            messages=messages_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        #check if the primary source is known\n",
    "        primary_source = state.get(\"primary_source\")\n",
    "\n",
    "        # human-readable assistant message for the chat\n",
    "        if primary_source:\n",
    "            chat_text = (\n",
    "                f\"We will now continue researching the claim.\"\n",
    "            )\n",
    "        else:\n",
    "            chat_text = (\n",
    "                f\"We will now first try to locate the primary source (who made the claim first).\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if primary_source:\n",
    "            return Command(\n",
    "                goto=\"get_search_queries\", \n",
    "                update={\n",
    "                    \"claim_url\": result.claim_url,\n",
    "                    \"source_description\": result.source_description,\n",
    "                    \"messages\":  [ai_chat_msg],\n",
    "                    \"next_node\": None,\n",
    "                    \"awaiting_user\": False,\n",
    "                }\n",
    "            )    \n",
    "        else:\n",
    "            return Command(\n",
    "                goto=\"get_source_queries\",\n",
    "                update={\n",
    "                    \"claim_url\": result.claim_url,\n",
    "                    \"source_description\": result.source_description,\n",
    "                    \"messages\":  [ai_chat_msg],\n",
    "                    \"next_node\": None,\n",
    "                    \"awaiting_user\": False,\n",
    "                }\n",
    "            )  \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES TO LOCATE PRIMARY SOURCE NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_source_queries(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = source_queries_prompt.format(\n",
    "        messages=messages_str,\n",
    "        summary = state.get(\"summary\", \"\"),\n",
    "        claim = state.get(\"claim\", \"\"),\n",
    "        claim_source=state.get(\"claim_source\", \"\"),\n",
    "        claim_url = state.get(\"claim_url\", \"\"),\n",
    "        claim_description = state.get(\"claim_description\", \"\")\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"critical_question\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "            \"research_focus\": \"select_primary_source\",\n",
    "        }\n",
    "    )    \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CONFIRM SEARCH QUERIES NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def confirm_search_queries(state: AgentStateClaim) -> dict:\n",
    "    \n",
    "    \"\"\"Update state based on user confirmation. Routing is handled by a router.\"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        # pause for user\n",
    "        return {\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": False,\n",
    "        }\n",
    "    else:\n",
    "        # retrieve conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # retrieve search_queries and format to string for the prompt\n",
    "        search_queries = state.get(\"search_queries\", [])\n",
    "        search_queries_str= \"\\n\".join(f\"- {a}\" for a in search_queries)\n",
    "\n",
    "        # Use structured output\n",
    "        structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt  =  confirm_queries_prompt.format(\n",
    "            search_queries=search_queries_str,\n",
    "            user_answer=user_answer,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "         # human-readable assistant message for the chat\n",
    "        queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "        if result.confirmed:\n",
    "            confirm_text = \"We will continue searching.\"\n",
    "        else:\n",
    "            confirm_text = (\n",
    "                \"Is there anything else you would like to change about the search queries?\\n\"\n",
    "                f\"{queries_text}\"\n",
    "            )\n",
    "\n",
    "        ai_chat_msg = AIMessage(content=confirm_text)\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return {\n",
    "            \"confirmed\": result.confirmed,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\": [ai_chat_msg],\n",
    "            \"next_node\": None,\n",
    "            \"awaiting_user\": (not result.confirmed),\n",
    "        }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# RESET THE SEARCH STATE NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reset_search_state(state: AgentStateClaim):\n",
    "    return {\n",
    "        \"tavily_context\": Overwrite([]),\n",
    "        }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES ORCHESTRATOR NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def route_after_confirm(state: AgentStateClaim):\n",
    "\n",
    "    \"\"\" Route based on user confirmation of search queries. \"\"\"\n",
    "\n",
    "    # If we need to stop and wait for user input\n",
    "    if state.get(\"next_node\") == \"confirm_search_queries\":\n",
    "        return \"__end__\"\n",
    "\n",
    "    # If user hasn't confirmed, loop\n",
    "    if not state.get(\"confirmed\", False):\n",
    "        return \"confirm_search_queries\"\n",
    "\n",
    "    # Confirmed => fan out to workers\n",
    "    return [\n",
    "        Send(\"find_sources_worker\", {\"current_query\": q})\n",
    "        for q in state.get(\"search_queries\", [])\n",
    "        if q\n",
    "    ]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES WORKER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "async def find_sources_worker(state: AgentStateClaim) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Worker: run Tavily for one query, return one compact result block.\"\"\"\n",
    "\n",
    "    # Run tavily search for the current query, get top 18 results\n",
    "    q = state[\"current_query\"]\n",
    "    tavily_tool = tools_dict.get(\"tavily_search\")\n",
    "\n",
    "    tool_output = await tavily_tool.ainvoke({\"query\": q, \"max_results\": 18})\n",
    "    out_dict = tool_output.model_dump() if hasattr(tool_output, \"model_dump\") else dict(tool_output)\n",
    "\n",
    "    compact = {\"query\": out_dict.get(\"query\", q), \"results\": []}\n",
    "\n",
    "    # Add the top 9 results only to the compact Dictionary\n",
    "    for r in (out_dict.get(\"results\") or []):\n",
    "        r = r.model_dump() if hasattr(r, \"model_dump\") else dict(r)\n",
    "        url = (r.get(\"url\") or \"\").strip()\n",
    "        if not url:\n",
    "            continue\n",
    "        compact[\"results\"].append({\"title\": r.get(\"title\"), \"url\": url})\n",
    "        if len(compact[\"results\"]) >= 9:\n",
    "            break\n",
    "\n",
    "    # Return and wrap in a list so it merges via operator.add\n",
    "    return {\"tavily_context\": [compact]}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# FIND SOURCES REDUCER NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reduce_sources(state: AgentStateClaim) -> Command[Literal[\"select_primary_source\", \"iterate_search\"]]:\n",
    "\n",
    "    \"\"\"Merge worker outputs, enforce diversity across queries, build message.\"\"\"\n",
    "\n",
    "    # Retrieve all tavily results from the worker outputs\n",
    "    tavily_results = state.get(\"tavily_context\", [])\n",
    "\n",
    "    # remember used urls and domains to enforce diversity\n",
    "    used_urls = set()\n",
    "    used_domains = set()\n",
    "    final_blocks: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Deduplicate across queries, and set limit to 3 results per query\n",
    "    for block in tavily_results:\n",
    "        query = block.get(\"query\")\n",
    "        results = block.get(\"results\", [])\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        compact = {\"query\": query, \"results\": []}\n",
    "        for r in results:\n",
    "            url = (r.get(\"url\") or \"\").strip()\n",
    "            if not url:\n",
    "                continue\n",
    "            dom = _domain(url)\n",
    "\n",
    "            if url in used_urls or dom in used_domains:\n",
    "                continue\n",
    "\n",
    "            compact[\"results\"].append({\"title\": r.get(\"title\"), \"url\": url})\n",
    "            used_urls.add(url)\n",
    "            used_domains.add(dom)\n",
    "\n",
    "            if len(compact[\"results\"]) >= 3:\n",
    "                break\n",
    "\n",
    "        final_blocks.append(compact)\n",
    "\n",
    "    # Human-readable assistant message for the chat\n",
    "    lines = [\"Here are the top results I found for each search query:\", \"\"]\n",
    "    startnr = 1\n",
    "    \n",
    "    for block in final_blocks:\n",
    "        query = block.get(\"query\")\n",
    "        results = block.get(\"results\", [])\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        # Add query header and blank lines\n",
    "        lines.append(\"\") \n",
    "        lines.append(f\"**Query:** {query}\")\n",
    "        lines.append(\"\") \n",
    "\n",
    "        # List results\n",
    "        for i, r in enumerate(results, start=startnr):\n",
    "            title = r.get(\"title\")\n",
    "            url = r.get(\"url\")\n",
    "            lines.append(f\"{i}. **[{title}]({url})**\")\n",
    "        startnr += len(results)\n",
    "  \n",
    "    new_msgs = [AIMessage(content=\"\\n\".join(lines))]\n",
    "\n",
    "    # Decide next node based on research focus\n",
    "    research_focus = state.get(\"research_focus\")\n",
    "    state_next_node = \"select_primary_source\" if research_focus == \"select_primary_source\" else \"iterate_search\"\n",
    "    \n",
    "    # Goto next node and update State\n",
    "    return Command(\n",
    "        goto=state_next_node,\n",
    "        update={\n",
    "            # overwrite with the finalized/deduped set for downstream nodes\n",
    "            \"tavily_context\": final_blocks,\n",
    "            \"messages\": new_msgs,\n",
    "            \"awaiting_user\": True,\n",
    "            \"next_node\": None,\n",
    "            \"search_queries\":[],\n",
    "        },\n",
    "    )\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# SELECT PRIMARY SOURCE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def select_primary_source(state: AgentStateClaim) -> Command[Literal[\"get_search_queries\"]]:\n",
    "\n",
    "    \"\"\" pick the best / most likely primary source. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Does any of these sources correspond to the primary source of the claim?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"select_primary_source\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Get the context and conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output \n",
    "        structured_llm = llm.with_structured_output(PrimarySourceSelection, method=\"json_mode\")\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = select_primary_source_prompt.format(\n",
    "            claim_source=state.get(\"claim_source\", \"\"),\n",
    "            claim_url=state.get(\"claim_url\", \"\"),\n",
    "            user_answer=user_answer,\n",
    "            messages=messages_str,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # retrieve existing alerts\n",
    "        alerts = list(state.get(\"alerts\", []))\n",
    "\n",
    "        # human-readable assistant message\n",
    "        if result.primary_source:\n",
    "            ai_chat_msg = AIMessage(\n",
    "                content=(\n",
    "                    f\"Great, you have identified **{result.claim_source}** as the primary source of the claim. \"\n",
    "                    \"I'll proceed with the research.\"\n",
    "                )\n",
    "            )\n",
    "        else:   \n",
    "            # Add an alert if the primary source is not found\n",
    "            if not result.primary_source:\n",
    "                alerts.append(\"primary source not found\")\n",
    "                ai_chat_msg = AIMessage(\n",
    "                    content=(\n",
    "                        \"I couldn’t identify a clear primary source from these results. \"\n",
    "                        \"I'll continue with the research anyway, but note that the original source is still missing.\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Goto next node and update State\n",
    "        return Command(\n",
    "            goto=\"get_search_queries\",\n",
    "            update={\n",
    "                \"primary_source\": result.primary_source,\n",
    "                \"claim_source\": result.claim_source,\n",
    "                \"claim_url\": result.claim_url,\n",
    "                \"messages\": [ai_chat_msg],\n",
    "                \"alerts\": alerts,\n",
    "                \"next_node\": None,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# GENERATE QUERIES TO FALSIFY OR VERIFY CLAIM NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def get_search_queries(state: AgentStateClaim) -> Command[Literal[\"critical_question\"]]:\n",
    "\n",
    "    \"\"\" Generate queries to locate the primary source of the claim. \"\"\"\n",
    "\n",
    "    # retrieve conversation history\n",
    "    conversation_history = list(state.get(\"messages\", []))\n",
    "\n",
    "    # Add the last message into a string for the prompt\n",
    "    recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "    messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts=state.get(\"alerts\", [])\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llm.with_structured_output(GetSearchQueries, method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  = search_queries_prompt.format(\n",
    "        messages=messages_str,\n",
    "        alerts=alerts_str,\n",
    "        summary = state.get(\"summary\", \"\"),\n",
    "        claim = state.get(\"claim\", \"\"),\n",
    "        claim_source=state.get(\"claim_source\", \"\"),\n",
    "        claim_url = state.get(\"claim_url\", \"\"),\n",
    "        claim_description = state.get(\"claim_description\", \"\")\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # create a human-readable assistant message for the chat\n",
    "    queries_text = \"\\n\".join(f\"- {q}\" for q in result.search_queries if q)\n",
    "    chat_text = (\n",
    "        \"I will perform a search with these queries, would you like to add or change something?\\n\"\n",
    "        f\"{queries_text}\"\n",
    "    )\n",
    "\n",
    "    ai_chat_msg = AIMessage(content=chat_text)\n",
    "\n",
    "    # Goto next node and update State  \n",
    "    return Command(\n",
    "        goto=\"critical_question\", \n",
    "        update={\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"messages\":  [ai_chat_msg],\n",
    "            \"next_node\": \"confirm_search_queries\",\n",
    "            \"awaiting_user\": True,\n",
    "            \"confirmed\":False,\n",
    "            \"research_focus\": \"iterate_search\",\n",
    "        }\n",
    "    ) \n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ASK TO ITERATE SEARCH NODE\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def iterate_search(state: AgentStateClaim) -> Command[Literal[\"get_search_queries\",\"__end__\"]]:\n",
    "\n",
    "    \"\"\" pick the best / most likely primary source. \"\"\"\n",
    "\n",
    "    if state.get(\"awaiting_user\"):\n",
    "        ask_msg = AIMessage(content=\"Do you want to search once more?\")\n",
    "        return Command(\n",
    "            goto=\"__end__\", \n",
    "            update={\n",
    "                \"messages\": [ask_msg],\n",
    "                \"next_node\": \"iterate_search\",\n",
    "                \"awaiting_user\": False,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        # Get the context and conversation history\n",
    "        conversation_history = list(state.get(\"messages\", []))\n",
    "        user_answer = get_new_user_reply(conversation_history)\n",
    "\n",
    "        # Use structured output \n",
    "        structured_llm = llm.with_structured_output(ConfirmationResult, method=\"json_mode\")\n",
    "\n",
    "        # Add the last message into a string for the prompt\n",
    "        recent_messages = conversation_history[-MAX_HISTORY_MESSAGES:]  # tune this number\n",
    "        messages_str = get_buffer_string(recent_messages)\n",
    "\n",
    "        # Create a prompt\n",
    "        prompt = iterate_search_prompt.format(\n",
    "                    user_answer=user_answer,\n",
    "                    messages=messages_str,\n",
    "        )\n",
    "\n",
    "        #invoke the LLM and store the output\n",
    "        result = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # retrieve existing alerts\n",
    "        alerts = list(state.get(\"alerts\", []))\n",
    "\n",
    "        # human-readable assistant message\n",
    "        if result.confirmed:\n",
    "            ai_chat_msg = AIMessage(content=\"Let's search once more.\")\n",
    "        else:   \n",
    "            ai_chat_msg = AIMessage(content=\"Good luck with your research, we have completed the search process.\")\n",
    "\n",
    "        # Goto next node and update State\n",
    "        if result.confirmed:\n",
    "            return Command(\n",
    "                goto=\"get_search_queries\",\n",
    "                update={\n",
    "                    \"confirmed\": result.confirmed,\n",
    "                    \"messages\": [ai_chat_msg],\n",
    "                    \"next_node\": None,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            return Command(\n",
    "                goto=\"__end__\",\n",
    "                update={\n",
    "                    \"confirmed\": result.confirmed,\n",
    "                    \"messages\": [ai_chat_msg],\n",
    "                }\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim analysis graph\n",
    "\n",
    "Build the claim analysis graph, that takes the user step by step through analysing and scoping the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# CLAIM GRAPH\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "from claim_nodes import checkable_fact,checkable_confirmation,retrieve_information,clarify_information,produce_summary,get_confirmation\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from state_scope import AgentStateClaim\n",
    "\n",
    "claim = StateGraph(AgentStateClaim)\n",
    "\n",
    "#getting all info about the claim nodes\n",
    "claim.add_node(\"checkable_fact\", checkable_fact)\n",
    "claim.add_node(\"checkable_confirmation\", checkable_confirmation)\n",
    "claim.add_node(\"retrieve_information\", retrieve_information)\n",
    "claim.add_node(\"clarify_information\", clarify_information)\n",
    "claim.add_node(\"produce_summary\", produce_summary)\n",
    "claim.add_node(\"get_confirmation\", get_confirmation)\n",
    "claim.add_node(\"critical_question\", critical_question)\n",
    "\n",
    "#Claim matching nodes\n",
    "claim.add_node(\"get_rag_queries\", get_rag_queries)\n",
    "claim.add_node(\"confirm_rag_queries\", confirm_rag_queries)\n",
    "claim.add_node(\"rag_retrieve_worker\", rag_retrieve_worker)\n",
    "claim.add_node(\"reduce_rag_results\", reduce_rag_results)\n",
    "claim.add_node(\"structure_claim_matching\", structure_claim_matching)\n",
    "claim.add_node(\"match_or_continue\", match_or_continue)\n",
    "\n",
    "# Source finding nodes and search query nodes\n",
    "claim.add_node(\"get_source\", get_source)\n",
    "claim.add_node(\"get_location_source\", get_location_source)\n",
    "claim.add_node(\"get_source_queries\", get_source_queries)\n",
    "claim.add_node(\"confirm_search_queries\", confirm_search_queries)\n",
    "claim.add_node(\"reset_search_state\", reset_search_state)\n",
    "claim.add_node(\"find_sources_worker\", find_sources_worker)\n",
    "claim.add_node(\"reduce_sources\", reduce_sources)\n",
    "claim.add_node(\"select_primary_source\", select_primary_source)\n",
    "claim.add_node(\"get_search_queries\", get_search_queries)\n",
    "claim.add_node(\"iterate_search\",iterate_search)\n",
    "claim.add_node(\"router\", router)\n",
    "\n",
    "# Entry point\n",
    "claim.add_edge(START, \"router\")\n",
    "claim.add_edge(\"checkable_fact\", \"critical_question\")\n",
    "claim.add_edge(\"retrieve_information\", \"clarify_information\")\n",
    "claim.add_edge(\"produce_summary\", \"critical_question\")\n",
    "\n",
    "# Connecting claim matching nodes\n",
    "claim.add_edge(\"get_rag_queries\", \"confirm_rag_queries\")\n",
    "claim.add_conditional_edges(\"confirm_rag_queries\", route_rag_confirm)\n",
    "claim.add_edge(\"rag_retrieve_worker\", \"reduce_rag_results\")\n",
    "claim.add_edge(\"reduce_rag_results\", \"structure_claim_matching\")\n",
    "\n",
    "# Connecting source finding and search query nodes\n",
    "claim.add_edge(\"get_source_queries\", \"critical_question\")\n",
    "claim.add_edge(\"get_search_queries\", \"critical_question\")\n",
    "claim.add_edge(\"confirm_search_queries\", \"reset_search_state\")\n",
    "claim.add_conditional_edges(\"reset_search_state\", route_after_confirm)\n",
    "claim.add_edge(\"find_sources_worker\", \"reduce_sources\")\n",
    "\n",
    "claim_flow = claim.compile()\n",
    "\n",
    "#visualization\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(claim_flow.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
