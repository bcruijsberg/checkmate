{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854808b5",
   "metadata": {},
   "source": [
    "## Measuring Latency and retrieve input/output tokens\n",
    "\n",
    "First we will run one task 1 to measure latency, only llama 3.2 -1b, failed 9 times on this task generating an exception, all other models did this task without generating an exception. \n",
    "\n",
    "We will start with the ollama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d776c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Ollama Models\n",
    "llama1 = ChatOllama(model=\"llama3.2:1b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "llama3 = ChatOllama(model=\"llama3.2:3b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "mistral7 = ChatOllama(model=\"mistral:7b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "qwen3_1 = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "qwen3_4 = ChatOllama(model=\"qwen3:4b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Models on Groq,\n",
    "llama8 = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1)\n",
    "GPTOSS20 = ChatGroq(model_name=\"openai/gpt-oss-20b\", temperature=0.1)\n",
    "qwen3_32 = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "llama70 = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.1)\n",
    "GPTOSS120 = ChatGroq(model_name=\"openai/gpt-oss-120b\", temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, json\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"llama1\":  \"llama3.2:1b\",\n",
    "    \"llama3\":  \"llama3.2:3b\",\n",
    "    \"mistral7\": \"mistral:7b\",\n",
    "    \"qwen3_1\": \"qwen3:1.7b\",\n",
    "    \"qwen3_4\": \"qwen3:4b\",\n",
    "}\n",
    "\n",
    "models_to_test = [\"llama1\", \"llama3\", \"mistral7\", \"qwen3_1\", \"qwen3_4\"]\n",
    "\n",
    "CLAIM_COL = \"claim\"\n",
    "\n",
    "class CheckResult(BaseModel):\n",
    "    checkable: Literal[\"POTENTIALLY CHECKABLE\", \"UNCHECKABLE\"]\n",
    "    explanation: str = Field(\"\")\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst.\n",
    "\n",
    "### Inputs\n",
    "Claim: {claim}\n",
    "\n",
    "### Task\n",
    "Classify the claim and determine if it can be fact-checked.\n",
    "\n",
    "### Classification Logic\n",
    "- UNCHECKABLE: Opinion, value judgment, or prediction.\n",
    "- POTENTIALLY CHECKABLE: Factual claims about the past or present.\n",
    "\n",
    "### Output\n",
    "Return ONLY valid JSON (no markdown, no code fences):\n",
    "{{\n",
    "  \"checkable\": \"POTENTIALLY CHECKABLE\",\n",
    "  \"explanation\": \"Brief justification for the classification.\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def run_model(df: pd.DataFrame, model_name: str, model_tag: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    checkable, explanation, latency_ms = [], [], []\n",
    "\n",
    "    for claim in out[CLAIM_COL].astype(str).tolist():\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        resp = ollama.chat(\n",
    "            model=model_tag,\n",
    "            messages=[{\"role\": \"user\", \"content\": PROMPT.format(claim=claim)}],\n",
    "            options={\"temperature\": 0.1},\n",
    "        )\n",
    "\n",
    "        latency_ms.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "        content = resp[\"message\"][\"content\"].strip()\n",
    "\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.strip(\"`\").replace(\"json\\n\", \"\", 1).strip()\n",
    "\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            parsed = CheckResult(**data)\n",
    "            checkable.append(parsed.checkable)\n",
    "            explanation.append(parsed.explanation)\n",
    "        except Exception:\n",
    "            checkable.append(None)\n",
    "            explanation.append(content)\n",
    "\n",
    "    out[f\"{model_name}_latency_ms\"] = latency_ms\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1145c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"validated_reference_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "for name in models_to_test:\n",
    "    print(f\"Running model: {name}\")\n",
    "    original_df = run_model(\n",
    "        original_df,\n",
    "        model_name=name,\n",
    "        model_tag=MODEL_MAP[name],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a70dc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'claim', 'rating', 'translated', 'year', 'checkable',\n",
       "       'explanation', 'details_text', 'alerts', 'question', 'user_answer',\n",
       "       'confirmed', 'llama1_latency_ms', 'llama3_latency_ms',\n",
       "       'mistral7_latency_ms', 'qwen3_1_latency_ms', 'qwen3_4_latency_ms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7ea8743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama1_latency_ms        379.519268\n",
       "llama3_latency_ms        414.826887\n",
       "mistral7_latency_ms      826.366297\n",
       "qwen3_1_latency_ms      2144.206554\n",
       "qwen3_4_latency_ms     12382.667738\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df[['llama1_latency_ms', 'llama3_latency_ms',\n",
    "       'mistral7_latency_ms', 'qwen3_1_latency_ms', 'qwen3_4_latency_ms']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dc415",
   "metadata": {},
   "source": [
    "Next, the groq logs are analysed to retrieve the latency averages (time_to_completion), and also the input and output token averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "020ac599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_completion</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama-3.1-8b-instant</th>\n",
       "      <td>0.211944</td>\n",
       "      <td>463.778523</td>\n",
       "      <td>64.838926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-3.3-70b-versatile</th>\n",
       "      <td>0.251380</td>\n",
       "      <td>463.778523</td>\n",
       "      <td>65.279642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai/gpt-oss-120b</th>\n",
       "      <td>0.534279</td>\n",
       "      <td>508.805369</td>\n",
       "      <td>219.107383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai/gpt-oss-20b</th>\n",
       "      <td>0.394004</td>\n",
       "      <td>508.805369</td>\n",
       "      <td>293.143177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen/qwen3-32b</th>\n",
       "      <td>1.016569</td>\n",
       "      <td>438.067114</td>\n",
       "      <td>402.422819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         time_to_completion  input_tokens  output_tokens\n",
       "model                                                                   \n",
       "llama-3.1-8b-instant               0.211944    463.778523      64.838926\n",
       "llama-3.3-70b-versatile            0.251380    463.778523      65.279642\n",
       "openai/gpt-oss-120b                0.534279    508.805369     219.107383\n",
       "openai/gpt-oss-20b                 0.394004    508.805369     293.143177\n",
       "qwen/qwen3-32b                     1.016569    438.067114     402.422819"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.read_csv(\"groq-logs.csv\", encoding=\"utf-8\")\n",
    "original_df.groupby(\"model\")[[\"time_to_completion\",\"input_tokens\",\"output_tokens\"]].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
