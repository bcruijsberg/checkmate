{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c48b07",
   "metadata": {},
   "source": [
    "## Setting up a manually validated reference dataset\n",
    "\n",
    "This notebook outlines the methodology for constructing a manually validated reference dataset. A semi-automated annotation pipeline was emplyed where initial data processing is handled programmatically and manually\n",
    "\n",
    "First we will start with some preprocessing. To limit the number of factchecks for manual review, only factchecks after 2021 are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70be2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# eufactcheck=pd.read_csv('../EUfactcheckData/eufactcheck_posts_2019_2025.csv', encoding=\"utf-8\")\n",
    "\n",
    "# #get rid of leading and trailing quotes in titles\n",
    "# eufactcheck['title']= eufactcheck['title'].str.replace(r'^“+|”+$', '', regex=True).str.strip()\n",
    "\n",
    "# #filter only factcheck URLs and year after 2021\n",
    "# eufactcheck = eufactcheck[eufactcheck[\"url\"].str.contains(\"https://eufactcheck.eu/factcheck\", na=False)]\n",
    "# eufactcheck = eufactcheck[eufactcheck[\"year\"]>2021]\n",
    "\n",
    "# eufactcheck.to_csv('eufactcheck_factchecks_2019_2025.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83bb1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "#from langchain_ollama import ChatOllama\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Initialize Tavily client \n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "\n",
    "#llama 3.3 70b is good a multilingual tasks\n",
    "llm_llama = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.1)\n",
    "\n",
    "#GPT OSS 120B is good for reasoning tasks\n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405b7f0",
   "metadata": {},
   "source": [
    "A sample of 149 fact-checks from the EUfactcheck platform was used and manually added a source URL indicating where each claim was originally published online, when such information was available in the corresponding EUfactcheck.com article. This step was essential because incorporating source context, also accessible to the assistant, substantially increases the cognitive load on the LLM, making the presence or absence of a source link a critical variable in our evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7f87f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>origin</th>\n",
       "      <th>claim</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We have to provide our soldier with basic equi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-h...</td>\n",
       "      <td>https://www.vice.com/nl/article/horrorfans-kun...</td>\n",
       "      <td>Horror fans are better at coping during the gl...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Government measures positively affected Sloven...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wages grew more than prices in countries that ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2022 FIFA World Cup in Qatar is fully carb...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany faces the highest energy costs worldwi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Of the men who arrived in Germany in 2015/16, ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>https://www.spiegel.de/politik/deutschland/bij...</td>\n",
       "      <td>Permanent border controls are “a necessity” in...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/true-germanys...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany’s defense expenditures have increased ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost 80% of working women (in Germany) canno...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "1    https://eufactcheck.eu/factcheck/mostly-true-h...   \n",
       "2    https://eufactcheck.eu/factcheck/mostly-true-g...   \n",
       "3    https://eufactcheck.eu/factcheck/mostly-true-w...   \n",
       "4    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "..                                                 ...   \n",
       "147  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "148  https://eufactcheck.eu/factcheck/mostly-true-o...   \n",
       "149  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "150  https://eufactcheck.eu/factcheck/true-germanys...   \n",
       "151  https://eufactcheck.eu/factcheck/mostly-true-a...   \n",
       "\n",
       "                                                origin  \\\n",
       "0                                                  NaN   \n",
       "1    https://www.vice.com/nl/article/horrorfans-kun...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149  https://www.spiegel.de/politik/deutschland/bij...   \n",
       "150                                                NaN   \n",
       "151                                                NaN   \n",
       "\n",
       "                                                 claim        rating  year  \n",
       "0    We have to provide our soldier with basic equi...  MOSTLY FALSE  2021  \n",
       "1    Horror fans are better at coping during the gl...   MOSTLY TRUE  2021  \n",
       "2    Government measures positively affected Sloven...   MOSTLY TRUE  2021  \n",
       "3    Wages grew more than prices in countries that ...   MOSTLY TRUE  2022  \n",
       "4    The 2022 FIFA World Cup in Qatar is fully carb...  MOSTLY FALSE  2022  \n",
       "..                                                 ...           ...   ...  \n",
       "147  Germany faces the highest energy costs worldwi...  MOSTLY FALSE  2025  \n",
       "148  Of the men who arrived in Germany in 2015/16, ...   MOSTLY TRUE  2025  \n",
       "149  Permanent border controls are “a necessity” in...  MOSTLY FALSE  2025  \n",
       "150  Germany’s defense expenditures have increased ...          TRUE  2025  \n",
       "151  Almost 80% of working women (in Germany) canno...   MOSTLY TRUE  2025  \n",
       "\n",
       "[152 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "factchecks = pd.read_csv('eufactcheck_factchecks_edit_2019_2025.csv', sep=\";\",encoding=\"utf-8\")\n",
    "\n",
    "factchecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95cce2",
   "metadata": {},
   "source": [
    "A manual audit of the 149 most recent fact-check articles revealed considerable heterogeneity in source availability and format. Many articles linked to non-textual sources, such as videos or social media posts (e.g., X/Twitter), while others contained no external links at all. Moreover, even when primary URLs were provided, access was often restricted by paywalls or limited by technical scraping constraints. As a result, only 30 articles ultimately included a link to an original, retrievable source, and of those 30 (eval_ground.csv), after some manual cleaning 25 could be scraped and translated (eval_ground_clean.csv)\n",
    "\n",
    "As this is a European project, the majority of the 14 articles linked within the fact-checks were not published in English. However, the scope of this study is limited to the English language. Consequently, all non-English source texts were retrieved using the Tavily client, translated into English (with llama 3.3 70B, which excels in multilingual tasks), and then incorporated back into the dataset for analysis. Errors introduced during this process were identified through manual review and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e10d5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def process_and_translate(url):\n",
    "\n",
    "    # Only process if it's a valid string and starts with http\n",
    "    if not isinstance(url, str) or not url.startswith('http'):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract content using Tavily\n",
    "        extract_response = tavily_client.extract(urls=[url])\n",
    "        raw_text = extract_response['results'][0].get('raw_content', \"\")\n",
    "        \n",
    "        if not raw_text:\n",
    "            return \"No content extracted.\"\n",
    "\n",
    "        # Step 2: Set up the translation chain\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a professional translator. If the following text is in English, return it exactly as is. If it is in any other language, translate it into clear, fluent English.\"),\n",
    "            (\"user\", \"{text}\")\n",
    "        ])\n",
    "        \n",
    "        # Build the chain: Prompt -> LLM -> String Output\n",
    "        translation_chain = prompt | llm_llama | StrOutputParser()\n",
    "        \n",
    "        # Execute (truncating to 4000 chars to save context/costs)\n",
    "        return translation_chain.invoke({\"text\": raw_text[:4000]})\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4098b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your 'factchecks' dataset\n",
    "# factchecks['translated'] = factchecks['origin'].apply(process_and_translate)\n",
    "\n",
    "# # Save to file\n",
    "# factchecks[[\"url\",\"claim\",\"rating\",\"translated\",\"year\"]].to_csv('eval_ground.csv', sep=';', encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4534",
   "metadata": {},
   "source": [
    "### Step 1: is a claim checkable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99379220",
   "metadata": {},
   "source": [
    "The manually edited fact-check dataset was then loaded, with 14 entries containing translated versions of the original articles in which the claims were identified by the students. In the subsequent three steps, generated content was added to the dataset and manually verified by the user. To facilitate this verification process, a Streamlit-based interface was developed using a rapid, exploratory (“vibe coding”) approach.\n",
    "\n",
    "In this first step, it was checked if the claim is POTENTIALLY CHECKABLE OR UNCHECKABLE, which is also the first step in the Assistant after the user provide the claim. It also gives a short explanation and a question for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3106f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckResult(BaseModel):\n",
    "    checkable: Literal[\"POTENTIALLY CHECKABLE\", \"UNCHECKABLE\"]\n",
    "    explanation: str = Field(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da96995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "checkable_check_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst.\n",
    "\n",
    "### Inputs\n",
    "Claim: {claim}\n",
    "Dataset rating (validated reference label): {rating}\n",
    "\n",
    "### Task\n",
    "Classify the claim and determine if it can be fact-checked.\n",
    "\n",
    "### Classification Logic\n",
    "- **UNCHECKABLE**:\n",
    "  - Opinion or value judgment\n",
    "  - Prediction or future-oriented statement\n",
    "  - If rating is UNCHECKABLE, it probably is one of the above\n",
    "- **POTENTIALLY CHECKABLE**:\n",
    "  - Factual claims about the past or present\n",
    "  - Rating is not UNCHECKABLE\n",
    "\n",
    "### Task\n",
    "Use the dataset rating to set the checkability label:\n",
    "- If rating is \"UNCHECKABLE\" -> checkable MUST be \"UNCHECKABLE\"\n",
    "- Otherwise -> checkable MUST be \"POTENTIALLY CHECKABLE\"\n",
    "- Write a brief explanation why the claim is classified this way, don't mention the link with the rating, ONLY explain why you think it is UNCHECKABLE.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"checkable\": \"POTENTIALLY CHECKABLE | UNCHECKABLE\",\n",
    "  \"explanation\": \"Brief justification\",\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(checkable_check_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd16125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(CheckResult, method=\"json_mode\")\n",
    "    return prompt | structured_llm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_checkable_columns(df: pd.DataFrame, chain, claim_col: str = \"claim\", rating_col: str = \"rating\",) -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        claim = row[claim_col]\n",
    "        rating = row[rating_col]\n",
    "\n",
    "        return chain.invoke({\n",
    "            \"claim\": claim,\n",
    "            \"rating\": rating,\n",
    "        })\n",
    "\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "\n",
    "    # Add everything to the dataset\n",
    "    out[\"checkable\"] = results.apply(lambda r: r.checkable)\n",
    "    out[\"explanation\"] = results.apply(lambda r: r.explanation)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Call the chain building\n",
    "chain = build_chain(llmGPTOSS)\n",
    "\n",
    "# Run it for every row\n",
    "step1_df = pd.read_csv(\"eval_ground_clean.csv\", sep=';', encoding=\"utf-8\")\n",
    "step1_df = add_checkable_columns(step1_df, chain)\n",
    "\n",
    "# Save the dataset\n",
    "step1_df.to_csv(\"eval_ground_step1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db5860",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve al info from a claim and if available source\n",
    "\n",
    "Step 2 retrieves all information from the claim, which is also the second step in the assistant. In the assistant the user can also provide a url to the source where the claim was published (if it was published in an article), in this case the translate text will be used if such a source was found in the fact check article. As mentioned before this was only for 14 claims the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa12061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class RetrieveInfoResult(BaseModel):\n",
    "    claim_source: str = Field(\"unknown\")\n",
    "    primary_source: bool = Field(False)\n",
    "    source_description: str = Field(\"\")\n",
    "    subject: str = Field(\"unclear\")\n",
    "    quantitative: str = Field(\"\") \n",
    "    precision: str = Field(\"\")\n",
    "    based_on: str = Field(\"\")\n",
    "    question: str = Field(\"\")\n",
    "    alerts: List[str] = Field(default_factory=list)\n",
    "    geography: str = Field(\"unclear\")\n",
    "    time_period: str = Field(\"unclear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d657f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_info_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst. Focus on objective evaluation and guiding the user's reasoning through reflective inquiry rather than providing definitive answers.\n",
    "\n",
    "### Context\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Task 1: Source & Intent Extraction\n",
    "1. **claim_source**: Identify the person or organization who originated the claim.\n",
    "2. **primary_source**: Set to true ONLY if the evidence confirms this is the original/foundational origin.\n",
    "3. **source_description**: Describe the medium (e.g., \"Official PDF\", \"Social Media Post\").\n",
    "\n",
    "### Task 2: Factual Dimension Analysis\n",
    "1. **Subject**: Identify the core entity or event.\n",
    "2. **Quantitative/Qualitative**: Explain if it is measurable data or a description.\n",
    "3. **Precision**: Categorize as Precise, Vague, or Absolute (100%), and provide specific numbers, or names from the evidence.\n",
    "4. **Based On**: Identify the likely methodology (e.g., Official stats, Survey, research). Provide a brief explanation.\n",
    "5. **Geography**: Identify the geographic scope of the claim.\n",
    "6. **Time Period**: Identify the time frame relevant to the claim, if nothing available use {year}.\n",
    "\n",
    "### Task 3: Guidance & Risk\n",
    "1. **Alerts**: Flag missing Geography, Time Period, unclear subject, qualitative claim, vague quantitative claim, methodological details absent. Do not flag if the info is present.\n",
    "2. **The Question**: Formulate a polite open **question** asking for additional information, and ALWAYS ADD \"Or do you want to continue to the next step?\".\n",
    "3. Include specific details (dates, numbers, names) from the additional context if available:\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Output Format (JSON)\n",
    "{{\n",
    "  \"claim_source\": \"Person/Organisation\" or \"unknown\",\n",
    "  \"primary_source\": true/false,\n",
    "  \"source_description\": \"medium description\",\n",
    "  \"subject\": \"subject text\" or \"unclear\",\n",
    "  \"quantitative\": \"quantitative/qualitative + short explanation\",\n",
    "  \"precision\": \"precise/vague/absolute + specifics\",\n",
    "  \"based_on\": \"methodology + short explanation\" or \"unclear\",\n",
    "  \"question\": \"Polite open question asking for addional information, or if the user wants to continue\",\n",
    "  \"alerts\": [\"...\"],\n",
    "  \"geography\": \"...\" or \"unclear\",\n",
    "  \"time_period\": \"...\" or \"unclear\",\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(retrieve_info_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8dffec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_retrieve_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(RetrieveInfoResult, method=\"json_mode\")\n",
    "    return prompt | structured_llm\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_retrieved_info_columns(df: pd.DataFrame, chain, claim_col: str = \"claim\", context_col: str = \"translated\", year_col: int = \"year\") -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        claim = row[claim_col]\n",
    "        translated = row[context_col]\n",
    "        year = row[year_col]\n",
    "        additional_context = translated if isinstance(translated, str) and translated.strip() else \"\"\n",
    "\n",
    "        return chain.invoke({\n",
    "            \"claim\": claim,\n",
    "            \"year\":year,\n",
    "            \"additional_context\": additional_context,\n",
    "        })\n",
    "\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "\n",
    "    # Build the human-readable summary per row\n",
    "    def _details_text(r: RetrieveInfoResult) -> str:\n",
    "        text = (\n",
    "            f\"- claim_source: {r.claim_source or 'unknown'}\\n\"\n",
    "            f\"- primary_source: {r.primary_source}\\n\"\n",
    "            f\"- source_description: {r.source_description or 'not clearly specified'}\\n\"\n",
    "            f\"- subject: {r.subject or 'unclear'}\\n\"\n",
    "            f\"- quantitative: {r.quantitative or 'not clearly specified'}\\n\"\n",
    "            f\"- precision: {r.precision or 'not clearly specified'}\\n\"\n",
    "            f\"- based_on: {r.based_on or 'unclear'}\\n\"\n",
    "            f\"- geography: {r.geography or 'unclear'}\\n\"\n",
    "            f\"- time_period: {r.time_period or 'unclear'}\\n\"\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    # Add everything to the dataset\n",
    "    out[\"details_text\"] = results.apply(_details_text)\n",
    "    out[\"alerts\"] = results.apply(lambda r: r.alerts)\n",
    "    out[\"question\"] = results.apply(lambda r: r.question)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f18e3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the chain building\n",
    "chain = build_retrieve_chain(llmGPTOSS)\n",
    "\n",
    "# Run it for every row\n",
    "step2_df = pd.read_csv(\"eval_ground_step1.csv\", encoding=\"utf-8\")\n",
    "step2_df = add_retrieved_info_columns(step2_df, chain)\n",
    "\n",
    "# Save the dataset\n",
    "step2_df.to_csv(\"eval_ground_step2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0656352",
   "metadata": {},
   "source": [
    "### Step 3 Adding users confirmation\n",
    "\n",
    "Checking if an llm can deduce from a user's answer if he confirms and wants to continue or not, is also part of the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e28f654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class AnswerResult(BaseModel):\n",
    "    answer: str = Field(\"\")\n",
    "\n",
    "class ConfirmResult(BaseModel):\n",
    "    confirmed: bool = Field(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76d017d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to confirm the extracted information with the user\n",
    "answer_prompt = \"\"\"\n",
    "### Role\n",
    "You are a student working with a fact-checking assistant. Reply in the voice of a student.\n",
    "\n",
    "### Context\n",
    "- Question: \"{question}\"\n",
    "- Should you indicate a desire to move on? {move_on_hint}\n",
    "\n",
    "### Task\n",
    "Respond to the assistant’s question naturally and concisely.\n",
    "\n",
    "### Strategy based on Move On Hint\n",
    "- IF \"{move_on_hint}\" is YES: Ensure your response ends with a \"Green Light\" phrase (e.g., \"Next,\" \"Move on,\" \"Go ahead,\" \"Proceed\").\n",
    "- IF \"{move_on_hint}\" is NO: Provide the information naturally but DO NOT use any \"move on\" or \"next step\" language.\n",
    "\n",
    "### Examples\n",
    "- (YES): \"Shared on FB by a friend (June 12). Move on.\"\n",
    "- (NO): \"I saw it in a WHO 2022 report on Air Quality.\"\n",
    "- (YES): \"Unsure of source. Next step.\"\n",
    "\n",
    "### Important\n",
    "- Don't ask questions.\n",
    "- Maintain a natural student persona.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"answer\": \"your concise response here\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "a_prompt = ChatPromptTemplate.from_template(answer_prompt)\n",
    "\n",
    "# Prompt to confirm the extracted information with the user\n",
    "confirm_prompt = \"\"\"\n",
    "### Role\n",
    "Linguistic Analyst specializing in intent detection.\n",
    "\n",
    "### Context\n",
    "- User's Response: \"{user_answer}\"\n",
    "\n",
    "### Task\n",
    "Determine if the User's Response provides a \"Green Light\" to proceed.\n",
    "\n",
    "### Decision Rules\n",
    "**Set \"confirmed\": true IF:**\n",
    "- User explicitly agrees (e.g., \"Yes,\" \"Correct,\" \"Exactly\").\n",
    "- User provides a neutral command somwhere in the answer to proceed (e.g., \"Continue,\" \"Next\" \"Proceed\" \"Move on\").\n",
    "- User admits they have no more information (e.g., \"I don't know,\" \"That's all I have,\" \"No more details\").\n",
    "\n",
    "**Set \"confirmed\": false IF:**\n",
    "- User provides **new additional context or corrections** (even if they agree with the rest).\n",
    "- User expresses uncertainty or asks a new question.\n",
    "\n",
    "### Important Rules\n",
    "- Maintain a neutral, analytical tone.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"confirmed\": boolean\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "c_prompt = ChatPromptTemplate.from_template(confirm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dee0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chains\n",
    "def build_answer_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(AnswerResult, method=\"json_mode\")\n",
    "    return a_prompt | structured_llm\n",
    "\n",
    "def build_confirm_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(ConfirmResult, method=\"json_mode\")\n",
    "    return c_prompt | structured_llm\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_answer_confirm_columns(df: pd.DataFrame, answer_chain, confirm_chain, question_col: str = \"question\") -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # Store history of whether the last 20 answers included a \"move on\" intent\n",
    "    history = []\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "\n",
    "        question = row[question_col]\n",
    "\n",
    "        # Look at the last 20 results to see how many were \"move on\" requests\n",
    "        recent_history = history[-20:]\n",
    "        move_on_count = sum(recent_history)\n",
    "        \n",
    "        # Calculate if we need more or fewer \"move on\" indicators to stay near 50%\n",
    "        should_move_on_hint = \"YES\" if (len(recent_history) > 0 and move_on_count / len(recent_history) < 0.5) else \"NO\"\n",
    "\n",
    "        # Generate answer with history context\n",
    "        answer = answer_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"move_on_hint\": should_move_on_hint\n",
    "        })\n",
    "\n",
    "        # Generate a confirmation\n",
    "        confirm = confirm_chain.invoke({\n",
    "            \"user_answer\": answer.answer,\n",
    "        })\n",
    "\n",
    "        # Update history (True = 1, False = 0)\n",
    "        history.append(1 if confirm.confirmed else 0)\n",
    "\n",
    "        return answer.answer, confirm.confirmed\n",
    "\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "\n",
    "    # Add everything to the dataset\n",
    "    out[\"user_answer\"] = results.apply(lambda t: t[0])\n",
    "    out[\"confirmed\"] = results.apply(lambda t: t[1])\n",
    "    return out\n",
    "\n",
    "# Call the chain building\n",
    "answer_chain = build_answer_chain(llmGPTOSS)\n",
    "confirm_chain = build_confirm_chain(llmGPTOSS)\n",
    "\n",
    "# Run it for every row\n",
    "step3_df = pd.read_csv(\"eval_ground_step2.csv\", encoding=\"utf-8\")\n",
    "step3_df = add_answer_confirm_columns(step3_df, answer_chain, confirm_chain, question_col=\"question\")\n",
    "\n",
    "# Save the dataset\n",
    "step3_df.to_csv(\"eval_ground_step3.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
