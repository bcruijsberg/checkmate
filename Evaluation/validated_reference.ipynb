{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c48b07",
   "metadata": {},
   "source": [
    "## Setting up a manually validated reference dataset\n",
    "\n",
    "This notebook outlines the methodology for constructing a manually validated reference dataset. A semi-automated annotation pipeline was emplyed where initial data processing is handled programmatically and manually\n",
    "\n",
    "First we will start with some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70be2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eufactcheck=pd.read_csv('../EUfactcheckData/eufactcheck_posts_2019_2025.csv', encoding=\"utf-8\")\n",
    "\n",
    "#get rid of leading and trailing quotes in titles\n",
    "eufactcheck['title']= eufactcheck['title'].str.replace(r'^“+|”+$', '', regex=True).str.strip()\n",
    "\n",
    "#filter only factcheck URLs\n",
    "eufactcheck = eufactcheck[eufactcheck[\"url\"].str.contains(\"https://eufactcheck.eu/factcheck\", na=False)]\n",
    "\n",
    "eufactcheck.to_csv('eufactcheck_factchecks_2019_2025.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bb1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\checkmate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "#from langchain_ollama import ChatOllama\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Initialize Tavily client \n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "\n",
    "#llama 3.3 70b is good a multilingual tasks\n",
    "llm_llama = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.1)\n",
    "\n",
    "#GPT OSS 120B is good for reasoning tasks\n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405b7f0",
   "metadata": {},
   "source": [
    "A sample of 152 fact-checks from the EUfactcheck platform was used and manually added a source URL indicating where each claim was originally published online, when such information was available in the corresponding EUfactcheck.com article.\n",
    "\n",
    "This step was essential because incorporating source context, also accessible to the assistant, substantially increases the cognitive load on the LLM, making the presence or absence of a source link a critical variable in our evaluation. A manual audit of the 152 most recent fact-check articles revealed considerable heterogeneity in source availability and format. Many articles linked to non-textual sources, such as videos or social media posts (e.g., X/Twitter), while others contained no external links at all. Moreover, even when primary URLs were provided, access was often restricted by paywalls or limited by technical scraping constraints. As a result, only 14 articles ultimately included a link to an original, retrievable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7f87f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>origin</th>\n",
       "      <th>claim</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We have to provide our soldier with basic equi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-h...</td>\n",
       "      <td>https://www.vice.com/nl/article/horrorfans-kun...</td>\n",
       "      <td>Horror fans are better at coping during the gl...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Government measures positively affected Sloven...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wages grew more than prices in countries that ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2022 FIFA World Cup in Qatar is fully carb...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany faces the highest energy costs worldwi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Of the men who arrived in Germany in 2015/16, ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>https://www.spiegel.de/politik/deutschland/bij...</td>\n",
       "      <td>Permanent border controls are “a necessity” in...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/true-germanys...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany’s defense expenditures have increased ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost 80% of working women (in Germany) canno...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "1    https://eufactcheck.eu/factcheck/mostly-true-h...   \n",
       "2    https://eufactcheck.eu/factcheck/mostly-true-g...   \n",
       "3    https://eufactcheck.eu/factcheck/mostly-true-w...   \n",
       "4    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "..                                                 ...   \n",
       "147  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "148  https://eufactcheck.eu/factcheck/mostly-true-o...   \n",
       "149  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "150  https://eufactcheck.eu/factcheck/true-germanys...   \n",
       "151  https://eufactcheck.eu/factcheck/mostly-true-a...   \n",
       "\n",
       "                                                origin  \\\n",
       "0                                                  NaN   \n",
       "1    https://www.vice.com/nl/article/horrorfans-kun...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149  https://www.spiegel.de/politik/deutschland/bij...   \n",
       "150                                                NaN   \n",
       "151                                                NaN   \n",
       "\n",
       "                                                 claim        rating  year  \n",
       "0    We have to provide our soldier with basic equi...  MOSTLY FALSE  2021  \n",
       "1    Horror fans are better at coping during the gl...   MOSTLY TRUE  2021  \n",
       "2    Government measures positively affected Sloven...   MOSTLY TRUE  2021  \n",
       "3    Wages grew more than prices in countries that ...   MOSTLY TRUE  2022  \n",
       "4    The 2022 FIFA World Cup in Qatar is fully carb...  MOSTLY FALSE  2022  \n",
       "..                                                 ...           ...   ...  \n",
       "147  Germany faces the highest energy costs worldwi...  MOSTLY FALSE  2025  \n",
       "148  Of the men who arrived in Germany in 2015/16, ...   MOSTLY TRUE  2025  \n",
       "149  Permanent border controls are “a necessity” in...  MOSTLY FALSE  2025  \n",
       "150  Germany’s defense expenditures have increased ...          TRUE  2025  \n",
       "151  Almost 80% of working women (in Germany) canno...   MOSTLY TRUE  2025  \n",
       "\n",
       "[152 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "factchecks = pd.read_csv('eufactcheck_factchecks_2019_2025.csv', sep=';', encoding=\"utf-8\")\n",
    "\n",
    "factchecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95cce2",
   "metadata": {},
   "source": [
    "As this is a European project, the majority of the 14 articles linked within the fact-checks were not published in English. However, the scope of this study is limited to the English language. Consequently, all non-English source texts were retrieved using the Tavily client, translated into English (with llama 3.3 70B, which excels in multilingual tasks), and then incorporated back into the dataset for analysis. Errors introduced during this process were identified through manual review and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e10d5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def process_and_translate(url):\n",
    "\n",
    "    # Only process if it's a valid string and starts with http\n",
    "    if not isinstance(url, str) or not url.startswith('http'):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract content using Tavily\n",
    "        extract_response = tavily_client.extract(urls=[url])\n",
    "        raw_text = extract_response['results'][0].get('raw_content', \"\")\n",
    "        \n",
    "        if not raw_text:\n",
    "            return \"No content extracted.\"\n",
    "\n",
    "        # Step 2: Set up the translation chain\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a professional translator. If the following text is in English, return it exactly as is. If it is in any other language, translate it into clear, fluent English.\"),\n",
    "            (\"user\", \"{text}\")\n",
    "        ])\n",
    "        \n",
    "        # Build the chain: Prompt -> LLM -> String Output\n",
    "        translation_chain = prompt | llm_llama | StrOutputParser()\n",
    "        \n",
    "        # Execute (truncating to 4000 chars to save context/costs)\n",
    "        return translation_chain.invoke({\"text\": raw_text[:4000]})\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4098b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>origin</th>\n",
       "      <th>claim</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We have to provide our soldier with basic equi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-h...</td>\n",
       "      <td>https://www.vice.com/nl/article/horrorfans-kun...</td>\n",
       "      <td>Horror fans are better at coping during the gl...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "      <td>Error: list index out of range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Government measures positively affected Sloven...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wages grew more than prices in countries that ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2022</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2022 FIFA World Cup in Qatar is fully carb...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2022</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany faces the highest energy costs worldwi...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Of the men who arrived in Germany in 2015/16, ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-false-...</td>\n",
       "      <td>https://www.spiegel.de/politik/deutschland/bij...</td>\n",
       "      <td>Permanent border controls are “a necessity” in...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>2025</td>\n",
       "      <td>Error: list index out of range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/true-germanys...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany’s defense expenditures have increased ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://eufactcheck.eu/factcheck/mostly-true-a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost 80% of working women (in Germany) canno...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "1    https://eufactcheck.eu/factcheck/mostly-true-h...   \n",
       "2    https://eufactcheck.eu/factcheck/mostly-true-g...   \n",
       "3    https://eufactcheck.eu/factcheck/mostly-true-w...   \n",
       "4    https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "..                                                 ...   \n",
       "147  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "148  https://eufactcheck.eu/factcheck/mostly-true-o...   \n",
       "149  https://eufactcheck.eu/factcheck/mostly-false-...   \n",
       "150  https://eufactcheck.eu/factcheck/true-germanys...   \n",
       "151  https://eufactcheck.eu/factcheck/mostly-true-a...   \n",
       "\n",
       "                                                origin  \\\n",
       "0                                                  NaN   \n",
       "1    https://www.vice.com/nl/article/horrorfans-kun...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149  https://www.spiegel.de/politik/deutschland/bij...   \n",
       "150                                                NaN   \n",
       "151                                                NaN   \n",
       "\n",
       "                                                 claim        rating  year  \\\n",
       "0    We have to provide our soldier with basic equi...  MOSTLY FALSE  2021   \n",
       "1    Horror fans are better at coping during the gl...   MOSTLY TRUE  2021   \n",
       "2    Government measures positively affected Sloven...   MOSTLY TRUE  2021   \n",
       "3    Wages grew more than prices in countries that ...   MOSTLY TRUE  2022   \n",
       "4    The 2022 FIFA World Cup in Qatar is fully carb...  MOSTLY FALSE  2022   \n",
       "..                                                 ...           ...   ...   \n",
       "147  Germany faces the highest energy costs worldwi...  MOSTLY FALSE  2025   \n",
       "148  Of the men who arrived in Germany in 2015/16, ...   MOSTLY TRUE  2025   \n",
       "149  Permanent border controls are “a necessity” in...  MOSTLY FALSE  2025   \n",
       "150  Germany’s defense expenditures have increased ...          TRUE  2025   \n",
       "151  Almost 80% of working women (in Germany) canno...   MOSTLY TRUE  2025   \n",
       "\n",
       "                         translated  \n",
       "0                              None  \n",
       "1    Error: list index out of range  \n",
       "2                              None  \n",
       "3                              None  \n",
       "4                              None  \n",
       "..                              ...  \n",
       "147                            None  \n",
       "148                            None  \n",
       "149  Error: list index out of range  \n",
       "150                            None  \n",
       "151                            None  \n",
       "\n",
       "[152 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to your 'factchecks' dataset\n",
    "factchecks['translated'] = factchecks['origin'].apply(process_and_translate)\n",
    "\n",
    "# Save to file\n",
    "factchecks[[\"url\",\"claim\",\"rating\",\"translated\",\"year\"]].to_csv('eval_ground.csv', sep=';', encoding=\"utf-8\", index=False)\n",
    "\n",
    "factchecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4534",
   "metadata": {},
   "source": [
    "### Step 1: is a claim checkable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99379220",
   "metadata": {},
   "source": [
    "The manually edited dataset of fact-checks was loaded, with 14 entries containing translated versions of the original articles, were the student found the claim.\n",
    "\n",
    "In this first step, it was checked if the claim is POTENTIALLY CHECKABLE OR UNCHECKABLE, which is also the first step in the Assistant after the user provide the claim. It also gives a short explanation and a question for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3106f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SubjectResult(BaseModel):\n",
    "    checkable: Literal[\"POTENTIALLY CHECKABLE\", \"UNCHECKABLE\"]\n",
    "    explanation: str = Field(\"\")\n",
    "    question: str = Field(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da96995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "checkable_check_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst.\n",
    "\n",
    "### Inputs\n",
    "Claim: {claim}\n",
    "Dataset rating (validated reference label): {rating}\n",
    "\n",
    "### Task\n",
    "Classify the claim and determine if it can be fact-checked.\n",
    "\n",
    "### Classification Logic\n",
    "- **UNCHECKABLE**:\n",
    "  - Opinion or value judgment\n",
    "  - Prediction or future-oriented statement\n",
    "  - If rating is UNCHECKABLE, it probably is one of the above\n",
    "- **POTENTIALLY CHECKABLE**:\n",
    "  - Factual claims about the past or present\n",
    "  - Rating is not UNCHECKABLE\n",
    "\n",
    "### Task\n",
    "Use the dataset rating to set the checkability label:\n",
    "- If rating is \"UNCHECKABLE\" -> checkable MUST be \"UNCHECKABLE\"\n",
    "- Otherwise -> checkable MUST be \"POTENTIALLY CHECKABLE\"\n",
    "\n",
    "Then:\n",
    "1) Write a brief explanation why the claim is classified this way, don't mention the link with the rating, ONLY explain why you think it is UNCHECKABLE.\n",
    "2) Ask a polite confirmation question to the user (do not offer help).\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"checkable\": \"POTENTIALLY CHECKABLE | UNCHECKABLE\",\n",
    "  \"explanation\": \"Brief justification\",\n",
    "  \"question\": \"Polite confirmation question.\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(checkable_check_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd16125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(SubjectResult, method=\"json_mode\")\n",
    "    return prompt | structured_llm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_checkable_columns(df: pd.DataFrame, chain, claim_col: str = \"claim\", rating_col: str = \"rating\",) -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        claim = row[claim_col]\n",
    "        rating = row[rating_col]\n",
    "\n",
    "        return chain.invoke({\n",
    "            \"claim\": claim,\n",
    "            \"rating\": rating,\n",
    "        })\n",
    "\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "\n",
    "    # Add everything to the dataset\n",
    "    out[\"checkable\"] = results.apply(lambda r: r.checkable)\n",
    "    out[\"explanation\"] = results.apply(lambda r: r.explanation)\n",
    "    out[\"question\"] = results.apply(lambda r: r.question)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Call the chain building\n",
    "chain = build_chain(llmGPTOSS)\n",
    "\n",
    "# Run it for every row\n",
    "step1_df = pd.read_csv(\"eval_ground_v2.csv\", sep=';', encoding=\"utf-8\")\n",
    "step1_df = add_checkable_columns(step1_df, chain)\n",
    "\n",
    "# Save the dataset\n",
    "step1_df.to_csv(\"eval_ground_step1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db5860",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve al info from a claim and if available source\n",
    "\n",
    "Step 2 retrieves all information from the claim, which is also the second step in the assistant. In the assistant the user can also provide a url to the source where the claim was published (if it was published in an article), in this case the translate text will be used if such a source was found in the fact check article. As mentioned before this was only for 14 claims the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa12061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class RetrieveInfoResult(BaseModel):\n",
    "    claim_source: str = Field(\"unknown\")\n",
    "    primary_source: bool = Field(False)\n",
    "    source_description: str = Field(\"\")\n",
    "    subject: str = Field(\"unclear\")\n",
    "    quantitative: str = Field(\"\") \n",
    "    precision: str = Field(\"\")\n",
    "    based_on: str = Field(\"\")\n",
    "    question: str = Field(\"\")\n",
    "    alerts: List[str] = Field(default_factory=list)\n",
    "    geography: str = Field(\"unclear\")\n",
    "    time_period: str = Field(\"unclear\")\n",
    "    details: str = Field(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d657f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_info_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst. Focus on objective evaluation and guiding the user's reasoning through reflective inquiry rather than providing definitive answers.\n",
    "\n",
    "### Context\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Task 1: Source & Intent Extraction\n",
    "1. **claim_source**: Identify the person or organization who originated the claim.\n",
    "2. **primary_source**: Set to true ONLY if the evidence confirms this is the original/foundational origin.\n",
    "3. **source_description**: Describe the medium (e.g., \"Official PDF\", \"Social Media Post\").\n",
    "\n",
    "### Task 2: Factual Dimension Analysis\n",
    "1. **Subject**: Identify the core entity or event.\n",
    "2. **Quantitative/Qualitative**: Explain if it is measurable data or a description.\n",
    "3. **Precision**: Categorize as Precise, Vague, or Absolute (100%), and provide specific numbers, or names from the evidence.\n",
    "4. **Based On**: Identify the likely methodology (e.g., Official stats, Survey, research). Provide a brief explanation.\n",
    "5. **Geography**: Identify the geographic scope of the claim.\n",
    "6. **Time Period**: Identify the time frame relevant to the claim, if nothing available use {year}.\n",
    "\n",
    "### Task 3: Guidance & Risk\n",
    "1. **Alerts**: Flag missing Geography, Time Period, unclear subject, qualitative claim, vague quantitative claim, methodological details absent. Do not flag if the info is present.\n",
    "2. **The Question**: Formulate exactly **one** polite, open-ended question to help the user refine the claim.\n",
    "3. **details**: Include specific details (dates, numbers, names) from the additional context if available:\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Output Format (JSON)\n",
    "{{\n",
    "  \"claim_source\": \"Person/Organisation\" or \"unknown\",\n",
    "  \"primary_source\": true/false,\n",
    "  \"source_description\": \"medium description\",\n",
    "  \"subject\": \"subject text\" or \"unclear\",\n",
    "  \"quantitative\": \"quantitative/qualitative + short explanation\",\n",
    "  \"precision\": \"precise/vague/absolute + specifics\",\n",
    "  \"based_on\": \"methodology + short explanation\" or \"unclear\",\n",
    "  \"question\": \"one polite open question\",\n",
    "  \"alerts\": [\"...\"],\n",
    "  \"geography\": \"...\" or \"unclear\",\n",
    "  \"time_period\": \"...\" or \"unclear\",\n",
    "  \"details\": \"specific extracted details\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "retrieve_prompt = ChatPromptTemplate.from_template(retrieve_info_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dffec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_retrieve_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(RetrieveInfoResult, method=\"json_mode\")\n",
    "    return retrieve_prompt | structured_llm\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_retrieved_info_columns(df: pd.DataFrame, chain, claim_c: str = \"claim\", context_c: str = \"translated\", year_c: int = \"year\") -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        claim = row[claim_c]\n",
    "        translated = row[context_c]\n",
    "        year = row[year_c]\n",
    "        additional_context = translated if isinstance(translated, str) and translated.strip() else \"\"\n",
    "\n",
    "        return chain.invoke({\n",
    "            \"claim\": claim,\n",
    "            \"year\":year,\n",
    "            \"additional_context\": additional_context,\n",
    "        })\n",
    "\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "\n",
    "    # Build the human-readable summary per row\n",
    "    def _details_text(r: RetrieveInfoResult) -> str:\n",
    "        text = (\n",
    "            f\"- claim_source: {r.claim_source or 'unknown'}\\n\"\n",
    "            f\"- primary_source: {r.primary_source}\\n\"\n",
    "            f\"- source_description: {r.source_description or 'not clearly specified'}\\n\"\n",
    "            f\"- subject: {r.subject or 'unclear'}\\n\"\n",
    "            f\"- quantitative: {r.quantitative or 'not clearly specified'}\\n\"\n",
    "            f\"- precision: {r.precision or 'not clearly specified'}\\n\"\n",
    "            f\"- based_on: {r.based_on or 'unclear'}\\n\"\n",
    "            f\"- geography: {r.geography or 'unclear'}\\n\"\n",
    "            f\"- time_period: {r.time_period or 'unclear'}\\n\"\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    # Add everything to the dataset\n",
    "    out[\"details_text\"] = results.apply(_details_text)\n",
    "    out[\"alerts\"] = results.apply(lambda r: r.alerts)\n",
    "    out[\"question\"] = results.apply(lambda r: r.question)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18e3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the chain building\n",
    "chain = build_retrieve_chain(llmGPTOSS)\n",
    "\n",
    "# Run it for every row\n",
    "step2_df = pd.read_csv(\"eval_ground_step1.csv\", encoding=\"utf-8\")\n",
    "step2_df = add_retrieved_info_columns(step2_df, chain)\n",
    "\n",
    "# Save the dataset\n",
    "step2_df.to_csv(\"eval_ground_step2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0656352",
   "metadata": {},
   "source": [
    "Finally, a streamlit interface was created with vibe coding, to verify all the answers manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
