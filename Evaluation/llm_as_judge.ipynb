{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f8bc32",
   "metadata": {},
   "source": [
    "## LLM as judge evaluation\n",
    "\n",
    "In **part 1** different models will perform the same 3 tasks as are in the ground truth model (and in the assistant). \n",
    "1. The first task is to check if a claim is checkable.\n",
    "2. The next task is to retrieve all relevant information, and if available also additional context from the original source.\n",
    "3. Finally a confirmation prompt will try to decide is the user confirms and wants to continue or not.\n",
    "\n",
    "In **part 2**\n",
    "Two larger different models will score the generated output of each model and task against the ground truth (the manually verified output). \n",
    "All tasks will be scored on a binary scale\n",
    "1. The first task will be scored on *Reasoning*, with 0=\"reasoning not that clear\", 1=\"clear reasoning\"\n",
    "2. The second task will be scored on:\n",
    "    - *Completeness*, with 0=\"missing facts\", 1=\"complete\"\n",
    "    - *Hallucination*, with 0=\"contains hallucinated facts\", 1=\"no hallucinations found\"\n",
    "3. The last task will be scored on *Intend* (did the user want to proceed or not), with 0=\"confirmed differs\", 1=\"confirmed is the same\"\n",
    "\n",
    "### Part 1 Generating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "02cfebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Initialize Tavily client \n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "# Models on Groq, to evaluate\n",
    "llama8 = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1)\n",
    "GPTOSS20 = ChatGroq(model_name=\"openai/gpt-oss-20b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)\n",
    "qwen3_32 = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "llama70 = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.1)\n",
    "GPTOSS120 = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "befd857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckResult(BaseModel):\n",
    "    checkable: Literal[\"POTENTIALLY CHECKABLE\", \"UNCHECKABLE\"]\n",
    "    explanation: str = Field(\"\")\n",
    "\n",
    "class RetrieveInfoResult(BaseModel):\n",
    "    claim_source: str = Field(\"unknown\")\n",
    "    primary_source: bool = Field(False)\n",
    "    source_description: str = Field(\"\")\n",
    "    subject: str = Field(\"unclear\")\n",
    "    quantitative: str = Field(\"\") \n",
    "    precision: str = Field(\"\")\n",
    "    based_on: str = Field(\"\")\n",
    "    alerts: str = Field(\"\",description=\"A comma-separated list of flags or 'None' if no issues found\")\n",
    "    geography: str = Field(\"unclear\")\n",
    "    time_period: str = Field(\"unclear\")\n",
    "\n",
    "class ConfirmResult(BaseModel):\n",
    "    confirmed: bool = Field(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b90f9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "checkable_check_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst.\n",
    "\n",
    "### Inputs\n",
    "Claim: {claim}\n",
    "\n",
    "### Task\n",
    "Classify the claim and determine if it can be fact-checked.\n",
    "\n",
    "### Classification Logic\n",
    "- **UNCHECKABLE**: Opinion, value judgment, or prediction.\n",
    "- **POTENTIALLY CHECKABLE**: Factual claims about the past or present.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"checkable\": \"POTENTIALLY CHECKABLE\",\n",
    "  \"explanation\": \"Brief justification for the classification.\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "c_prompt = ChatPromptTemplate.from_template(checkable_check_prompt)\n",
    "\n",
    "retrieve_info_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst. Focus on objective evaluation.\n",
    "\n",
    "### Context\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Task 1: Source & Intent Extraction\n",
    "1. **claim_source**: Identify the person or organization who originated the claim.\n",
    "2. **primary_source**: Set to true ONLY if the evidence confirms this is the original/foundational origin.\n",
    "3. **source_description**: Describe the medium (e.g., \"Official PDF\", \"Social Media Post\").\n",
    "\n",
    "### Task 2: Factual Dimension Analysis\n",
    "1. **Subject**: Identify the core entity or event.\n",
    "2. **Quantitative/Qualitative**: Explain if it is measurable data or a description.\n",
    "3. **Precision**: Categorize as Precise, Vague, or Absolute (100%), and provide specific numbers, or names from the evidence.\n",
    "4. **Based On**: Identify the likely methodology (e.g., Official stats, Survey, research). Provide a brief explanation.\n",
    "5. **Geography**: Identify the geographic scope of the claim.\n",
    "6. **Time Period**: The time frame is normally {year}, unless otherwise stated in the claim or additional context.\n",
    "\n",
    "### Task 3: Guidance & Risk\n",
    "1. **Alerts**: Flag \"missing geography\", \"qualitative claim\", \"missing methodological details\", \"missing source\". \n",
    "  If the claim is quantitative, but no numbers are provide or textual precise indications (e.g., \"All\", \"None\"), all), *Flag in Alerts*: \"vague quantitative claim\".\n",
    "  Do not flag if the info is present.\n",
    "2. Include specific details (dates, numbers, names) from the additional context if available:\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Output Format (JSON)\n",
    "{{\n",
    "  \"claim_source\": \"a Person or an Organisation\" or \"unknown\"\n",
    "  \"primary_source\": true/false\n",
    "  \"source_description\": \"medium description\"\n",
    "  \"subject\": \"subject text\" or \"unclear\"\n",
    "  \"quantitative\": \"quantitative/qualitative and a  short explanation\"\n",
    "  \"precision\": \"precise/vague/absolute and some specifics\"\n",
    "  \"based_on\": \"methodology and short explanation\" or \"unclear\"\n",
    "  \"alerts\": \"string containing all alerts separated by commas\",\n",
    "  \"geography\": \"Geography\",\n",
    "  \"time_period\": \"Year or more precise timeframe\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "r_prompt = ChatPromptTemplate.from_template(retrieve_info_prompt)\n",
    "\n",
    "# Prompt to confirm the extracted information with the user\n",
    "intent_prompt = \"\"\"\n",
    "### Role\n",
    "Linguistic Analyst specializing in intent detection.\n",
    "\n",
    "### Context\n",
    "- User's Response: \"{user_answer}\"\n",
    "\n",
    "### Task\n",
    "Analyze the \"User's Response\" to determine if the assistant has permission to proceed to the next stage of the process (\"Green Light\").\n",
    "\n",
    "### Decision Rules\n",
    "**Set \"confirmed\": true IF:**\n",
    "1. **Explicit Command:** The user uses navigational keywords (e.g., \"Next,\" \"Continue,\" \"Proceed,\" \"Move on,\" \"Go ahead\").\n",
    "2. **Affirmation:** The user agrees with a previous summary (e.g., \"Yes,\" \"Correct,\" \"Exactly,\" \"That's right\").\n",
    "3. **Closure:** The user indicates they have no further details to provide (e.g., \"I don't know,\" \"That's all I have,\" \"No more info\").\n",
    "\n",
    "**Set \"confirmed\": false IF:**\n",
    "1. **New Info Only:** The user provides additional facts or context but does NOT include a command to move on.\n",
    "2. **Correction:** The user is correcting a previous mistake.\n",
    "3. **Uncertainty:** The user asks a follow-up question or expresses confusion.\n",
    "\n",
    "### Important Rules\n",
    "- Focus on the **intent to progress**.\n",
    "- If the response is ambiguous, default to `false`.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"confirmed\": true\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "i_prompt = ChatPromptTemplate.from_template(intent_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c5a88531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add checkable columns to DataFrame\n",
    "def add_model_columns(\n",
    "        df: pd.DataFrame, \n",
    "        retrieval_chain, \n",
    "        check_chain,\n",
    "        intent_chain,\n",
    "        model_name: str, \n",
    "        claim_col: str = \"claim\", \n",
    "        context_col: str = \"translated\", \n",
    "        year_col: int = \"year\",\n",
    "        user_answer_col: str = \"user_answer\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        claim = row[claim_col]\n",
    "        translated = row[context_col]\n",
    "        year = row[year_col]\n",
    "        user_answer= row[user_answer_col]\n",
    "        additional_context = translated if isinstance(translated, str) and translated.strip() else \"\"\n",
    "\n",
    "        check= check_chain.invoke({\n",
    "            \"claim\": claim,\n",
    "        })\n",
    "\n",
    "        retrieval= retrieval_chain.invoke({\n",
    "            \"claim\": claim,\n",
    "            \"year\":year,\n",
    "            \"additional_context\": additional_context\n",
    "        })\n",
    "\n",
    "        intent= intent_chain.invoke({\n",
    "            \"user_answer\": user_answer,\n",
    "        })\n",
    "\n",
    "        # Build the human-readable summary per row\n",
    "        details_text = (\n",
    "            f\"- claim_source: {retrieval.claim_source}\\n\"\n",
    "            f\"- primary_source: {retrieval.primary_source}\\n\"\n",
    "            f\"- source_description: {retrieval.source_description}\\n\"\n",
    "            f\"- subject: {retrieval.subject}\\n\"\n",
    "            f\"- quantitative: {retrieval.quantitative or 'not clearly specified'}\\n\"\n",
    "            f\"- precision: {retrieval.precision}\\n\"\n",
    "            f\"- based_on: {retrieval.based_on}\\n\"\n",
    "            f\"- geography: {retrieval.geography}\\n\"\n",
    "            f\"- time_period: {retrieval.time_period}\\n\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Return a Series so we can easily join it back to the dataframe\n",
    "        return pd.Series({\n",
    "            f\"checkable_{model_name}\": check.checkable,\n",
    "            f\"explanation_{model_name}\": check.explanation,\n",
    "            f\"details_{model_name}\": details_text,\n",
    "            f\"alerts_{model_name}\": retrieval.alerts,\n",
    "            f\"confirmed_{model_name}\": intent.confirmed,\n",
    "        })\n",
    "\n",
    "    # Apply the function and join the new columns to the original dataframe\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "    out = pd.concat([out, results], axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4357ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for: llama8...\n",
      "Running evaluation for: gptoss20...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_check_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(CheckResult, method=\"json_mode\")\n",
    "    return c_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_retrieval_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(RetrieveInfoResult, method=\"json_mode\")\n",
    "    return r_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_intent_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(ConfirmResult, method=\"json_mode\")\n",
    "    return i_prompt | structured_llm\n",
    "\n",
    "# Build chains for Llama8\n",
    "llama8_retrieval = build_retrieval_chain(llama8)\n",
    "llama8_check = build_check_chain(llama8)\n",
    "llama8_intent = build_intent_chain(llama8)\n",
    "\n",
    "# Build chains for GPT OSS-20B\n",
    "gpt20_retrieval = build_retrieval_chain(GPTOSS20)\n",
    "gpt20_check = build_check_chain(GPTOSS20)\n",
    "gpt20_intent = build_intent_chain(GPTOSS20)\n",
    "\n",
    "# Build chains for GPT OSS-120B\n",
    "gpt120_retrieval = build_retrieval_chain(GPTOSS120)\n",
    "gpt120_check = build_check_chain(GPTOSS120)\n",
    "gpt120_intent = build_intent_chain(GPTOSS120)\n",
    "\n",
    "# Build chains for Qwen2-32B\n",
    "qwen_retrieval = build_retrieval_chain(qwen3_32)\n",
    "qwen_check = build_check_chain(qwen3_32)\n",
    "qwen_intent = build_intent_chain(qwen3_32)\n",
    "\n",
    "# Build chains for Llama70\n",
    "llama70_retrieval = build_retrieval_chain(llama70)\n",
    "llama70_check = build_check_chain(llama70)\n",
    "llama70_intent = build_intent_chain(llama70)\n",
    "\n",
    "# Map the model nicknames to the chains\n",
    "models_to_run = {\n",
    "    \"llama8\": {\"retrieval\": llama8_retrieval, \"check\": llama8_check, \"intent\": llama8_intent},\n",
    "    \"llama70\": {\"retrieval\": llama70_retrieval, \"check\": llama70_check, \"intent\": llama70_intent},\n",
    "    \"gptoss20\": {\"retrieval\": gpt20_retrieval, \"check\": gpt20_check, \"intent\": gpt20_intent},\n",
    "    \"gptoss120\": {\"retrieval\": gpt120_retrieval, \"check\": gpt120_check, \"intent\": gpt120_intent},\n",
    "    \"qwen3_32\": {\"retrieval\": qwen_retrieval, \"check\": qwen_check, \"intent\": qwen_intent}\n",
    "}\n",
    "\n",
    "# Load your ground dataset\n",
    "df_models = pd.read_csv(\"validated_reference_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, chains in models_to_run.items():\n",
    "    print(f\"Running evaluation for: {name}...\")\n",
    "    df_models = add_model_columns(\n",
    "        df_models,\n",
    "        retrieval_chain=chains[\"retrieval\"],\n",
    "        check_chain=chains[\"check\"],\n",
    "        intent_chain=chains[\"intent\"],\n",
    "        model_name=name\n",
    "    )\n",
    "\n",
    "# 4. Save the final combined results\n",
    "df_models.to_csv(\"eval_models_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46afc83",
   "metadata": {},
   "source": [
    "### Part 2 Judging the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "916a3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Task1Result(BaseModel):\n",
    "    reasoning: bool\n",
    "\n",
    "class Task2Result(BaseModel):\n",
    "    hallucination: bool\n",
    "    completeness: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0ee7da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "score_task1_prompt = \"\"\"\n",
    "### Role\n",
    "You are an expert Logical Analyst acting as an LLM Judge. \n",
    "Your goal is to perform a semantic and logical comparison between a validated reference data and a generated candidate output.\n",
    "\n",
    "### Inputs\n",
    "- Claim: {claim}\n",
    "\n",
    "### Reference\n",
    "- **Explanation (Reference):** \n",
    "<RExplanation>{r_explanation}</RExplanation>\n",
    "\n",
    "- **Rating (Reference):** {r_rating}\n",
    "\n",
    "### Candidate \n",
    "- **Explanation (Candidate):** \n",
    "<CExplanation>{c_explanation}</CExplanation>\n",
    "\n",
    "- **Rating (Candidate):** {c_rating}\n",
    "\n",
    "### Evaluation Objective\n",
    "Determine if the **Candidate Explanation** aligns with the **Reference Explanation** or provides a logically sound alternative based solely on the provided Claim.\n",
    "\n",
    "### Judgment Principles\n",
    "- **Logical Path**: Prioritize the underlying premises and causal links over tone, vocabulary, or verbosity.\n",
    "- **Rating Consistency**: The Candidate must reach the same conclusion or \"rating\" as the Reference.\n",
    "- **Permissible Variance**: Mark as TRUE if the Candidate uses a different but valid logical approach to reach the same conclusion, provided it does not contradict known facts.\n",
    "\n",
    "### Scoring Rubric\n",
    "**Set \"reasoning\": true IF:**\n",
    "- The core logical flow matches the Reference reasoning.\n",
    "- OR the reasoning differs but is factually correct, logically consistent, and reaches the same final rating.\n",
    "- All stated facts are accurate and no critical steps in the chain are missing.\n",
    "\n",
    "**Set \"reasoning\": false IF:**\n",
    "- The candidate introduces factual hallucinations (data or entities not in the claim/reference).\n",
    "- The explanation contains logical fallacies or misses a critical constraint.\n",
    "- The candidate suggests/implies a different fact-checking outcome than the Reference.\n",
    "\n",
    "### Output Format\n",
    "Return a JSON object with the following keys:\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"reasoning\": true/false,\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "t1_prompt = ChatPromptTemplate.from_template(score_task1_prompt)\n",
    "\n",
    "score_task2_prompt = \"\"\"\n",
    "### Role\n",
    "You are a Precision Auditor. Your task is to evaluate a generated response based on a set of provided source materials.\n",
    "\n",
    "### Inputs\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Reference\n",
    "- **Details(Reference):** \n",
    "<RDetails>{r_details}</RDetails>\n",
    "\n",
    "- **Alerts (Reference):** \n",
    "<RDetails>{r_alerts}</RDetails>\n",
    "\n",
    "### Candidate \n",
    "- **Details (Candidate):**\n",
    "<CDetails>{c_details}</CDetails>\n",
    "\n",
    "- **Alerts (Candidate):**\n",
    "<CDetails>{c_alerts}</CDetails>\n",
    "\n",
    "### Evaluation Objective 1: Completeness\n",
    "Assess if the **Candidate Data** includes the critical information, numbers, and alerts found in the **Reference Data**.\n",
    "\n",
    "**Judgment Principles**:\n",
    "  - Focus on essential facts: names, specific numbers, and methodological approach.\n",
    "  - Check for Alignment: Ensure the most critical \"Alerts\" or \"Flags\" from the Reference are present.\n",
    "  - **Leniency**: Do not penalize for minor differences in \"Quantitative\" vs \"Qualitative\" labeling, as these are often subjective.\n",
    "\n",
    "### Evaluation Objective 2: Hallucination\n",
    "Determine if the **Candidate Data** introduces information that is not supported by the provided sources.\n",
    "\n",
    "**Judgment Principles**:\n",
    "  - **No Hallucination (1)**: Every piece of evidence in the details or alerts exists strictly within the **Claim** or the **Additional Context**.\n",
    "  - **Hallucination (0)**: The response includes \"made up\" facts, figures, or external knowledge not explicitly found in the provided context.\n",
    "\n",
    "### Scoring Rubric\n",
    "\n",
    "#### **1. Completeness Score (0.0 to 1.0)**\n",
    "- **1.0**: All critical facts, numbers, and alerts from the Reference are present.\n",
    "- **0.5**: Some key details are missing, but the core essence remains.\n",
    "- **0.0**: Critical names, numbers, or alerts are entirely absent.\n",
    "\n",
    "#### **2. Hallucination Free (Boolean)**\n",
    "- **true**: Every claim in the Candidate is grounded in the Claim/Context.\n",
    "- **false**: The Candidate introduces information from outside the provided text.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"completeness\": 0.0 to 1.0\n",
    "  \"hallucination\": true/false\n",
    "}}.strip()\n",
    "\"\"\"\n",
    "t2_prompt = ChatPromptTemplate.from_template(score_task2_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "50a5e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def run_multi_model_evaluation(\n",
    "    df: pd.DataFrame, \n",
    "    task1_chain, \n",
    "    task2_chain, \n",
    "    model_suffixes: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs evaluation for multiple models and returns a NEW dataframe\n",
    "    with the results.\n",
    "    \"\"\"\n",
    "    # Copy of the dataframe\n",
    "    out_df = df.copy()\n",
    "\n",
    "    for suffix in model_suffixes:\n",
    "        print(f\"Evaluating model: {suffix}...\")\n",
    "        \n",
    "        # Define dynamic column names based on the suffix\n",
    "        c_expl_col = f\"explanation_{suffix}\"\n",
    "        c_rating_col = f\"checkable_{suffix}\"\n",
    "        c_details_col = f\"details_{suffix}\"\n",
    "        c_alerts_col = f\"alerts_{suffix}\"\n",
    "        c_confirmed_col = f\"confirmed_{suffix}\"\n",
    "        \n",
    "        # Reference columns (assuming these are constant/static)\n",
    "        r_expl_col = \"explanation\"\n",
    "        r_rating_col = \"checkable\"\n",
    "        r_details_col = \"details_text\"\n",
    "        r_alerts_col = \"alerts\"\n",
    "        r_confirmed_col = \"confirmed\"\n",
    "\n",
    "        # primary source data\n",
    "        claim_col = \"claim\"\n",
    "        translated = \"translated\"\n",
    "        year_col = \"year\"\n",
    "\n",
    "        def _evaluate_row(row):\n",
    "\n",
    "            # Get the additional context from the translated column, if it is not empty\n",
    "            additional_context = row[\"translated\"] if \"translated\" in row and isinstance(row[\"translated\"], str) and row[\"translated\"].strip() else \"\"\n",
    "\n",
    "            # Reasoning Comparison\n",
    "            t1_output = task1_chain.invoke({\n",
    "                \"r_explanation\": row[r_expl_col],\n",
    "                \"c_explanation\": row[c_expl_col],\n",
    "                \"r_rating\": row[r_rating_col],\n",
    "                \"c_rating\": row[c_rating_col],\n",
    "                \"claim\": row[claim_col],\n",
    "            })\n",
    "\n",
    "            # Completeness & Hallucination\n",
    "            t2_output = task2_chain.invoke({\n",
    "                \"r_details\": row[r_details_col],\n",
    "                \"c_details\": row[c_details_col],\n",
    "                \"r_alerts\": row[r_alerts_col],\n",
    "                \"c_alerts\": row[c_alerts_col],\n",
    "                \"claim\": row[claim_col],\n",
    "                \"year\":row[year_col],\n",
    "                \"additional_context\": additional_context,\n",
    "            })\n",
    "\n",
    "            # Check confirmed columns\n",
    "            intent = row[r_confirmed_col] == row[c_confirmed_col]\n",
    "\n",
    "            return (\n",
    "                t1_output.reasoning,\n",
    "                t2_output.completeness,\n",
    "                t2_output.hallucination,\n",
    "                intent,\n",
    "            )\n",
    "\n",
    "        # Apply the evaluation\n",
    "        temp_results = out_df.apply(_evaluate_row, axis=1)\n",
    "\n",
    "        # Unpack results into new columns in our results_df\n",
    "        out_df[f\"reason_{suffix}\"] = temp_results.apply(lambda t: t[0])\n",
    "        out_df[f\"complete_{suffix}\"] = temp_results.apply(lambda t: t[1])\n",
    "        out_df[f\"halluci_{suffix}\"] = temp_results.apply(lambda t: t[2])\n",
    "        out_df[f\"intent_{suffix}\"] = temp_results.apply(lambda t: t[3])\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c81e94",
   "metadata": {},
   "source": [
    "Finally we will run the evaluation calculating all the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fe29b073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: llama8...\n",
      "Evaluating model: gptoss20...\n"
     ]
    }
   ],
   "source": [
    "# these models are being evaluated\n",
    "models_to_test = [\"llama8\", \"gptoss20\"]\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_task1_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(Task1Result, method=\"json_mode\")\n",
    "    return t1_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_task2_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(Task2Result, method=\"json_mode\")\n",
    "    return t2_prompt | structured_llm\n",
    "\n",
    "# Build chains for GPT\n",
    "task1_chain = build_task1_chain(GPTOSS120)\n",
    "task2_chain = build_task2_chain(GPTOSS120)\n",
    "\n",
    "# Load your ground dataset\n",
    "original_df = pd.read_csv(\"eval_models_output.csv\", encoding=\"utf-8\")\n",
    "\n",
    "scores_df = run_multi_model_evaluation(original_df, task1_chain, task2_chain, models_to_test)\n",
    "\n",
    "scores_df.to_csv(\"eval_gptoss_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5f31d108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reason_llama8        0.865772\n",
       "complete_llama8      0.661074\n",
       "halluci_llama8       0.523490\n",
       "intent_llama8        0.664430\n",
       "reason_gptoss20      0.825503\n",
       "complete_gptoss20    0.597315\n",
       "halluci_gptoss20     0.489933\n",
       "intent_gptoss20      1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores_df[[\"reason_llama8\",\"complete_llama8\",\"halluci_llama8\",\"intent_llama8\",\"reason_gptoss20\",\"complete_gptoss20\",\"halluci_gptoss20\",\"intent_gptoss20\",\"reason_qwen3_32\",\"complete_qwen3_32\",\"halluci_qwen3_32\",\"intent_qwen3_32\"]].sum()\n",
    "scores_df[[\"reason_llama8\",\"complete_llama8\",\"halluci_llama8\",\"intent_llama8\",\"reason_gptoss20\",\"complete_gptoss20\",\"halluci_gptoss20\",\"intent_gptoss20\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58ec1c",
   "metadata": {},
   "source": [
    "Next to the LLM as judge approach, Bertscore was also used to measure the simularity between the reference an candidate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "43b0c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore for llama8...\n",
      "Computing BERTScore for gptoss20...\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(model_type=\"distilroberta-base\",lang=\"en\",rescale_with_baseline=True,device=\"cuda\")\n",
    "\n",
    "# Copy of dataset\n",
    "metrics_df = scores_df.copy()\n",
    "\n",
    "# Models to evaluate\n",
    "models = [\"llama8\", \"gptoss20\"]\n",
    "\n",
    "# Reference texts\n",
    "r_explanation = metrics_df[\"explanation\"].fillna(\"\").astype(str).tolist()\n",
    "r_details = metrics_df[\"details_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Computing BERTScore for {model}...\")\n",
    "\n",
    "    # Candidate texts\n",
    "    c_explanation = metrics_df[f\"explanation_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "    c_details = metrics_df[f\"details_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Explanation scores\n",
    "    P_e, R_e, F1_e = scorer.score(c_explanation, r_explanation)\n",
    "\n",
    "    # Details scores\n",
    "    P_d, R_d, F1_d = scorer.score(c_details, r_details)\n",
    "\n",
    "    # Store results\n",
    "    metrics_df[f\"P_explanation_{model}\"] = P_e.cpu().numpy()\n",
    "    metrics_df[f\"R_explanation_{model}\"] = R_e.cpu().numpy()\n",
    "    metrics_df[f\"F1_explanation_{model}\"] = F1_e.cpu().numpy()\n",
    "\n",
    "    metrics_df[f\"P_details_{model}\"] = P_d.cpu().numpy()\n",
    "    metrics_df[f\"R_details_{model}\"] = R_d.cpu().numpy()\n",
    "    metrics_df[f\"F1_details_{model}\"] = F1_d.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c8717",
   "metadata": {},
   "source": [
    "And create a better overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fcbc17a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reasoning</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>completeness</th>\n",
       "      <th>intent</th>\n",
       "      <th>P_e</th>\n",
       "      <th>R_e</th>\n",
       "      <th>F1_e</th>\n",
       "      <th>P_d</th>\n",
       "      <th>R_d</th>\n",
       "      <th>F1_d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama8</th>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.523490</td>\n",
       "      <td>0.661074</td>\n",
       "      <td>0.66443</td>\n",
       "      <td>0.570290</td>\n",
       "      <td>0.576105</td>\n",
       "      <td>0.573177</td>\n",
       "      <td>0.788007</td>\n",
       "      <td>0.650249</td>\n",
       "      <td>0.718141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gptoss20</th>\n",
       "      <td>0.825503</td>\n",
       "      <td>0.489933</td>\n",
       "      <td>0.597315</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.678966</td>\n",
       "      <td>0.645880</td>\n",
       "      <td>0.662246</td>\n",
       "      <td>0.788700</td>\n",
       "      <td>0.673410</td>\n",
       "      <td>0.730230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reasoning  hallucination  completeness   intent       P_e       R_e  \\\n",
       "model                                                                           \n",
       "llama8     0.865772       0.523490      0.661074  0.66443  0.570290  0.576105   \n",
       "gptoss20   0.825503       0.489933      0.597315  1.00000  0.678966  0.645880   \n",
       "\n",
       "              F1_e       P_d       R_d      F1_d  \n",
       "model                                             \n",
       "llama8    0.573177  0.788007  0.650249  0.718141  \n",
       "gptoss20  0.662246  0.788700  0.673410  0.730230  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for model in models:\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"reasoning\": scores_df[f\"reason_{model}\"].mean(),\n",
    "        \"hallucination\": scores_df[f\"halluci_{model}\"].mean(),\n",
    "        \"completeness\": scores_df[f\"complete_{model}\"].mean(),\n",
    "        \"intent\": scores_df[f\"intent_{model}\"].mean(),\n",
    "\n",
    "        \"P_e\": scores_df[f\"P_explanation_{model}\"].mean(),\n",
    "        \"R_e\": scores_df[f\"R_explanation_{model}\"].mean(),\n",
    "        \"F1_e\": scores_df[f\"F1_explanation_{model}\"].mean(),\n",
    "\n",
    "        \"P_d\": scores_df[f\"P_details_{model}\"].mean(),\n",
    "        \"R_d\": scores_df[f\"R_details_{model}\"].mean(),\n",
    "        \"F1_d\": scores_df[f\"F1_details_{model}\"].mean(),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
