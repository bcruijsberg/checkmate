{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f8bc32",
   "metadata": {},
   "source": [
    "## LLM as judge evaluation\n",
    "\n",
    "In **part 1** different models will perform the same 3 tasks as are in the ground truth model (and in the assistant). \n",
    "1. The first task is to check if a claim is checkable.\n",
    "2. The next task is to retrieve all relevant information, and if available also additional context from the original source.\n",
    "3. Finally a confirmation prompt will try to decide is the user confirms and wants to continue or not.\n",
    "\n",
    "In **part 2**\n",
    "Two larger different models will score the generated output of each model and task against the ground truth (the manually verified output). \n",
    "All tasks will be scored on a binary scale\n",
    "1. The first task will be scored on *Reasoning*, with 0=\"reasoning not that clear\", 1=\"clear reasoning\"\n",
    "2. The second task will be scored on:\n",
    "    - *Completeness*, with 0=\"missing facts\", 1=\"complete\"\n",
    "    - *Hallucination*, with 0=\"contains hallucinated facts\", 1=\"no hallucinations found\"\n",
    "3. The last task will be scored on *Intend* (did the user want to proceed or not), with 0=\"confirmed differs\", 1=\"confirmed is the same\"\n",
    "\n",
    "### Part 1 Generating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load alle the API keys\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Models on Groq,\n",
    "llama8 = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1)\n",
    "GPTOSS20 = ChatGroq(model_name=\"openai/gpt-oss-20b\", temperature=0.1)\n",
    "qwen3_32 = ChatGroq(model_name=\"qwen/qwen3-32b\", temperature=0.1)\n",
    "llama70 = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.1)\n",
    "GPTOSS120 = ChatGroq(model_name=\"openai/gpt-oss-120b\", temperature=0.1)\n",
    "\n",
    "# Ollama Models\n",
    "llama1 = ChatOllama(model=\"llama3.2:1b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "llama3 = ChatOllama(model=\"llama3.2:3b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "mistral7 = ChatOllama(model=\"mistral:7b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "qwen3_1 = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "qwen3_4 = ChatOllama(model=\"qwen3:4b\", temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "\n",
    "#judge\n",
    "llmGPT5 = ChatOpenAI(model=\"gpt-5\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "befd857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckResult(BaseModel):\n",
    "    checkable: Literal[\"POTENTIALLY CHECKABLE\", \"UNCHECKABLE\"]\n",
    "    explanation: str = Field(\"\")\n",
    "\n",
    "class RetrieveInfoResult(BaseModel):\n",
    "    claim_source: str = Field(\"unknown\")\n",
    "    primary_source: bool = Field(False)\n",
    "    source_description: str = Field(\"\")\n",
    "    subject: str = Field(\"unclear\")\n",
    "    data_type:str = Field(\"\") \n",
    "    precision: str = Field(\"\")\n",
    "    based_on: str = Field(\"\")\n",
    "    alerts: str = Field(\"\",description=\"A comma-separated list of flags or 'None' if no issues found\")\n",
    "    geography: str = Field(\"unclear\")\n",
    "    time_period: str = Field(\"unclear\")\n",
    "\n",
    "class ConfirmResult(BaseModel):\n",
    "    confirmed: bool = Field(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b90f9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "checkable_check_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst.\n",
    "\n",
    "### Inputs\n",
    "Claim: {claim}\n",
    "\n",
    "### Task\n",
    "Classify the claim and determine if it can be fact-checked.\n",
    "\n",
    "### Classification Logic\n",
    "- **UNCHECKABLE**: Opinion, value judgment, or prediction.\n",
    "- **POTENTIALLY CHECKABLE**: Factual claims about the past or present.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"checkable\": \"POTENTIALLY CHECKABLE\",\n",
    "  \"explanation\": \"Brief justification for the classification.\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "c_prompt = ChatPromptTemplate.from_template(checkable_check_prompt)\n",
    "\n",
    "retrieve_info_prompt = \"\"\"\n",
    "### Role\n",
    "Neutral Fact-Checking Analyst. Focus on objective evaluation.\n",
    "\n",
    "### Context\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Task 1: Source & Intent Extraction\n",
    "1. **claim_source**: Identify the person or organization who originated the claim.\n",
    "2. **primary_source**: Set to true ONLY if the evidence confirms this is the original/foundational origin.\n",
    "3. **source_description**: Describe the medium (e.g., \"Official PDF\", \"Social Media Post\").\n",
    "\n",
    "### Task 2: Factual Dimension Analysis\n",
    "1. **Subject**: Identify the core entity or event.\n",
    "2. **Data Type**: Quantitative or Qualitative, explain if it is measurable data or a descriptive data.\n",
    "3. **Precision**: Categorize as Precise, Vague, or Absolute (100%), and provide specific numbers, or names from the evidence.\n",
    "4. **Based On**: Identify the likely methodology (e.g., Official stats, Survey, research). Provide a brief explanation.\n",
    "5. **Geography**: Identify the geographic scope of the claim.\n",
    "6. **Time Period**: The time frame is normally {year}, unless otherwise stated in the claim or additional context.\n",
    "\n",
    "### Task 3: Guidance & Risk\n",
    "1. **Alerts**: Flag \"missing geography\", \"qualitative claim\", \"missing methodological details\", \"missing source\". \n",
    "  If the claim is quantitative, but no numbers are provide or textual precise indications (e.g., \"All\", \"None\"), all), *Flag in Alerts*: \"vague quantitative claim\".\n",
    "  Do not flag if the info is present.\n",
    "  Example output: \"missing geography, missing source\"\n",
    "2. Include specific details (dates, numbers, names) from the additional context if available:\n",
    "\"{additional_context}\"\n",
    "\n",
    "### IMPORTANT OUTPUT CONSTRAINTS\n",
    "- \"alerts\" MUST be a single STRING, NOT a list, NOT an object/dict.\n",
    "\n",
    "### Output Format (JSON)\n",
    "{{\n",
    "  \"claim_source\": \"a Person or an Organisation\" or \"unknown\"\n",
    "  \"primary_source\": true/false\n",
    "  \"source_description\": \"medium description\"\n",
    "  \"subject\": \"subject text\" or \"unclear\"\n",
    "  \"data_type\": \"STRING ONLY. Must start with one of: 'quantitative:', 'qualitative:', or 'unclear:' followed by a short explanation. Never boolean/null.\"\n",
    "  \"precision\": \"precise/vague/absolute and some specifics\"\n",
    "  \"based_on\": \"methodology and short explanation\" or \"unclear\"\n",
    "  \"alerts\": \"MUST be a String NOT a List, NOT a Dict. Either '' or a comma-separated(e.g., 'missing geography, missing source')\"\n",
    "  \"geography\": \"Geography\",\n",
    "  \"time_period\": \"Year or more precise timeframe\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "r_prompt = ChatPromptTemplate.from_template(retrieve_info_prompt)\n",
    "\n",
    "# Prompt to confirm the extracted information with the user\n",
    "intent_prompt = \"\"\"\n",
    "### Role\n",
    "Linguistic Analyst specializing in intent detection.\n",
    "\n",
    "### Context\n",
    "- User's Response: \"{user_answer}\"\n",
    "\n",
    "### Task\n",
    "Analyze the \"User's Response\" to determine if the assistant has permission to proceed to the next stage of the process (\"Green Light\").\n",
    "\n",
    "### Decision Rules\n",
    "**Set \"confirmed\": true IF:**\n",
    "1. **Explicit Command:** The user uses navigational keywords (e.g., \"Next,\" \"Continue,\" \"Proceed,\" \"Move on,\" \"Go ahead\").\n",
    "2. **Affirmation:** The user agrees with a previous summary (e.g., \"Yes,\" \"Correct,\" \"Exactly,\" \"That's right\").\n",
    "3. **Closure:** The user indicates they have no further details to provide (e.g., \"I don't know,\" \"That's all I have,\" \"No more info\").\n",
    "\n",
    "**Set \"confirmed\": false IF:**\n",
    "1. **New Info Only:** The user provides additional facts or context but does NOT include a command to move on.\n",
    "2. **Correction:** The user is correcting a previous mistake.\n",
    "3. **Uncertainty:** The user asks a follow-up question or expresses confusion.\n",
    "\n",
    "### Important Rules\n",
    "- Focus on the **intent to progress**.\n",
    "- If the response is ambiguous, default to `false`.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"confirmed\": true\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "i_prompt = ChatPromptTemplate.from_template(intent_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c5a88531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "# Function to add checkable columns to DataFrame\n",
    "def add_model_columns(\n",
    "        df: pd.DataFrame, \n",
    "        retrieval_chain, \n",
    "        check_chain,\n",
    "        intent_chain,\n",
    "        model_name: str, \n",
    "        claim_col: str = \"claim\", \n",
    "        context_col: str = \"translated\", \n",
    "        year_col: int = \"year\",\n",
    "        user_answer_col: str = \"user_answer\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    # Copy the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    # For each row in the dataset call the llm\n",
    "    def _run_row(row):\n",
    "        try:\n",
    "            claim = row[claim_col]\n",
    "            translated = row[context_col]\n",
    "            year = row[year_col]\n",
    "            user_answer= row[user_answer_col]\n",
    "            additional_context = (\n",
    "                translated if isinstance(translated, str) and translated.strip() else \"\"\n",
    "            )\n",
    "\n",
    "            check= check_chain.invoke({\n",
    "                \"claim\": claim,\n",
    "            })\n",
    "\n",
    "            retrieval= retrieval_chain.invoke({\n",
    "                \"claim\": claim,\n",
    "                \"year\":year,\n",
    "                \"additional_context\": additional_context\n",
    "            })\n",
    "\n",
    "            intent= intent_chain.invoke({\n",
    "                \"user_answer\": user_answer,\n",
    "            })\n",
    "\n",
    "            # Build the human-readable summary per row\n",
    "            details_text = (\n",
    "                f\"- claim_source: {retrieval.claim_source}\\n\"\n",
    "                f\"- primary_source: {retrieval.primary_source}\\n\"\n",
    "                f\"- source_description: {retrieval.source_description}\\n\"\n",
    "                f\"- subject: {retrieval.subject}\\n\"\n",
    "                f\"- quantitative: {retrieval.data_type or 'not clearly specified'}\\n\"\n",
    "                f\"- precision: {retrieval.precision}\\n\"\n",
    "                f\"- based_on: {retrieval.based_on}\\n\"\n",
    "                f\"- geography: {retrieval.geography}\\n\"\n",
    "                f\"- time_period: {retrieval.time_period}\\n\"\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Return a Series so we can easily join it back to the dataframe\n",
    "            return pd.Series({\n",
    "                f\"checkable_{model_name}\": check.checkable,\n",
    "                f\"explanation_{model_name}\": check.explanation,\n",
    "                f\"details_{model_name}\": details_text,\n",
    "                f\"alerts_{model_name}\": retrieval.alerts,\n",
    "                f\"confirmed_{model_name}\": intent.confirmed,\n",
    "            })\n",
    "\n",
    "        except (OutputParserException, ValueError, KeyError) as e:\n",
    "            # Soft failure: log and continue\n",
    "            return pd.Series({\n",
    "                f\"checkable_{model_name}\": None,\n",
    "                f\"explanation_{model_name}\": \"\",\n",
    "                f\"details_{model_name}\": \"\",\n",
    "                f\"alerts_{model_name}\": \"\",\n",
    "                f\"confirmed_{model_name}\": None,\n",
    "                f\"error_{model_name}\": str(e),\n",
    "        })\n",
    "\n",
    "    # Apply the function and join the new columns to the original dataframe\n",
    "    results = out.apply(_run_row, axis=1)\n",
    "    out = pd.concat([out, results], axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e342127",
   "metadata": {},
   "source": [
    "First a number of Models from Groq will perform the set tasks and later on scored on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4357ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_check_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(CheckResult, method=\"json_mode\")\n",
    "    return c_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_retrieval_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(RetrieveInfoResult, method=\"json_mode\")\n",
    "    return r_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_intent_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(ConfirmResult, method=\"json_mode\")\n",
    "    return i_prompt | structured_llm\n",
    "\n",
    "# Build chains for Llama8\n",
    "llama8_retrieval = build_retrieval_chain(llama8)\n",
    "llama8_check = build_check_chain(llama8)\n",
    "llama8_intent = build_intent_chain(llama8)\n",
    "\n",
    "# Build chains for GPT OSS-20B\n",
    "gpt20_retrieval = build_retrieval_chain(GPTOSS20)\n",
    "gpt20_check = build_check_chain(GPTOSS20)\n",
    "gpt20_intent = build_intent_chain(GPTOSS20)\n",
    "\n",
    "# Build chains for GPT OSS-120B\n",
    "gpt120_retrieval = build_retrieval_chain(GPTOSS120)\n",
    "gpt120_check = build_check_chain(GPTOSS120)\n",
    "gpt120_intent = build_intent_chain(GPTOSS120)\n",
    "\n",
    "# Build chains for Qwen2-32B\n",
    "qwen_retrieval = build_retrieval_chain(qwen3_32)\n",
    "qwen_check = build_check_chain(qwen3_32)\n",
    "qwen_intent = build_intent_chain(qwen3_32)\n",
    "\n",
    "# Build chains for Llama70\n",
    "llama70_retrieval = build_retrieval_chain(llama70)\n",
    "llama70_check = build_check_chain(llama70)\n",
    "llama70_intent = build_intent_chain(llama70)\n",
    "\n",
    "\n",
    "# Map the model nicknames to the chains\n",
    "models_to_run = {\n",
    "    \"llama8\": {\"retrieval\": llama8_retrieval, \"check\": llama8_check, \"intent\": llama8_intent},\n",
    "    \"llama70\": {\"retrieval\": llama70_retrieval, \"check\": llama70_check, \"intent\": llama70_intent},\n",
    "    \"gptoss20\": {\"retrieval\": gpt20_retrieval, \"check\": gpt20_check, \"intent\": gpt20_intent},\n",
    "    \"gptoss120\": {\"retrieval\": gpt120_retrieval, \"check\": gpt120_check, \"intent\": gpt120_intent},\n",
    "    \"qwen3_32\": {\"retrieval\": qwen_retrieval, \"check\": qwen_check, \"intent\": qwen_intent}\n",
    "}\n",
    "\n",
    "# Load your ground dataset\n",
    "df_models = pd.read_csv(\"validated_reference_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, chains in models_to_run.items():\n",
    "    print(f\"Running evaluation for: {name}...\")\n",
    "    df_models = add_model_columns(\n",
    "        df_models,\n",
    "        retrieval_chain=chains[\"retrieval\"],\n",
    "        check_chain=chains[\"check\"],\n",
    "        intent_chain=chains[\"intent\"],\n",
    "        model_name=name\n",
    "    )\n",
    "\n",
    "# 4. Save the final combined results\n",
    "df_models.to_csv(\"eval_models_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bb3143f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for: qwen3_1...\n",
      "Running evaluation for: qwen3_4...\n",
      "Running evaluation for: llama1...\n",
      "Running evaluation for: llama3...\n",
      "Running evaluation for: mistral7...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_check_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(CheckResult, method=\"json_mode\")\n",
    "    return c_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_retrieval_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(RetrieveInfoResult, method=\"json_mode\")\n",
    "    return r_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_intent_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(ConfirmResult, method=\"json_mode\")\n",
    "    return i_prompt | structured_llm\n",
    "\n",
    "# Build chains for Llama8\n",
    "llama1_retrieval = build_retrieval_chain(llama1)\n",
    "llama1_check = build_check_chain(llama1)\n",
    "llama1_intent = build_intent_chain(llama1)\n",
    "\n",
    "# Build chains for Llama3.2 3B\n",
    "llama3_retrieval = build_retrieval_chain(llama3)\n",
    "llama3_check = build_check_chain(llama3)\n",
    "llama3_intent = build_intent_chain(llama3)\n",
    "\n",
    "# Build chains for Mistral 7B\n",
    "mistral7_retrieval = build_retrieval_chain(mistral7)\n",
    "mistral7_check = build_check_chain(mistral7)\n",
    "mistral7_intent = build_intent_chain(mistral7)\n",
    "\n",
    "# Build chains for Qwen3-4B\n",
    "qwen3_1_retrieval = build_retrieval_chain(qwen3_1)\n",
    "qwen3_1_check = build_check_chain(qwen3_1)\n",
    "qwen3_1_intent = build_intent_chain(qwen3_1)\n",
    "\n",
    "# Build chains for Qwen3-4B\n",
    "qwen3_4_retrieval = build_retrieval_chain(qwen3_4)\n",
    "qwen3_4_check = build_check_chain(qwen3_4)\n",
    "qwen3_4_intent = build_intent_chain(qwen3_4)\n",
    "\n",
    "\n",
    "# Map the model nicknames to the chains\n",
    "models_to_run = {\n",
    "    \"qwen3_1\": {\"retrieval\": qwen3_1_retrieval, \"check\": qwen3_1_check, \"intent\": qwen3_1_intent},\n",
    "    \"qwen3_4\": {\"retrieval\": qwen3_4_retrieval, \"check\": qwen3_4_check, \"intent\": qwen3_4_intent},\n",
    "    \"llama1\": {\"retrieval\": llama1_retrieval, \"check\": llama1_check, \"intent\": llama1_intent},\n",
    "    \"llama3\": {\"retrieval\": llama3_retrieval, \"check\": llama3_check, \"intent\": llama3_intent},\n",
    "    \"mistral7\": {\"retrieval\": mistral7_retrieval, \"check\": mistral7_check, \"intent\": mistral7_intent},\n",
    "}+\n",
    "df_models = pd.read_csv(\"validated_reference_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, chains in models_to_run.items():\n",
    "    print(f\"Running evaluation for: {name}...\")\n",
    "    df_models = add_model_columns(\n",
    "        df_models,\n",
    "        retrieval_chain=chains[\"retrieval\"],\n",
    "        check_chain=chains[\"check\"],\n",
    "        intent_chain=chains[\"intent\"],\n",
    "        model_name=name\n",
    "    )\n",
    "\n",
    "# 4. Save the final combined results\n",
    "df_models.to_csv(\"eval_models_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46afc83",
   "metadata": {},
   "source": [
    "### Part 2 Judging the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "916a3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Task1Result(BaseModel):\n",
    "    reasoning: bool\n",
    "\n",
    "class Task2Result(BaseModel):\n",
    "    hallucination: bool\n",
    "    completeness: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0ee7da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "score_task1_prompt = \"\"\"\n",
    "### Role\n",
    "You are an expert Logical Analyst acting as an LLM Judge. \n",
    "Your goal is to perform a semantic and logical comparison between a validated reference data and a generated candidate output.\n",
    "\n",
    "### Inputs\n",
    "- Claim: {claim}\n",
    "\n",
    "### Reference\n",
    "- **Explanation (Reference):** \n",
    "<RExplanation>{r_explanation}</RExplanation>\n",
    "\n",
    "- **Rating (Reference):** {r_rating}\n",
    "\n",
    "### Candidate \n",
    "- **Explanation (Candidate):** \n",
    "<CExplanation>{c_explanation}</CExplanation>\n",
    "\n",
    "- **Rating (Candidate):** {c_rating}\n",
    "\n",
    "### Evaluation Objective\n",
    "Determine if the **Candidate Explanation** aligns with the **Reference Explanation** or provides a logically sound alternative based solely on the provided Claim.\n",
    "\n",
    "### Judgment Principles\n",
    "- **Logical Path**: Prioritize the underlying premises and causal links over tone, vocabulary, or verbosity.\n",
    "- **Rating Consistency**: The Candidate must reach the same conclusion or \"rating\" as the Reference.\n",
    "- **Permissible Variance**: Mark as TRUE if the Candidate uses a different but valid logical approach to reach the same conclusion, provided it does not contradict known facts.\n",
    "\n",
    "### Scoring Rubric\n",
    "**Set \"reasoning\": true IF:**\n",
    "- The core logical flow matches the Reference reasoning.\n",
    "- OR the reasoning differs but is factually correct, logically consistent, and reaches the same final rating.\n",
    "- All stated facts are accurate and no critical steps in the chain are missing.\n",
    "\n",
    "**Set \"reasoning\": false IF:**\n",
    "- The candidate introduces factual hallucinations (data or entities not in the claim/reference).\n",
    "- The explanation contains logical fallacies or misses a critical constraint.\n",
    "- The candidate suggests/implies a different fact-checking outcome than the Reference.\n",
    "\n",
    "### Output Format\n",
    "Return a JSON object with the following keys:\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"reasoning\": true/false,\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "t1_prompt = ChatPromptTemplate.from_template(score_task1_prompt)\n",
    "\n",
    "score_task2_prompt = \"\"\"\n",
    "### Role\n",
    "You are a Precision Auditor. Your task is to evaluate a generated response based on a set of provided source materials.\n",
    "\n",
    "### Inputs\n",
    "- Claim: {claim}\n",
    "- Year: {year}\n",
    "\n",
    "### Additional context the user provided\n",
    "\"{additional_context}\"\n",
    "\n",
    "### Reference\n",
    "- **Details(Reference):** \n",
    "<RDetails>{r_details}</RDetails>\n",
    "\n",
    "- **Alerts (Reference):** \n",
    "<RDetails>{r_alerts}</RDetails>\n",
    "\n",
    "### Candidate \n",
    "- **Details (Candidate):**\n",
    "<CDetails>{c_details}</CDetails>\n",
    "\n",
    "- **Alerts (Candidate):**\n",
    "<CDetails>{c_alerts}</CDetails>\n",
    "\n",
    "### Evaluation Objective 1: Completeness\n",
    "Assess if the **Candidate Data** includes the critical information, numbers, and alerts found in the **Reference Data**.\n",
    "\n",
    "**Judgment Principles**:\n",
    "  - Focus on essential facts: names, specific numbers, and methodological approach.\n",
    "  - Check for Alignment: Ensure the most critical \"Alerts\" or \"Flags\" from the Reference are present.\n",
    "  - **Leniency**: Do not penalize for minor differences in \"Quantitative\" vs \"Qualitative\" labeling, as these are often subjective.\n",
    "\n",
    "### Evaluation Objective 2: Hallucination\n",
    "Determine if the **Candidate Data** introduces information that is not supported by the provided sources.\n",
    "\n",
    "**Judgment Principles**:\n",
    "  - **No Hallucination (1)**: Every piece of evidence in the details or alerts exists strictly within the **Claim** or the **Additional Context**.\n",
    "  - **Hallucination (0)**: The response includes \"made up\" facts, figures, or external knowledge not explicitly found in the provided context.\n",
    "\n",
    "### Scoring Rubric\n",
    "\n",
    "#### **1. Completeness Score (0.0 to 1.0)**\n",
    "- **1.0**: All critical facts, numbers, and alerts from the Reference are present.\n",
    "- **0.5**: Some key details are missing, but the core essence remains.\n",
    "- **0.0**: Critical names, numbers, or alerts are entirely absent.\n",
    "\n",
    "#### **2. Hallucination Free (Boolean)**\n",
    "- **true**: Every claim in the Candidate is grounded in the Claim/Context.\n",
    "- **false**: The Candidate introduces information from outside the provided text.\n",
    "\n",
    "### Output (JSON)\n",
    "{{\n",
    "  \"completeness\": 0.0 to 1.0\n",
    "  \"hallucination\": true/false\n",
    "}}.strip()\n",
    "\"\"\"\n",
    "t2_prompt = ChatPromptTemplate.from_template(score_task2_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def run_multi_model_evaluation(\n",
    "    df: pd.DataFrame, \n",
    "    task1_chain, \n",
    "    task2_chain, \n",
    "    model_suffixes: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs evaluation for multiple models and returns a NEW dataframe\n",
    "    with the results.\n",
    "    \"\"\"\n",
    "    # Copy of the dataframe\n",
    "    out_df = df.copy()\n",
    "\n",
    "    for suffix in model_suffixes:\n",
    "        print(f\"Evaluating model: {suffix}...\")\n",
    "        \n",
    "        # Define dynamic column names based on the suffix\n",
    "        c_expl_col = f\"explanation_{suffix}\"\n",
    "        c_rating_col = f\"checkable_{suffix}\"\n",
    "        c_details_col = f\"details_{suffix}\"\n",
    "        c_alerts_col = f\"alerts_{suffix}\"\n",
    "        c_confirmed_col = f\"confirmed_{suffix}\"\n",
    "        \n",
    "        # Reference columns (assuming these are constant/static)\n",
    "        r_expl_col = \"explanation\"\n",
    "        r_rating_col = \"checkable\"\n",
    "        r_details_col = \"details_text\"\n",
    "        r_alerts_col = \"alerts\"\n",
    "        r_confirmed_col = \"confirmed\"\n",
    "\n",
    "        # primary source data\n",
    "        claim_col = \"claim\"\n",
    "        translated = \"translated\"\n",
    "        year_col = \"year\"\n",
    "\n",
    "        def _evaluate_row(row):\n",
    "            try:\n",
    "                # Get the additional context from the translated column, if it is not empty\n",
    "                additional_context = row[\"translated\"] if \"translated\" in row and isinstance(row[\"translated\"], str) and row[\"translated\"].strip() else \"\"\n",
    "\n",
    "                # Reasoning Comparison\n",
    "                t1_output = task1_chain.invoke({\n",
    "                    \"r_explanation\": row[r_expl_col],\n",
    "                    \"c_explanation\": row[c_expl_col],\n",
    "                    \"r_rating\": row[r_rating_col],\n",
    "                    \"c_rating\": row[c_rating_col],\n",
    "                    \"claim\": row[claim_col],\n",
    "                })\n",
    "\n",
    "                # Completeness & Hallucination\n",
    "                t2_output = task2_chain.invoke({\n",
    "                    \"r_details\": row[r_details_col],\n",
    "                    \"c_details\": row[c_details_col],\n",
    "                    \"r_alerts\": row[r_alerts_col],\n",
    "                    \"c_alerts\": row[c_alerts_col],\n",
    "                    \"claim\": row[claim_col],\n",
    "                    \"year\":row[year_col],\n",
    "                    \"additional_context\": additional_context,\n",
    "                })\n",
    "\n",
    "                # Check confirmed columns\n",
    "                intent = row[r_confirmed_col] == row[c_confirmed_col]\n",
    "\n",
    "                return (\n",
    "                    t1_output.reasoning,\n",
    "                    t2_output.completeness,\n",
    "                    t2_output.hallucination,\n",
    "                    intent,\n",
    "                )\n",
    "            except (OutputParserException, ValueError, KeyError) as e:\n",
    "                # Soft failure: log and continue\n",
    "                return (None,None,None,None)\n",
    "\n",
    "        # Apply the evaluation\n",
    "        temp_results = out_df.apply(_evaluate_row, axis=1)\n",
    "\n",
    "        # Unpack results into new columns in our results_df\n",
    "        out_df[f\"reason_{suffix}\"] = temp_results.apply(lambda t: t[0])\n",
    "        out_df[f\"complete_{suffix}\"] = temp_results.apply(lambda t: t[1])\n",
    "        out_df[f\"halluci_{suffix}\"] = temp_results.apply(lambda t: t[2])\n",
    "        out_df[f\"intent_{suffix}\"] = temp_results.apply(lambda t: t[3])\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c81e94",
   "metadata": {},
   "source": [
    "Finally we will run the evaluation calculating all the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1aa1e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: llama1...\n"
     ]
    }
   ],
   "source": [
    "# these models are being evaluated\n",
    "# models_to_test = [\"llama8\", \"llama70\", \"gptoss20\", \"gptoss120\",\"qwen3_32\"]\n",
    "# models_to_test = [\"llama1\",\"llama3\",\"mistral7\",\"qwen3_1\",\"qwen3_4\"]\n",
    "models_to_test = [\"llama1\"]\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_task1_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(Task1Result, method=\"json_mode\")\n",
    "    return t1_prompt | structured_llm\n",
    "\n",
    "# Build the langchain chain\n",
    "def build_task2_chain(llm):\n",
    "    structured_llm = llm.with_structured_output(Task2Result, method=\"json_mode\")\n",
    "    return t2_prompt | structured_llm\n",
    "\n",
    "# Build chains for GPT\n",
    "task1_chain = build_task1_chain(llmGPT5)\n",
    "task2_chain = build_task2_chain(llmGPT5)\n",
    "\n",
    "# Load your ground dataset\n",
    "original_df = pd.read_csv(\"eval_models_output.csv\", encoding=\"utf-8\")\n",
    "\n",
    "scores_df = run_multi_model_evaluation(original_df, task1_chain, task2_chain, models_to_test)\n",
    "\n",
    "scores_df.to_csv(\"scores_output-llama1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "432391e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 149 entries, 0 to 148\n",
      "Data columns (total 59 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   url                   149 non-null    object \n",
      " 1   reason_llama1         149 non-null    bool   \n",
      " 2   complete_llama1       149 non-null    float64\n",
      " 3   halluci_llama1        149 non-null    bool   \n",
      " 4   intent_llama1         149 non-null    bool   \n",
      " 5   claim                 149 non-null    object \n",
      " 6   rating                148 non-null    object \n",
      " 7   translated            25 non-null     object \n",
      " 8   year                  148 non-null    float64\n",
      " 9   checkable             149 non-null    object \n",
      " 10  explanation           149 non-null    object \n",
      " 11  details_text          149 non-null    object \n",
      " 12  alerts                149 non-null    object \n",
      " 13  question              149 non-null    object \n",
      " 14  user_answer           149 non-null    object \n",
      " 15  confirmed             149 non-null    bool   \n",
      " 16  alerts_qwen3_1        137 non-null    object \n",
      " 17  checkable_qwen3_1     144 non-null    object \n",
      " 18  confirmed_qwen3_1     144 non-null    object \n",
      " 19  details_qwen3_1       144 non-null    object \n",
      " 20  error_qwen3_1         5 non-null      object \n",
      " 21  explanation_qwen3_1   144 non-null    object \n",
      " 22  checkable_qwen3_4     149 non-null    object \n",
      " 23  explanation_qwen3_4   149 non-null    object \n",
      " 24  details_qwen3_4       149 non-null    object \n",
      " 25  alerts_qwen3_4        137 non-null    object \n",
      " 26  confirmed_qwen3_4     149 non-null    bool   \n",
      " 27  alerts_llama1         7 non-null      object \n",
      " 28  checkable_llama1      140 non-null    object \n",
      " 29  confirmed_llama1      140 non-null    object \n",
      " 30  details_llama1        140 non-null    object \n",
      " 31  error_llama1          9 non-null      object \n",
      " 32  explanation_llama1    140 non-null    object \n",
      " 33  checkable_llama3      149 non-null    object \n",
      " 34  explanation_llama3    149 non-null    object \n",
      " 35  details_llama3        149 non-null    object \n",
      " 36  alerts_llama3         0 non-null      float64\n",
      " 37  confirmed_llama3      149 non-null    bool   \n",
      " 38  checkable_mistral7    149 non-null    object \n",
      " 39  explanation_mistral7  149 non-null    object \n",
      " 40  details_mistral7      149 non-null    object \n",
      " 41  alerts_mistral7       124 non-null    object \n",
      " 42  confirmed_mistral7    149 non-null    bool   \n",
      " 43  reason_llama3         149 non-null    bool   \n",
      " 44  complete_llama3       149 non-null    float64\n",
      " 45  halluci_llama3        149 non-null    bool   \n",
      " 46  intent_llama3         149 non-null    bool   \n",
      " 47  reason_mistral7       149 non-null    bool   \n",
      " 48  complete_mistral7     149 non-null    float64\n",
      " 49  halluci_mistral7      149 non-null    bool   \n",
      " 50  intent_mistral7       149 non-null    bool   \n",
      " 51  reason_qwen3_1        149 non-null    bool   \n",
      " 52  complete_qwen3_1      149 non-null    float64\n",
      " 53  halluci_qwen3_1       149 non-null    bool   \n",
      " 54  intent_qwen3_1        149 non-null    bool   \n",
      " 55  reason_qwen3_4        149 non-null    bool   \n",
      " 56  complete_qwen3_4      149 non-null    float64\n",
      " 57  halluci_qwen3_4       149 non-null    bool   \n",
      " 58  intent_qwen3_4        149 non-null    bool   \n",
      "dtypes: bool(19), float64(7), object(33)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df3.info()\n",
    "df3.to_csv(\"scores_output-ollama.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58ec1c",
   "metadata": {},
   "source": [
    "Next to the LLM as judge approach, Bertscore was also used to measure the simularity between the reference an candidate texts\n",
    "\n",
    "First for the Groq models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore for llama8...\n",
      "Computing BERTScore for llama70...\n",
      "Computing BERTScore for gptoss20...\n",
      "Computing BERTScore for gptoss120...\n",
      "Computing BERTScore for qwen3_32...\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(model_type=\"distilroberta-base\",lang=\"en\",rescale_with_baseline=True,device=\"cuda\")\n",
    "\n",
    "# Load your ground dataset\n",
    "metrics_df  = pd.read_csv(\"scores_output-groq.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_test = [\"llama8\", \"llama70\", \"gptoss20\", \"gptoss120\",\"qwen3_32\"]\n",
    "\n",
    "# Reference texts\n",
    "r_explanation = metrics_df[\"explanation\"].fillna(\"\").astype(str).tolist()\n",
    "r_details = metrics_df[\"details_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Computing BERTScore for {model}...\")\n",
    "\n",
    "    # Candidate texts\n",
    "    c_explanation = metrics_df[f\"explanation_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "    c_details = metrics_df[f\"details_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Explanation scores\n",
    "    P_e, R_e, F1_e = scorer.score(c_explanation, r_explanation)\n",
    "\n",
    "    # Details scores\n",
    "    P_d, R_d, F1_d = scorer.score(c_details, r_details)\n",
    "\n",
    "    # Store results\n",
    "    metrics_df[f\"P_explanation_{model}\"] = P_e.cpu().numpy()\n",
    "    metrics_df[f\"R_explanation_{model}\"] = R_e.cpu().numpy()\n",
    "    metrics_df[f\"F1_explanation_{model}\"] = F1_e.cpu().numpy()\n",
    "\n",
    "    metrics_df[f\"P_details_{model}\"] = P_d.cpu().numpy()\n",
    "    metrics_df[f\"R_details_{model}\"] = R_d.cpu().numpy()\n",
    "    metrics_df[f\"F1_details_{model}\"] = F1_d.cpu().numpy()\n",
    "\n",
    "metrics_df.to_csv(\"metrics_output-groq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fcbc17a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reasoning</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>completeness</th>\n",
       "      <th>intent</th>\n",
       "      <th>P_e</th>\n",
       "      <th>R_e</th>\n",
       "      <th>F1_e</th>\n",
       "      <th>P_d</th>\n",
       "      <th>R_d</th>\n",
       "      <th>F1_d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama8</th>\n",
       "      <td>0.885906</td>\n",
       "      <td>0.308725</td>\n",
       "      <td>0.667919</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.562578</td>\n",
       "      <td>0.571306</td>\n",
       "      <td>0.566904</td>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.651549</td>\n",
       "      <td>0.720314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama70</th>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.789396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588661</td>\n",
       "      <td>0.608305</td>\n",
       "      <td>0.598483</td>\n",
       "      <td>0.737947</td>\n",
       "      <td>0.699739</td>\n",
       "      <td>0.718642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gptoss20</th>\n",
       "      <td>0.832215</td>\n",
       "      <td>0.503356</td>\n",
       "      <td>0.647315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.640113</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.785242</td>\n",
       "      <td>0.673822</td>\n",
       "      <td>0.728718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gptoss120</th>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.624161</td>\n",
       "      <td>0.895973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707879</td>\n",
       "      <td>0.674877</td>\n",
       "      <td>0.691228</td>\n",
       "      <td>0.815085</td>\n",
       "      <td>0.825006</td>\n",
       "      <td>0.819807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen3_32</th>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.704698</td>\n",
       "      <td>0.770470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522467</td>\n",
       "      <td>0.608977</td>\n",
       "      <td>0.565265</td>\n",
       "      <td>0.732736</td>\n",
       "      <td>0.718060</td>\n",
       "      <td>0.725126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reasoning  hallucination  completeness    intent       P_e  \\\n",
       "model                                                                   \n",
       "llama8      0.885906       0.308725      0.667919  0.671141  0.562578   \n",
       "llama70     0.872483       0.671141      0.789396  1.000000  0.588661   \n",
       "gptoss20    0.832215       0.503356      0.647315  1.000000  0.678425   \n",
       "gptoss120   0.791946       0.624161      0.895973  1.000000  0.707879   \n",
       "qwen3_32    0.838926       0.704698      0.770470  1.000000  0.522467   \n",
       "\n",
       "                R_e      F1_e       P_d       R_d      F1_d  \n",
       "model                                                        \n",
       "llama8     0.571306  0.566904  0.790965  0.651549  0.720314  \n",
       "llama70    0.608305  0.598483  0.737947  0.699739  0.718642  \n",
       "gptoss20   0.640113  0.659091  0.785242  0.673822  0.728718  \n",
       "gptoss120  0.674877  0.691228  0.815085  0.825006  0.819807  \n",
       "qwen3_32   0.608977  0.565265  0.732736  0.718060  0.725126  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"reasoning\": metrics_df[f\"reason_{model}\"].mean(),\n",
    "        \"hallucination\": metrics_df[f\"halluci_{model}\"].mean(),\n",
    "        \"completeness\": metrics_df[f\"complete_{model}\"].mean(),\n",
    "        \"intent\": metrics_df[f\"intent_{model}\"].mean(),\n",
    "\n",
    "        \"P_e\": metrics_df[f\"P_explanation_{model}\"].mean(),\n",
    "        \"R_e\": metrics_df[f\"R_explanation_{model}\"].mean(),\n",
    "        \"F1_e\": metrics_df[f\"F1_explanation_{model}\"].mean(),\n",
    "\n",
    "        \"P_d\": metrics_df[f\"P_details_{model}\"].mean(),\n",
    "        \"R_d\": metrics_df[f\"R_details_{model}\"].mean(),\n",
    "        \"F1_d\": metrics_df[f\"F1_details_{model}\"].mean(),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4860d2",
   "metadata": {},
   "source": [
    "Next for the local Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore for llama1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore for llama3...\n",
      "Computing BERTScore for mistral7...\n",
      "Computing BERTScore for qwen3_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BERTScore for qwen3_4...\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(model_type=\"distilroberta-base\",lang=\"en\",rescale_with_baseline=True,device=\"cuda\")\n",
    "\n",
    "# Load your ground dataset\n",
    "metrics_df  = pd.read_csv(\"scores_output-ollama.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_test = [\"llama1\",\"llama3\",\"mistral7\",\"qwen3_1\",\"qwen3_4\"]\n",
    "\n",
    "# Reference texts\n",
    "r_explanation = metrics_df[\"explanation\"].fillna(\"\").astype(str).tolist()\n",
    "r_details = metrics_df[\"details_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Computing BERTScore for {model}...\")\n",
    "\n",
    "    # Candidate texts\n",
    "    c_explanation = metrics_df[f\"explanation_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "    c_details = metrics_df[f\"details_{model}\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Explanation scores\n",
    "    P_e, R_e, F1_e = scorer.score(c_explanation, r_explanation)\n",
    "\n",
    "    # Details scores\n",
    "    P_d, R_d, F1_d = scorer.score(c_details, r_details)\n",
    "\n",
    "    # Store results\n",
    "    metrics_df[f\"P_explanation_{model}\"] = P_e.cpu().numpy()\n",
    "    metrics_df[f\"R_explanation_{model}\"] = R_e.cpu().numpy()\n",
    "    metrics_df[f\"F1_explanation_{model}\"] = F1_e.cpu().numpy()\n",
    "\n",
    "    metrics_df[f\"P_details_{model}\"] = P_d.cpu().numpy()\n",
    "    metrics_df[f\"R_details_{model}\"] = R_d.cpu().numpy()\n",
    "    metrics_df[f\"F1_details_{model}\"] = F1_d.cpu().numpy()\n",
    "\n",
    "metrics_df.to_csv(\"metrics_output-ollama.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "225fb615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reasoning</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>completeness</th>\n",
       "      <th>intent</th>\n",
       "      <th>P_e</th>\n",
       "      <th>R_e</th>\n",
       "      <th>F1_e</th>\n",
       "      <th>P_d</th>\n",
       "      <th>R_d</th>\n",
       "      <th>F1_d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama1</th>\n",
       "      <td>0.167785</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.095973</td>\n",
       "      <td>0.449664</td>\n",
       "      <td>-0.018388</td>\n",
       "      <td>0.100064</td>\n",
       "      <td>0.040633</td>\n",
       "      <td>0.278482</td>\n",
       "      <td>0.189079</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3</th>\n",
       "      <td>0.771812</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.263087</td>\n",
       "      <td>0.476510</td>\n",
       "      <td>0.366785</td>\n",
       "      <td>0.511736</td>\n",
       "      <td>0.438353</td>\n",
       "      <td>0.742092</td>\n",
       "      <td>0.593688</td>\n",
       "      <td>0.666777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral7</th>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.432886</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.490872</td>\n",
       "      <td>0.548634</td>\n",
       "      <td>0.519646</td>\n",
       "      <td>0.728498</td>\n",
       "      <td>0.632122</td>\n",
       "      <td>0.679709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen3_1</th>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.455034</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.185339</td>\n",
       "      <td>0.343355</td>\n",
       "      <td>0.263365</td>\n",
       "      <td>0.609139</td>\n",
       "      <td>0.354528</td>\n",
       "      <td>0.479303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen3_4</th>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.705168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.389928</td>\n",
       "      <td>0.585888</td>\n",
       "      <td>0.486039</td>\n",
       "      <td>0.789125</td>\n",
       "      <td>0.680113</td>\n",
       "      <td>0.733916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reasoning  hallucination  completeness    intent       P_e  \\\n",
       "model                                                                  \n",
       "llama1     0.167785       0.134228      0.095973  0.449664 -0.018388   \n",
       "llama3     0.771812       0.409396      0.263087  0.476510  0.366785   \n",
       "mistral7   0.872483       0.093960      0.432886  0.838926  0.490872   \n",
       "qwen3_1    0.718121       0.328859      0.455034  0.946309  0.185339   \n",
       "qwen3_4    0.691275       0.657718      0.705168  1.000000  0.389928   \n",
       "\n",
       "               R_e      F1_e       P_d       R_d      F1_d  \n",
       "model                                                       \n",
       "llama1    0.100064  0.040633  0.278482  0.189079  0.233333  \n",
       "llama3    0.511736  0.438353  0.742092  0.593688  0.666777  \n",
       "mistral7  0.548634  0.519646  0.728498  0.632122  0.679709  \n",
       "qwen3_1   0.343355  0.263365  0.609139  0.354528  0.479303  \n",
       "qwen3_4   0.585888  0.486039  0.789125  0.680113  0.733916  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"reasoning\": metrics_df[f\"reason_{model}\"].mean(),\n",
    "        \"hallucination\": metrics_df[f\"halluci_{model}\"].mean(),\n",
    "        \"completeness\": metrics_df[f\"complete_{model}\"].mean(),\n",
    "        \"intent\": metrics_df[f\"intent_{model}\"].mean(),\n",
    "\n",
    "        \"P_e\": metrics_df[f\"P_explanation_{model}\"].mean(),\n",
    "        \"R_e\": metrics_df[f\"R_explanation_{model}\"].mean(),\n",
    "        \"F1_e\": metrics_df[f\"F1_explanation_{model}\"].mean(),\n",
    "\n",
    "        \"P_d\": metrics_df[f\"P_details_{model}\"].mean(),\n",
    "        \"R_d\": metrics_df[f\"R_details_{model}\"].mean(),\n",
    "        \"F1_d\": metrics_df[f\"F1_details_{model}\"].mean(),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"model\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
